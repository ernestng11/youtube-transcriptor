{
  "video_id": "T4XDMeoyvU0",
  "video_title": "SmolLMv3 - A Small Reasoner with Tool Use.",
  "video_url": "https://www.youtube.com/watch?v=T4XDMeoyvU0",
  "channel_title": "Unknown",
  "published_at": null,
  "duration_seconds": null,
  "view_count": null,
  "like_count": null,
  "description": "Blog: https://huggingface.co/blog/smollm3\nColab: https://dripl.ink/oFvSw\n\nFor more tutorials on using LLMs and building agents, check out my Patreon\nPatreon: https://www.patreon.com/SamWitteveen\nTwitter: https://x.com/Sam_Witteveen\n\nüïµÔ∏è Interested in building LLM Agents? Fill out the form below\nBuilding LLM Agents Form: https://drp.li/dIMes\n\nüë®‚ÄçüíªGithub:\nhttps://github.com/samwit/llm-tutorials\n\n‚è±Ô∏èTime Stamps:\n00:00 Intro\n00:18 SmolLM3 Hugging Face Blog\n02:27 SmolLM3 Blueprint\n07:26 Demo",
  "transcript": {
    "language": "en-US",
    "is_auto_generated": false,
    "total_segments": 275,
    "aggregated_text": "Okay, so in this video I want to cover SmolLM-V3. this is a new model that came out today from Hugging Face, actually trained by Hugging Face, created by Hugging Face. It builds on the previous SmolLMs that they've had in the past. slightly bigger than the other ones, at 3B. But in this video I wanna cover not just the model itself. I think it's really interesting around what they've released about how they created this model and the different kinds of training techniques that they've used. And also at the end, I wanna basically test it not just with reasoning, and non reasoning, but also with agentic uses. So let's see how it does at things like function calling and stuff like that. Could this be the model that you run locally to run all your agents without having to use any sort of proprietary model, et cetera? Alright, let's jump in. So Hugging Face has released this new 3B model And they've released both the, base model and instruct model, and an Onyx version, of the model. and already people, are making GGUF versions, which you can use on Olama and LM Studio. the model itself is certainly impressive. We can see that it lies in this area between the Qwen3, 1.7 B and the Qwen3, 4B. And 3B is an interesting size in that it's like a nice size that you could actually run on many mobile devices that are out there now. and sure enough, for that size, it seems to be beating out the old Qwen, 2.53 B, and the LlaMA 3.2 B. So if we look at some of the core stats for it, we can see that it's a model that's been trained on 11 trillion tokens, which is a very impressive feat, right? To actually train something for that long. We can see that they're claiming that state-of-the-art for all the 3B models and also, strongly competing with 4B models. And it's one of these new models that allows you to basically turn on and turn off reasoning. So we've looked at some of these before where you've got some kind of flag that you can turn on thinking or non-thinking. they're claiming that it's multilingual. for me, there's only six languages there. They're all European languages. for me personally, that's not very multilingual but also interestingly, this can actually use a long context of up to 128 K in here. Now the thing that I find that actually, is perhaps more interesting than the model itself is that they've released this blueprint. And this blueprint is exactly how they've done each step of the training. So even with the open weights models from DeepSeek, from Qwen, usually they're quite good in telling us at least roughly what they did. Unlike the papers coming outta the proprietary labs, which really aren't giving you any sort of real facts and basically just maybe have some vague clues every now and then. This is actually a full blueprint of, exactly what they did, for each of the steps from the pre-training recipe, how they set up the distributed training, how they did the model architecture, right through to things like the long context training and all the post-training recipes for actually putting this together. So if we just go through those quickly, they've got, some of these are already up on the blog post. but like I said, you've also got the blueprint where you can just come in and actually, analyze these things yourself. We can see the long context can go up to 256. I'm not sure how great it's gonna be past 128. it can use tools, which I definitely wanna look at the end, and we'll test that when we do the code test, for this and we can see that, it's got this dual think reasoning, system where you can turn the thinking on and off. And if we jump into the blog post, we can just look at some of the key facts in here. so the architecture that they've gone for is actually similar to some of the, LlaMA 3 architectures. if we think way back now, it's been quite a long time since LlaMA 3 was released. Clearly the LlaMA 3 release was a lot more successful than the LlaMA 4 release. it didn't require Zuckerberg going out and trying to, hire as many LLM researchers as possible, for doing that. but we can see that in there, they've got the group query attention. Now, the interesting thing is they've got this new thing, called NoPE. and they're fully citing, in here. So my brief understanding of this is using like a causal mask rather than the rotary embeddings, et cetera. And you can see that even though they've taken certain things from the LlaMA series. they've also basically looked around and taken, ideas from things like Olmo two, where they talk about, removing this weight decay from embedding layers to improve training stability. Now that's interesting. And gonna be very useful for anyone that does want to try and, train one of these small models. And I gotta say that looking at their GPU budget for this, they had 384 H100s for 24 days. That's not a crazy amount of GPUs time. So actually I think that works out to roughly 220,000 plus GPU hours there. And the interesting thing is you can get H100s now, the price of being able to rent them out is going down. So we are looking at something that probably cost a few hundred thousand dollars to train. It is certainly not in the large millions of dollars, that we've seen people do in the past. All right, so one of the things that is always the mystery of the proprietary models is how do they do their data mixes and how do they do their pre-training? So it's really nice to see here that they're talking about doing a three-phase, pre-training with an anealing sort of at the end of that. But more interestingly, looking at the splits of, data as they go through this. so you can see at the start it's very web heavy for that sort of long phase. But then in phase two and phase three, they actually increased the code, they increased the math, quite a lot in there. just for this pre-training phrase. Now after that they talk about, how they did the long context stuff, and also how they did the actual reasoning stuff. Now, for me, looking at this sort of, briefly, it does look like they've heavily relied on both the DeepSeekR1, reasoning tracers, as well as Qwen3 for creating synthetic data to be able to, make some of these, reasoning traces out. So it doesn't look like they've done, a large amount of sort of RLVR or anything like that in here. At least in comparison to perhaps some of the Chinese labs that these people are trying to spend longer doing the reinforcement learning with verifiable rewards, et cetera. Interestingly, they use a new form of alignment, in here, which is a variant on DPO. and finally they also do things like model merging to basically take various checkpoints and try and create a super checkpoint in here. Now as I mentioned, they've put the base model and the instruction fine tune model up on Hugging Face. Actually, perhaps it would've been nice for them to put up the different checkpoints at the end of each phase of training as well. I'm not sure if they, perhaps they will release that in the future. That would be really good to see. We can also see that they've also gathered all the data sets here, both for things like SFT for, reasoning data sets that they were using in there. And generally it seems to me that we've gotta think of this as being one of the most open releases that we've seen, in quite a while. Okay, so let's jump in the code and see how the model actually performs. Okay, so jumping into the code, we can see that it's really easy to actually, set this model up. No surprise, it's been created by Hugging Face. It's gonna work with, transformers out of the box. it also works with SG Lang and with, VLLM. but anyway, in here you can see I've just loaded up the model. if we want to do, reasoning outputs, we can just basically pass in a prompt and you'll see we'll get the standard sort of DeepSeek think tags, where in this case we've got, quite a long think tag, going on there. And then we've got the answer coming out. If we want to get it with no reasoning, in the system prompt, we just need to pass in this slash no think. in there. and then you'll see that we just get, an answer out with no thinking tags, just the final answer out there. Alright, so you can manipulate the thinking with the system prompt a bit by referring into it. and the thinking, will often be very long, Now, unfortunately, I wasn't able to get it to number the thinking and break it down into sections. which is the same for the DeepSeek ones. With the way that they train that model and the Qwen3 model, they're really using this RLVR by verifiable rewards, which doesn't give you a lot of control over that long chain of thought to get to the right answer. You see this being actually different in the Gemini models and some of the other proprietary, reasoning models out there, which kind of sectionize, their thinking, tokens as they go through. Anyway, here we can see that okay, it did a nice long think, And then it gave us quite a detailed, response out, here. Now if we look at sometimes like this, so this is a code generation one. it will just generate empty thinking. so I found this on a few sort of examples. I'm not sure you know what they expect, the behavior is, for this, maybe if we had asked some things before we actually sent in the Python, function definition, we would've got a better response out. So I do notice this, for a number of responses where you will get empty thinking out there. I gotta say though, for such a small model, the thinking turns out to be quite good. I can't help but feel that perhaps it is just fine tuned in from DeepSeek and Qwen3 as opposed to actually getting the model, to do thinking itself. It's just extended chain of thought. but that said, we can look at, you know, so a question like this, write a detailed breakdown of building a successful lemonade stand. the thinking actually is quite good, right? it goes through the different elements before giving a detailed plan, out, for this. Now, when we compare this to a version with no thinking, you can see in this case, the plan is much simpler. definitely not as well thought out and stuff like that. So it does show that the thinking, will help for, certain kind of responses. often you will also get the model wanting to give some justification at the end or to give, so in this case additional tips at the end. Okay. Another one, old question was the difference between llama, Vicuna, alpaca Again here, it just dropped out the thinking. I'm not sure, why, but it gave us detailed, a very detailed, sort of response, probably on par with some of the models that we'd seen before. Now one area where I thought it was a bit, not so great here where I've asked it, what is the capital of England? I've left the thinking on, we get a huge amount of thinking and then we get a step by step, breakdown of the answer like the justification that I was talking about before. Another example here of the thinking for the Geoffrey Hinton question, gives some nice rationale at the end, for this. Does really well on the GSM 8K stuff, not surprisingly now. it is kind of amazing to think that a 3B model is, doing so much better on this kind of stuff compared to going back a year, 18 months ago where we were looking at 14 B models that couldn't do, a lot of these things. Alright, finally, just to finish up the tool use. So, one of the things that I find, most interesting about this model is that it can do function calling. and that it's supposedly been, primed a bit for agentic use. So when we come in and look at the tool use, we can basically just define a tool. you can see here that we've just defined this, the schema for the tool in here. we can then, pass in a, message and sure enough, what it will do is that the message, this is showing all the data that's coming out. sure enough, the message will actually then include a tool call in it. so in this case, the tool call is the get weather, and it worked out that it was Copenhagen, which is what it should have been for this. so I tried this out for a number of different, functions. So doing like a search thing, where we can say, okay, when will OpenAI release the open weights model? What are the rumors? and we can see, sure enough, we're taking in that one tool there. and, it's got some instructions of how to do it. And sure enough, it returns back a tool call. so the tool call in this case is the search web. And the arguments it's come up with is OpenAI, open weights model, release date rumors. So it actually is working out what are the keywords? So I tried this for a few different things, for things like, okay, who won the Nobel Prize in chemistry? Now, technically if it's cutoff date is June 2025 should actually know that. But in this case, it decided, okay, let's do a search 24 Nobel Prize chemistry winner. I actually think that's good, right? that I don't expect a 3B model to be really good with facts. I feel it's like, better for it to actually try things out there. Alright. And then finally, if we are passing in multiple tools here, I've got the search web and the weather, and we ask it something that really shouldn't use either tool. We want it to not use a tool, right? We want it to know that, okay, it should just respond, to this. Now, I have mixed responses with this. Sometimes, it works really well as you see here where, okay, we've passed in the tools. we've passed in something. We didn't get a tool call back. We didn't get any thinking. We just got, I'm an AI, I don't have feelings, but I'm here to help. we got a response back. often it would be like this every now and then. I found that okay, it will try to use the search tool, for things that perhaps maybe you don't want it to use it as tool. Now, that could also be, down to my description in here I've got, allows you to use the search the internet, find useful and up-to-date information. you could, play around with that and I think you certainly want to do that if you're planning on using this thing. So the model is out also on, Olama. I think this is a custom version, someone's uploaded, but my guess is we'll see probably an official version, up there as well. and I'm sure it's just a matter of time before you see it on LM Studio as well. Overall, this is, a really interesting release from Hugging Face. And, I really think they should be commended for this. they've not only released, an interesting model, but they've also released this really nice blueprint of how, they created it, what they did at each step. my guess is that there's probably a full paper coming with maybe even more details, which will be great. Hopefully they will release the checkpoints for like each of the phases and stuff like that. So people could look at testing different ideas, for example, if you wanted to change something in the phase two, you don't wanna have to do the whole phase one by yourself. If you could just take their checkpoint for that and then try out, new ideas for the phase two, phase three, or for the different sections of the post-training stuff as well. There's quite a lot of, tokens that they trained on in there. Anyway, check it out. Let me know in the comments, are you actually still looking to use, small local models, on your own machine or are you more moving to the really cheap proprietary models that can do all of these things? this is certainly one of the things I'm looking at more and more. and talking to other people, I see, use cases for sort of both systems and stuff. So I'd love to hear your thoughts about that in the comments. Anyway, as always, if you found the video useful, please click like and subscribe, and I will talk to you in the next video. Bye for now.",
    "text_length": 15673,
    "word_count": 2925
  },
  "segments": [
    {
      "start": 0.0,
      "duration": 4.949,
      "text": "Okay, so in this video I\nwant to cover SmolLM-V3.",
      "timestamp": "00:00"
    },
    {
      "start": 5.229,
      "duration": 4.469,
      "text": "this is a new model that came out today\nfrom Hugging Face, actually trained by",
      "timestamp": "00:05"
    },
    {
      "start": 9.699,
      "duration": 2.19,
      "text": "Hugging Face, created by Hugging Face.",
      "timestamp": "00:09"
    },
    {
      "start": 12.169,
      "duration": 3.279,
      "text": "It builds on the previous SmolLMs\nthat they've had in the past.",
      "timestamp": "00:12"
    },
    {
      "start": 15.639,
      "duration": 2.55,
      "text": "slightly bigger than\nthe other ones, at 3B.",
      "timestamp": "00:15"
    },
    {
      "start": 18.459,
      "duration": 3.6,
      "text": "But in this video I wanna cover\nnot just the model itself.",
      "timestamp": "00:18"
    },
    {
      "start": 22.109,
      "duration": 3.75,
      "text": "I think it's really interesting around\nwhat they've released about how they",
      "timestamp": "00:22"
    },
    {
      "start": 25.859,
      "duration": 4.5,
      "text": "created this model and the different kinds\nof training techniques that they've used.",
      "timestamp": "00:25"
    },
    {
      "start": 30.689,
      "duration": 4.54,
      "text": "And also at the end, I wanna basically\ntest it not just with reasoning, and non",
      "timestamp": "00:30"
    },
    {
      "start": 35.229,
      "duration": 2.44,
      "text": "reasoning, but also with agentic uses.",
      "timestamp": "00:35"
    },
    {
      "start": 37.689,
      "duration": 3.57,
      "text": "So let's see how it does at things like\nfunction calling and stuff like that.",
      "timestamp": "00:37"
    },
    {
      "start": 41.259,
      "duration": 3.465,
      "text": "Could this be the model that you\nrun locally to run all your agents",
      "timestamp": "00:41"
    },
    {
      "start": 44.724,
      "duration": 3.27,
      "text": "without having to use any sort\nof proprietary model, et cetera?",
      "timestamp": "00:44"
    },
    {
      "start": 48.471,
      "duration": 0.825,
      "text": "Alright, let's jump in.",
      "timestamp": "00:48"
    },
    {
      "start": 49.796,
      "duration": 4.74,
      "text": "So Hugging Face has released this new\n3B model And they've released both",
      "timestamp": "00:49"
    },
    {
      "start": 54.536,
      "duration": 4.81,
      "text": "the, base model and instruct model,\nand an Onyx version, of the model.",
      "timestamp": "00:54"
    },
    {
      "start": 59.536,
      "duration": 3.63,
      "text": "and already people, are making\nGGUF versions, which you can",
      "timestamp": "00:59"
    },
    {
      "start": 63.166,
      "duration": 2.79,
      "text": "use on Olama and LM Studio.",
      "timestamp": "01:03"
    },
    {
      "start": 66.269,
      "duration": 1.919,
      "text": "the model itself is certainly impressive.",
      "timestamp": "01:06"
    },
    {
      "start": 68.189,
      "duration": 4.075,
      "text": "We can see that it lies in\nthis area between the Qwen3,",
      "timestamp": "01:08"
    },
    {
      "start": 72.284,
      "duration": 2.68,
      "text": "1.7 B and the Qwen3, 4B.",
      "timestamp": "01:12"
    },
    {
      "start": 75.394,
      "duration": 3.9,
      "text": "And 3B is an interesting size in\nthat it's like a nice size that you",
      "timestamp": "01:15"
    },
    {
      "start": 79.294,
      "duration": 4.44,
      "text": "could actually run on many mobile\ndevices that are out there now.",
      "timestamp": "01:19"
    },
    {
      "start": 84.068,
      "duration": 3.49,
      "text": "and sure enough, for that size, it\nseems to be beating out the old Qwen,",
      "timestamp": "01:24"
    },
    {
      "start": 87.578,
      "duration": 6.953,
      "text": "2.53 B, and the LlaMA 3.2 B.  So if we\nlook at some of the core stats for it,",
      "timestamp": "01:27"
    },
    {
      "start": 94.771,
      "duration": 4.23,
      "text": "we can see that it's a model that's\nbeen trained on 11 trillion tokens,",
      "timestamp": "01:34"
    },
    {
      "start": 99.001,
      "duration": 3.0,
      "text": "which is a very impressive feat, right?",
      "timestamp": "01:39"
    },
    {
      "start": 102.001,
      "duration": 2.25,
      "text": "To actually train something for that long.",
      "timestamp": "01:42"
    },
    {
      "start": 104.611,
      "duration": 4.59,
      "text": "We can see that they're claiming that\nstate-of-the-art for all the 3B models and",
      "timestamp": "01:44"
    },
    {
      "start": 109.201,
      "duration": 2.69,
      "text": "also, strongly competing with 4B models.",
      "timestamp": "01:49"
    },
    {
      "start": 112.391,
      "duration": 3.15,
      "text": "And it's one of these new models\nthat allows you to basically",
      "timestamp": "01:52"
    },
    {
      "start": 115.541,
      "duration": 2.79,
      "text": "turn on and turn off reasoning.",
      "timestamp": "01:55"
    },
    {
      "start": 118.458,
      "duration": 3.84,
      "text": "So we've looked at some of these before\nwhere you've got some kind of flag that",
      "timestamp": "01:58"
    },
    {
      "start": 122.298,
      "duration": 2.73,
      "text": "you can turn on thinking or non-thinking.",
      "timestamp": "02:02"
    },
    {
      "start": 125.338,
      "duration": 1.564,
      "text": "they're claiming that it's multilingual.",
      "timestamp": "02:05"
    },
    {
      "start": 127.002,
      "duration": 2.119,
      "text": "for me, there's only six languages there.",
      "timestamp": "02:07"
    },
    {
      "start": 129.122,
      "duration": 2.58,
      "text": "They're all European languages.",
      "timestamp": "02:09"
    },
    {
      "start": 132.032,
      "duration": 3.408,
      "text": "for me personally, that's not very\nmultilingual but also interestingly,",
      "timestamp": "02:12"
    },
    {
      "start": 135.44,
      "duration": 5.28,
      "text": "this can actually use a long\ncontext of up to 128 K in here.",
      "timestamp": "02:15"
    },
    {
      "start": 141.26,
      "duration": 3.49,
      "text": "Now the thing that I find\nthat actually, is perhaps more",
      "timestamp": "02:21"
    },
    {
      "start": 144.75,
      "duration": 2.25,
      "text": "interesting than the model itself",
      "timestamp": "02:24"
    },
    {
      "start": 147.0,
      "duration": 2.49,
      "text": "is that they've released this blueprint.",
      "timestamp": "02:27"
    },
    {
      "start": 149.76,
      "duration": 5.58,
      "text": "And this blueprint is exactly how\nthey've done each step of the training.",
      "timestamp": "02:29"
    },
    {
      "start": 155.58,
      "duration": 5.73,
      "text": "So even with the open weights models\nfrom DeepSeek, from Qwen, usually",
      "timestamp": "02:35"
    },
    {
      "start": 161.31,
      "duration": 3.695,
      "text": "they're quite good in telling us\nat least roughly what they did.",
      "timestamp": "02:41"
    },
    {
      "start": 165.425,
      "duration": 3.44,
      "text": "Unlike the papers coming outta the\nproprietary labs, which really aren't",
      "timestamp": "02:45"
    },
    {
      "start": 168.865,
      "duration": 4.47,
      "text": "giving you any sort of real facts\nand basically just maybe have some",
      "timestamp": "02:48"
    },
    {
      "start": 173.335,
      "duration": 1.92,
      "text": "vague clues every now and then.",
      "timestamp": "02:53"
    },
    {
      "start": 175.675,
      "duration": 5.3,
      "text": "This is actually a full blueprint of,\nexactly what they did, for each of the",
      "timestamp": "02:55"
    },
    {
      "start": 180.975,
      "duration": 5.0,
      "text": "steps from the pre-training recipe, how\nthey set up the distributed training, how",
      "timestamp": "03:00"
    },
    {
      "start": 185.975,
      "duration": 4.46,
      "text": "they did the model architecture, right\nthrough to things like the long context",
      "timestamp": "03:05"
    },
    {
      "start": 190.435,
      "duration": 4.89,
      "text": "training and all the post-training recipes\nfor actually putting this together.",
      "timestamp": "03:10"
    },
    {
      "start": 195.825,
      "duration": 3.05,
      "text": "So if we just go through those\nquickly, they've got, some of these",
      "timestamp": "03:15"
    },
    {
      "start": 198.875,
      "duration": 1.92,
      "text": "are already up on the blog post.",
      "timestamp": "03:18"
    },
    {
      "start": 201.065,
      "duration": 3.12,
      "text": "but like I said, you've also got the\nblueprint where you can just come in and",
      "timestamp": "03:21"
    },
    {
      "start": 204.185,
      "duration": 2.55,
      "text": "actually, analyze these things yourself.",
      "timestamp": "03:24"
    },
    {
      "start": 207.095,
      "duration": 2.89,
      "text": "We can see the long\ncontext can go up to 256.",
      "timestamp": "03:27"
    },
    {
      "start": 210.005,
      "duration": 2.74,
      "text": "I'm not sure how great\nit's gonna be past 128.",
      "timestamp": "03:30"
    },
    {
      "start": 213.155,
      "duration": 3.0,
      "text": "it can use tools, which I definitely\nwanna look at the end, and we'll test",
      "timestamp": "03:33"
    },
    {
      "start": 216.155,
      "duration": 3.56,
      "text": "that when we do the code test, for\nthis and we can see that, it's got",
      "timestamp": "03:36"
    },
    {
      "start": 219.715,
      "duration": 4.45,
      "text": "this dual think reasoning, system where\nyou can turn the thinking on and off.",
      "timestamp": "03:39"
    },
    {
      "start": 224.465,
      "duration": 2.22,
      "text": "And if we jump into the blog\npost, we can just look at",
      "timestamp": "03:44"
    },
    {
      "start": 226.685,
      "duration": 1.5,
      "text": "some of the key facts in here.",
      "timestamp": "03:46"
    },
    {
      "start": 228.185,
      "duration": 2.88,
      "text": "so the architecture that they've\ngone for is actually similar to",
      "timestamp": "03:48"
    },
    {
      "start": 231.065,
      "duration": 2.984,
      "text": "some of the, LlaMA 3 architectures.",
      "timestamp": "03:51"
    },
    {
      "start": 234.519,
      "duration": 4.77,
      "text": "if we think way back now, it's been quite\na long time since LlaMA 3 was released.",
      "timestamp": "03:54"
    },
    {
      "start": 239.449,
      "duration": 4.14,
      "text": "Clearly the LlaMA 3 release was a lot\nmore successful than the LlaMA 4 release.",
      "timestamp": "03:59"
    },
    {
      "start": 243.879,
      "duration": 4.899,
      "text": "it didn't require Zuckerberg going\nout and trying to, hire as many LLM",
      "timestamp": "04:03"
    },
    {
      "start": 248.779,
      "duration": 2.41,
      "text": "researchers as possible, for doing that.",
      "timestamp": "04:08"
    },
    {
      "start": 251.759,
      "duration": 2.4,
      "text": "but we can see that in there, they've\ngot the group query attention.",
      "timestamp": "04:11"
    },
    {
      "start": 254.369,
      "duration": 2.99,
      "text": "Now, the interesting thing is they've\ngot this new thing, called NoPE.",
      "timestamp": "04:14"
    },
    {
      "start": 257.639,
      "duration": 2.36,
      "text": "and they're fully citing, in here.",
      "timestamp": "04:17"
    },
    {
      "start": 260.159,
      "duration": 2.22,
      "text": "So my brief understanding\nof this is using like",
      "timestamp": "04:20"
    },
    {
      "start": 262.429,
      "duration": 2.91,
      "text": "a causal mask rather than the\nrotary embeddings, et cetera.",
      "timestamp": "04:22"
    },
    {
      "start": 265.699,
      "duration": 2.21,
      "text": "And you can see that even\nthough they've taken certain",
      "timestamp": "04:25"
    },
    {
      "start": 267.909,
      "duration": 1.926,
      "text": "things from the LlaMA series.",
      "timestamp": "04:27"
    },
    {
      "start": 269.995,
      "duration": 4.25,
      "text": "they've also basically looked around\nand taken, ideas from things like Olmo",
      "timestamp": "04:29"
    },
    {
      "start": 274.245,
      "duration": 3.48,
      "text": "two, where they talk about, removing\nthis weight decay from embedding",
      "timestamp": "04:34"
    },
    {
      "start": 277.725,
      "duration": 2.07,
      "text": "layers to improve training stability.",
      "timestamp": "04:37"
    },
    {
      "start": 279.795,
      "duration": 0.81,
      "text": "Now that's interesting.",
      "timestamp": "04:39"
    },
    {
      "start": 280.605,
      "duration": 3.12,
      "text": "And gonna be very useful for\nanyone that does want to try and,",
      "timestamp": "04:40"
    },
    {
      "start": 283.775,
      "duration": 1.68,
      "text": "train one of these small models.",
      "timestamp": "04:43"
    },
    {
      "start": 285.635,
      "duration": 3.87,
      "text": "And I gotta say that looking\nat their GPU budget for this,",
      "timestamp": "04:45"
    },
    {
      "start": 289.725,
      "duration": 3.63,
      "text": "they had 384 H100s for 24 days.",
      "timestamp": "04:49"
    },
    {
      "start": 293.355,
      "duration": 3.84,
      "text": "That's not a crazy amount of GPUs time.",
      "timestamp": "04:53"
    },
    {
      "start": 297.195,
      "duration": 5.68,
      "text": "So actually I think that works out to\nroughly 220,000 plus GPU hours there.",
      "timestamp": "04:57"
    },
    {
      "start": 303.375,
      "duration": 4.17,
      "text": "And the interesting thing is you can\nget H100s now, the price of being",
      "timestamp": "05:03"
    },
    {
      "start": 307.545,
      "duration": 2.04,
      "text": "able to rent them out is going down.",
      "timestamp": "05:07"
    },
    {
      "start": 309.585,
      "duration": 2.16,
      "text": "So we are looking at something\nthat probably cost a few hundred",
      "timestamp": "05:09"
    },
    {
      "start": 312.435,
      "duration": 1.29,
      "text": "thousand dollars to train.",
      "timestamp": "05:12"
    },
    {
      "start": 313.875,
      "duration": 3.165,
      "text": "It is certainly not in the large\nmillions of dollars, that we've",
      "timestamp": "05:13"
    },
    {
      "start": 317.04,
      "duration": 1.83,
      "text": "seen people do in the past.",
      "timestamp": "05:17"
    },
    {
      "start": 319.37,
      "duration": 4.26,
      "text": "All right, so one of the things that is\nalways the mystery of the proprietary",
      "timestamp": "05:19"
    },
    {
      "start": 323.63,
      "duration": 5.58,
      "text": "models is how do they do their data mixes\nand how do they do their pre-training?",
      "timestamp": "05:23"
    },
    {
      "start": 329.4,
      "duration": 2.1,
      "text": "So it's really nice to see here\nthat they're talking about doing a",
      "timestamp": "05:29"
    },
    {
      "start": 331.5,
      "duration": 4.615,
      "text": "three-phase, pre-training with an\nanealing sort of at the end of that.",
      "timestamp": "05:31"
    },
    {
      "start": 336.455,
      "duration": 4.709,
      "text": "But more interestingly, looking at the\nsplits of, data as they go through this.",
      "timestamp": "05:36"
    },
    {
      "start": 341.385,
      "duration": 4.26,
      "text": "so you can see at the start it's very\nweb heavy for that sort of long phase.",
      "timestamp": "05:41"
    },
    {
      "start": 346.125,
      "duration": 3.96,
      "text": "But then in phase two and phase three,\nthey actually increased the code, they",
      "timestamp": "05:46"
    },
    {
      "start": 350.085,
      "duration": 2.92,
      "text": "increased the math, quite a lot in there.",
      "timestamp": "05:50"
    },
    {
      "start": 353.355,
      "duration": 2.13,
      "text": "just for this pre-training phrase.",
      "timestamp": "05:53"
    },
    {
      "start": 356.0,
      "duration": 4.25,
      "text": "Now after that they talk about, how they\ndid the long context stuff, and also",
      "timestamp": "05:56"
    },
    {
      "start": 360.25,
      "duration": 2.25,
      "text": "how they did the actual reasoning stuff.",
      "timestamp": "06:00"
    },
    {
      "start": 362.5,
      "duration": 4.68,
      "text": "Now, for me, looking at this sort of,\nbriefly, it does look like they've",
      "timestamp": "06:02"
    },
    {
      "start": 367.18,
      "duration": 7.16,
      "text": "heavily relied on both the DeepSeekR1,\nreasoning tracers, as well as Qwen3 for",
      "timestamp": "06:07"
    },
    {
      "start": 374.34,
      "duration": 6.06,
      "text": "creating synthetic data to be able to,\nmake some of these, reasoning traces out.",
      "timestamp": "06:14"
    },
    {
      "start": 380.4,
      "duration": 4.079,
      "text": "So it doesn't look like they've\ndone, a large amount of sort of",
      "timestamp": "06:20"
    },
    {
      "start": 384.48,
      "duration": 2.845,
      "text": "RLVR or anything like that in here.",
      "timestamp": "06:24"
    },
    {
      "start": 387.825,
      "duration": 3.634,
      "text": "At least in comparison to perhaps\nsome of the Chinese labs that these",
      "timestamp": "06:27"
    },
    {
      "start": 391.459,
      "duration": 3.344,
      "text": "people are trying to spend longer\ndoing the reinforcement learning",
      "timestamp": "06:31"
    },
    {
      "start": 394.803,
      "duration": 2.07,
      "text": "with verifiable rewards, et cetera.",
      "timestamp": "06:34"
    },
    {
      "start": 397.373,
      "duration": 3.429,
      "text": "Interestingly, they use a new\nform of alignment, in here,",
      "timestamp": "06:37"
    },
    {
      "start": 401.066,
      "duration": 2.04,
      "text": "which is a variant on DPO.",
      "timestamp": "06:41"
    },
    {
      "start": 403.656,
      "duration": 3.18,
      "text": "and finally they also do things\nlike model merging to basically",
      "timestamp": "06:43"
    },
    {
      "start": 406.836,
      "duration": 4.14,
      "text": "take various checkpoints and try and\ncreate a super checkpoint in here.",
      "timestamp": "06:46"
    },
    {
      "start": 411.543,
      "duration": 3.93,
      "text": "Now as I mentioned, they've put\nthe base model and the instruction",
      "timestamp": "06:51"
    },
    {
      "start": 415.473,
      "duration": 2.989,
      "text": "fine tune model up on Hugging Face.",
      "timestamp": "06:55"
    },
    {
      "start": 418.733,
      "duration": 2.56,
      "text": "Actually, perhaps it would've\nbeen nice for them to put up the",
      "timestamp": "06:58"
    },
    {
      "start": 421.293,
      "duration": 4.2,
      "text": "different checkpoints at the end\nof each phase of training as well.",
      "timestamp": "07:01"
    },
    {
      "start": 425.623,
      "duration": 2.63,
      "text": "I'm not sure if they, perhaps they\nwill release that in the future.",
      "timestamp": "07:05"
    },
    {
      "start": 428.253,
      "duration": 1.29,
      "text": "That would be really good to see.",
      "timestamp": "07:08"
    },
    {
      "start": 429.943,
      "duration": 4.22,
      "text": "We can also see that they've also\ngathered all the data sets here, both",
      "timestamp": "07:09"
    },
    {
      "start": 434.163,
      "duration": 4.585,
      "text": "for things like SFT for, reasoning data\nsets that they were using in there.",
      "timestamp": "07:14"
    },
    {
      "start": 439.005,
      "duration": 2.37,
      "text": "And generally it seems to me that\nwe've gotta think of this as being",
      "timestamp": "07:19"
    },
    {
      "start": 441.375,
      "duration": 3.81,
      "text": "one of the most open releases\nthat we've seen, in quite a while.",
      "timestamp": "07:21"
    },
    {
      "start": 445.429,
      "duration": 3.734,
      "text": "Okay, so let's jump in the code and\nsee how the model actually performs.",
      "timestamp": "07:25"
    },
    {
      "start": 449.664,
      "duration": 3.06,
      "text": "Okay, so jumping into the code,\nwe can see that it's really easy",
      "timestamp": "07:29"
    },
    {
      "start": 452.724,
      "duration": 1.75,
      "text": "to actually, set this model up.",
      "timestamp": "07:32"
    },
    {
      "start": 454.534,
      "duration": 2.85,
      "text": "No surprise, it's been\ncreated by Hugging Face.",
      "timestamp": "07:34"
    },
    {
      "start": 457.594,
      "duration": 2.799,
      "text": "It's gonna work with,\ntransformers out of the box.",
      "timestamp": "07:37"
    },
    {
      "start": 460.664,
      "duration": 3.23,
      "text": "it also works with SG Lang and with, VLLM.",
      "timestamp": "07:40"
    },
    {
      "start": 464.264,
      "duration": 2.34,
      "text": "but anyway, in here you can see\nI've just loaded up the model.",
      "timestamp": "07:44"
    },
    {
      "start": 466.944,
      "duration": 4.9,
      "text": "if we want to do, reasoning outputs,\nwe can just basically pass in a",
      "timestamp": "07:46"
    },
    {
      "start": 471.844,
      "duration": 6.0,
      "text": "prompt and you'll see we'll get the\nstandard sort of DeepSeek think tags,",
      "timestamp": "07:51"
    },
    {
      "start": 478.154,
      "duration": 3.459,
      "text": "where in this case we've got, quite\na long think tag, going on there.",
      "timestamp": "07:58"
    },
    {
      "start": 481.884,
      "duration": 1.77,
      "text": "And then we've got the answer coming out.",
      "timestamp": "08:01"
    },
    {
      "start": 484.014,
      "duration": 3.87,
      "text": "If we want to get it with no\nreasoning, in the system prompt,",
      "timestamp": "08:04"
    },
    {
      "start": 487.884,
      "duration": 2.43,
      "text": "we just need to pass\nin this slash no think.",
      "timestamp": "08:07"
    },
    {
      "start": 490.669,
      "duration": 0.69,
      "text": "in there.",
      "timestamp": "08:10"
    },
    {
      "start": 491.729,
      "duration": 4.19,
      "text": "and then you'll see that we just get,\nan answer out with no thinking tags,",
      "timestamp": "08:11"
    },
    {
      "start": 495.969,
      "duration": 1.92,
      "text": "just the final answer out there.",
      "timestamp": "08:15"
    },
    {
      "start": 498.552,
      "duration": 3.6,
      "text": "Alright, so you can manipulate\nthe thinking with the system",
      "timestamp": "08:18"
    },
    {
      "start": 502.152,
      "duration": 2.46,
      "text": "prompt a bit by referring into it.",
      "timestamp": "08:22"
    },
    {
      "start": 504.892,
      "duration": 4.46,
      "text": "and the thinking, will often be very\nlong, Now, unfortunately, I wasn't",
      "timestamp": "08:24"
    },
    {
      "start": 509.352,
      "duration": 4.299,
      "text": "able to get it to number the thinking\nand break it down into sections.",
      "timestamp": "08:29"
    },
    {
      "start": 514.042,
      "duration": 2.219,
      "text": "which is the same for the DeepSeek ones.",
      "timestamp": "08:34"
    },
    {
      "start": 516.562,
      "duration": 4.44,
      "text": "With the way that they train that\nmodel and the Qwen3 model, they're",
      "timestamp": "08:36"
    },
    {
      "start": 521.002,
      "duration": 6.09,
      "text": "really using this RLVR by verifiable\nrewards, which doesn't give you a lot",
      "timestamp": "08:41"
    },
    {
      "start": 527.092,
      "duration": 4.577,
      "text": "of control over that long chain of\nthought to get to the right answer.",
      "timestamp": "08:47"
    },
    {
      "start": 532.029,
      "duration": 4.08,
      "text": "You see this being actually different\nin the Gemini models and some of the",
      "timestamp": "08:52"
    },
    {
      "start": 536.109,
      "duration": 4.86,
      "text": "other proprietary, reasoning models out\nthere, which kind of sectionize, their",
      "timestamp": "08:56"
    },
    {
      "start": 540.969,
      "duration": 2.18,
      "text": "thinking, tokens as they go through.",
      "timestamp": "09:00"
    },
    {
      "start": 543.539,
      "duration": 4.056,
      "text": "Anyway, here we can see that okay, it\ndid a nice long think, And then it gave",
      "timestamp": "09:03"
    },
    {
      "start": 547.595,
      "duration": 3.443,
      "text": "us quite a detailed, response out, here.",
      "timestamp": "09:07"
    },
    {
      "start": 551.522,
      "duration": 5.01,
      "text": "Now if we look at sometimes like this,\nso this is a code generation one.",
      "timestamp": "09:11"
    },
    {
      "start": 557.072,
      "duration": 2.1,
      "text": "it will just generate empty thinking.",
      "timestamp": "09:17"
    },
    {
      "start": 559.542,
      "duration": 3.146,
      "text": "so I found this on a few sort of examples.",
      "timestamp": "09:19"
    },
    {
      "start": 562.799,
      "duration": 5.896,
      "text": "I'm not sure you know what they expect,\nthe behavior is, for this, maybe if we",
      "timestamp": "09:22"
    },
    {
      "start": 568.695,
      "duration": 5.47,
      "text": "had asked some things before we actually\nsent in the Python, function definition,",
      "timestamp": "09:28"
    },
    {
      "start": 574.502,
      "duration": 1.68,
      "text": "we would've got a better response out.",
      "timestamp": "09:34"
    },
    {
      "start": 576.682,
      "duration": 3.27,
      "text": "So I do notice this, for a number\nof responses where you will",
      "timestamp": "09:36"
    },
    {
      "start": 579.952,
      "duration": 1.95,
      "text": "get empty thinking out there.",
      "timestamp": "09:39"
    },
    {
      "start": 582.402,
      "duration": 3.69,
      "text": "I gotta say though, for such\na small model, the thinking",
      "timestamp": "09:42"
    },
    {
      "start": 586.152,
      "duration": 1.859,
      "text": "turns out to be quite good.",
      "timestamp": "09:46"
    },
    {
      "start": 588.342,
      "duration": 5.0,
      "text": "I can't help but feel that perhaps it\nis just fine tuned in from DeepSeek and",
      "timestamp": "09:48"
    },
    {
      "start": 593.342,
      "duration": 5.129,
      "text": "Qwen3 as opposed to actually getting\nthe model, to do thinking itself.",
      "timestamp": "09:53"
    },
    {
      "start": 598.471,
      "duration": 1.98,
      "text": "It's just extended chain of thought.",
      "timestamp": "09:58"
    },
    {
      "start": 600.731,
      "duration": 3.0,
      "text": "but that said, we can look at, you\nknow, so a question like this, write",
      "timestamp": "10:00"
    },
    {
      "start": 603.731,
      "duration": 2.76,
      "text": "a detailed breakdown of building\na successful lemonade stand.",
      "timestamp": "10:03"
    },
    {
      "start": 606.771,
      "duration": 1.68,
      "text": "the thinking actually\nis quite good, right?",
      "timestamp": "10:06"
    },
    {
      "start": 608.451,
      "duration": 3.08,
      "text": "it goes through the different\nelements before giving a",
      "timestamp": "10:08"
    },
    {
      "start": 611.711,
      "duration": 2.78,
      "text": "detailed plan, out, for this.",
      "timestamp": "10:11"
    },
    {
      "start": 614.491,
      "duration": 5.454,
      "text": "Now, when we compare this to a version\nwith no thinking, you can see in",
      "timestamp": "10:14"
    },
    {
      "start": 619.945,
      "duration": 2.4,
      "text": "this case, the plan is much simpler.",
      "timestamp": "10:19"
    },
    {
      "start": 622.595,
      "duration": 2.94,
      "text": "definitely not as well thought\nout and stuff like that.",
      "timestamp": "10:22"
    },
    {
      "start": 625.535,
      "duration": 4.57,
      "text": "So it does show that the thinking, will\nhelp for, certain kind of responses.",
      "timestamp": "10:25"
    },
    {
      "start": 630.335,
      "duration": 4.56,
      "text": "often you will also get the model\nwanting to give some justification",
      "timestamp": "10:30"
    },
    {
      "start": 634.895,
      "duration": 4.3,
      "text": "at the end or to give, so in this\ncase additional tips at the end.",
      "timestamp": "10:34"
    },
    {
      "start": 639.695,
      "duration": 0.3,
      "text": "Okay.",
      "timestamp": "10:39"
    },
    {
      "start": 640.055,
      "duration": 2.64,
      "text": "Another one, old question was\nthe difference between llama,",
      "timestamp": "10:40"
    },
    {
      "start": 642.715,
      "duration": 3.12,
      "text": "Vicuna, alpaca Again here, it\njust dropped out the thinking.",
      "timestamp": "10:42"
    },
    {
      "start": 645.835,
      "duration": 4.21,
      "text": "I'm not sure, why, but it gave us\ndetailed, a very detailed, sort of",
      "timestamp": "10:45"
    },
    {
      "start": 650.045,
      "duration": 4.3,
      "text": "response, probably on par with some\nof the models that we'd seen before.",
      "timestamp": "10:50"
    },
    {
      "start": 654.68,
      "duration": 3.805,
      "text": "Now one area where I thought it was a\nbit, not so great here where I've asked",
      "timestamp": "10:54"
    },
    {
      "start": 658.485,
      "duration": 1.38,
      "text": "it, what is the capital of England?",
      "timestamp": "10:58"
    },
    {
      "start": 660.115,
      "duration": 3.893,
      "text": "I've left the thinking on, we get\na huge amount of thinking and then",
      "timestamp": "11:00"
    },
    {
      "start": 664.008,
      "duration": 4.9,
      "text": "we get a step by step, breakdown of\nthe answer like the justification",
      "timestamp": "11:04"
    },
    {
      "start": 668.908,
      "duration": 1.47,
      "text": "that I was talking about before.",
      "timestamp": "11:08"
    },
    {
      "start": 670.878,
      "duration": 4.517,
      "text": "Another example here of the thinking for\nthe Geoffrey Hinton question, gives some",
      "timestamp": "11:10"
    },
    {
      "start": 675.395,
      "duration": 2.18,
      "text": "nice rationale at the end, for this.",
      "timestamp": "11:15"
    },
    {
      "start": 677.785,
      "duration": 4.109,
      "text": "Does really well on the GSM 8K\nstuff, not surprisingly now.",
      "timestamp": "11:17"
    },
    {
      "start": 682.215,
      "duration": 4.81,
      "text": "it is kind of amazing to think\nthat a 3B model is, doing so",
      "timestamp": "11:22"
    },
    {
      "start": 687.025,
      "duration": 2.4,
      "text": "much better on this kind of stuff",
      "timestamp": "11:27"
    },
    {
      "start": 689.525,
      "duration": 5.22,
      "text": "compared to going back a year, 18 months\nago where we were looking at 14 B models",
      "timestamp": "11:29"
    },
    {
      "start": 694.745,
      "duration": 2.29,
      "text": "that couldn't do, a lot of these things.",
      "timestamp": "11:34"
    },
    {
      "start": 697.425,
      "duration": 2.28,
      "text": "Alright, finally, just to\nfinish up the tool use.",
      "timestamp": "11:37"
    },
    {
      "start": 699.705,
      "duration": 3.456,
      "text": "So, one of the things that I find,\nmost interesting about this model",
      "timestamp": "11:39"
    },
    {
      "start": 703.161,
      "duration": 2.28,
      "text": "is that it can do function calling.",
      "timestamp": "11:43"
    },
    {
      "start": 705.721,
      "duration": 4.69,
      "text": "and that it's supposedly been,\nprimed a bit for agentic use.",
      "timestamp": "11:45"
    },
    {
      "start": 710.971,
      "duration": 4.3,
      "text": "So when we come in and look at the tool\nuse, we can basically just define a tool.",
      "timestamp": "11:50"
    },
    {
      "start": 715.521,
      "duration": 4.3,
      "text": "you can see here that we've just defined\nthis, the schema for the tool in here.",
      "timestamp": "11:55"
    },
    {
      "start": 719.871,
      "duration": 5.064,
      "text": "we can then, pass in a, message\nand sure enough, what it will do is",
      "timestamp": "11:59"
    },
    {
      "start": 724.935,
      "duration": 2.88,
      "text": "that the message, this is showing\nall the data that's coming out.",
      "timestamp": "12:04"
    },
    {
      "start": 728.005,
      "duration": 4.29,
      "text": "sure enough, the message will actually\nthen include a tool call in it.",
      "timestamp": "12:08"
    },
    {
      "start": 732.725,
      "duration": 3.179,
      "text": "so in this case, the tool call is\nthe get weather, and it worked out",
      "timestamp": "12:12"
    },
    {
      "start": 735.905,
      "duration": 3.36,
      "text": "that it was Copenhagen, which is\nwhat it should have been for this.",
      "timestamp": "12:15"
    },
    {
      "start": 739.575,
      "duration": 2.98,
      "text": "so I tried this out for a\nnumber of different, functions.",
      "timestamp": "12:19"
    },
    {
      "start": 742.555,
      "duration": 4.185,
      "text": "So doing like a search thing, where\nwe can say, okay, when will OpenAI",
      "timestamp": "12:22"
    },
    {
      "start": 746.761,
      "duration": 1.38,
      "text": "release the open weights model?",
      "timestamp": "12:26"
    },
    {
      "start": 748.141,
      "duration": 1.17,
      "text": "What are the rumors?",
      "timestamp": "12:28"
    },
    {
      "start": 749.641,
      "duration": 2.969,
      "text": "and we can see, sure enough, we're\ntaking in that one tool there.",
      "timestamp": "12:29"
    },
    {
      "start": 753.151,
      "duration": 2.764,
      "text": "and, it's got some\ninstructions of how to do it.",
      "timestamp": "12:33"
    },
    {
      "start": 756.288,
      "duration": 1.92,
      "text": "And sure enough, it\nreturns back a tool call.",
      "timestamp": "12:36"
    },
    {
      "start": 758.368,
      "duration": 2.46,
      "text": "so the tool call in this\ncase is the search web.",
      "timestamp": "12:38"
    },
    {
      "start": 761.038,
      "duration": 3.193,
      "text": "And the arguments it's come up\nwith is OpenAI, open weights",
      "timestamp": "12:41"
    },
    {
      "start": 764.231,
      "duration": 1.77,
      "text": "model, release date rumors.",
      "timestamp": "12:44"
    },
    {
      "start": 766.401,
      "duration": 3.375,
      "text": "So it actually is working\nout what are the keywords?",
      "timestamp": "12:46"
    },
    {
      "start": 769.956,
      "duration": 3.219,
      "text": "So I tried this for a few different\nthings, for things like, okay, who",
      "timestamp": "12:49"
    },
    {
      "start": 773.176,
      "duration": 1.83,
      "text": "won the Nobel Prize in chemistry?",
      "timestamp": "12:53"
    },
    {
      "start": 775.246,
      "duration": 5.289,
      "text": "Now, technically if it's cutoff date\nis June 2025 should actually know that.",
      "timestamp": "12:55"
    },
    {
      "start": 780.535,
      "duration": 3.596,
      "text": "But in this case, it decided,\nokay, let's do a search 24",
      "timestamp": "13:00"
    },
    {
      "start": 784.131,
      "duration": 2.03,
      "text": "Nobel Prize chemistry winner.",
      "timestamp": "13:04"
    },
    {
      "start": 786.358,
      "duration": 1.289,
      "text": "I actually think that's good, right?",
      "timestamp": "13:06"
    },
    {
      "start": 787.648,
      "duration": 4.11,
      "text": "that I don't expect a 3B model\nto be really good with facts.",
      "timestamp": "13:07"
    },
    {
      "start": 791.758,
      "duration": 3.09,
      "text": "I feel it's like, better for it\nto actually try things out there.",
      "timestamp": "13:11"
    },
    {
      "start": 795.238,
      "duration": 0.27,
      "text": "Alright.",
      "timestamp": "13:15"
    },
    {
      "start": 795.508,
      "duration": 3.99,
      "text": "And then finally, if we are passing\nin multiple tools here, I've got",
      "timestamp": "13:15"
    },
    {
      "start": 799.498,
      "duration": 3.09,
      "text": "the search web and the weather,\nand we ask it something that",
      "timestamp": "13:19"
    },
    {
      "start": 802.588,
      "duration": 1.65,
      "text": "really shouldn't use either tool.",
      "timestamp": "13:22"
    },
    {
      "start": 804.238,
      "duration": 2.159,
      "text": "We want it to not use a tool, right?",
      "timestamp": "13:24"
    },
    {
      "start": 806.398,
      "duration": 3.16,
      "text": "We want it to know that, okay,\nit should just respond, to this.",
      "timestamp": "13:26"
    },
    {
      "start": 809.828,
      "duration": 2.43,
      "text": "Now, I have mixed responses with this.",
      "timestamp": "13:29"
    },
    {
      "start": 812.258,
      "duration": 3.64,
      "text": "Sometimes, it works really well\nas you see here where, okay,",
      "timestamp": "13:32"
    },
    {
      "start": 815.898,
      "duration": 1.38,
      "text": "we've passed in the tools.",
      "timestamp": "13:35"
    },
    {
      "start": 817.603,
      "duration": 1.35,
      "text": "we've passed in something.",
      "timestamp": "13:37"
    },
    {
      "start": 818.953,
      "duration": 1.5,
      "text": "We didn't get a tool call back.",
      "timestamp": "13:38"
    },
    {
      "start": 820.453,
      "duration": 1.2,
      "text": "We didn't get any thinking.",
      "timestamp": "13:40"
    },
    {
      "start": 821.653,
      "duration": 3.63,
      "text": "We just got, I'm an AI, I don't\nhave feelings, but I'm here to help.",
      "timestamp": "13:41"
    },
    {
      "start": 825.313,
      "duration": 1.23,
      "text": "we got a response back.",
      "timestamp": "13:45"
    },
    {
      "start": 826.843,
      "duration": 1.83,
      "text": "often it would be like\nthis every now and then.",
      "timestamp": "13:46"
    },
    {
      "start": 828.673,
      "duration": 5.409,
      "text": "I found that okay, it will try to use\nthe search tool, for things that perhaps",
      "timestamp": "13:48"
    },
    {
      "start": 834.083,
      "duration": 1.78,
      "text": "maybe you don't want it to use it as tool.",
      "timestamp": "13:54"
    },
    {
      "start": 835.883,
      "duration": 4.35,
      "text": "Now, that could also be, down to my\ndescription in here I've got, allows",
      "timestamp": "13:55"
    },
    {
      "start": 840.233,
      "duration": 3.26,
      "text": "you to use the search the internet,\nfind useful and up-to-date information.",
      "timestamp": "14:00"
    },
    {
      "start": 843.673,
      "duration": 3.979,
      "text": "you could, play around with that and I\nthink you certainly want to do that if",
      "timestamp": "14:03"
    },
    {
      "start": 847.653,
      "duration": 1.69,
      "text": "you're planning on using this thing.",
      "timestamp": "14:07"
    },
    {
      "start": 849.646,
      "duration": 3.424,
      "text": "So the model is out also on, Olama.",
      "timestamp": "14:09"
    },
    {
      "start": 853.31,
      "duration": 2.7,
      "text": "I think this is a custom version,\nsomeone's uploaded, but my guess",
      "timestamp": "14:13"
    },
    {
      "start": 856.01,
      "duration": 3.01,
      "text": "is we'll see probably an official\nversion, up there as well.",
      "timestamp": "14:16"
    },
    {
      "start": 859.47,
      "duration": 3.179,
      "text": "and I'm sure it's just a matter of time\nbefore you see it on LM Studio as well.",
      "timestamp": "14:19"
    },
    {
      "start": 863.17,
      "duration": 3.919,
      "text": "Overall, this is, a really\ninteresting release from Hugging Face.",
      "timestamp": "14:23"
    },
    {
      "start": 867.15,
      "duration": 2.86,
      "text": "And, I really think they\nshould be commended for this.",
      "timestamp": "14:27"
    },
    {
      "start": 870.01,
      "duration": 4.643,
      "text": "they've not only released, an interesting\nmodel, but they've also released this",
      "timestamp": "14:30"
    },
    {
      "start": 874.653,
      "duration": 5.08,
      "text": "really nice blueprint of how, they\ncreated it, what they did at each step.",
      "timestamp": "14:34"
    },
    {
      "start": 880.103,
      "duration": 3.09,
      "text": "my guess is that there's probably\na full paper coming with maybe even",
      "timestamp": "14:40"
    },
    {
      "start": 883.193,
      "duration": 1.77,
      "text": "more details, which will be great.",
      "timestamp": "14:43"
    },
    {
      "start": 885.463,
      "duration": 3.03,
      "text": "Hopefully they will release the\ncheckpoints for like each of",
      "timestamp": "14:45"
    },
    {
      "start": 888.493,
      "duration": 1.44,
      "text": "the phases and stuff like that.",
      "timestamp": "14:48"
    },
    {
      "start": 889.933,
      "duration": 3.966,
      "text": "So people could look at testing\ndifferent ideas, for example, if you",
      "timestamp": "14:49"
    },
    {
      "start": 893.9,
      "duration": 3.205,
      "text": "wanted to change something in the\nphase two, you don't wanna have to",
      "timestamp": "14:53"
    },
    {
      "start": 897.105,
      "duration": 1.799,
      "text": "do the whole phase one by yourself.",
      "timestamp": "14:57"
    },
    {
      "start": 898.905,
      "duration": 4.02,
      "text": "If you could just take their checkpoint\nfor that and then try out, new ideas",
      "timestamp": "14:58"
    },
    {
      "start": 902.925,
      "duration": 3.27,
      "text": "for the phase two, phase three,\nor for the different sections of",
      "timestamp": "15:02"
    },
    {
      "start": 906.195,
      "duration": 1.65,
      "text": "the post-training stuff as well.",
      "timestamp": "15:06"
    },
    {
      "start": 907.845,
      "duration": 2.619,
      "text": "There's quite a lot of, tokens\nthat they trained on in there.",
      "timestamp": "15:07"
    },
    {
      "start": 910.915,
      "duration": 0.9,
      "text": "Anyway, check it out.",
      "timestamp": "15:10"
    },
    {
      "start": 911.815,
      "duration": 4.2,
      "text": "Let me know in the comments, are\nyou actually still looking to use,",
      "timestamp": "15:11"
    },
    {
      "start": 916.215,
      "duration": 4.69,
      "text": "small local models, on your own\nmachine or are you more moving to",
      "timestamp": "15:16"
    },
    {
      "start": 920.905,
      "duration": 4.44,
      "text": "the really cheap proprietary models\nthat can do all of these things?",
      "timestamp": "15:20"
    },
    {
      "start": 925.535,
      "duration": 2.52,
      "text": "this is certainly one of the things\nI'm looking at more and more.",
      "timestamp": "15:25"
    },
    {
      "start": 928.295,
      "duration": 3.99,
      "text": "and talking to other people, I see, use\ncases for sort of both systems and stuff.",
      "timestamp": "15:28"
    },
    {
      "start": 932.335,
      "duration": 2.55,
      "text": "So I'd love to hear your thoughts\nabout that in the comments.",
      "timestamp": "15:32"
    },
    {
      "start": 935.245,
      "duration": 3.33,
      "text": "Anyway, as always, if you found the video\nuseful, please click like and subscribe,",
      "timestamp": "15:35"
    },
    {
      "start": 938.605,
      "duration": 1.68,
      "text": "and I will talk to you in the next video.",
      "timestamp": "15:38"
    },
    {
      "start": 940.525,
      "duration": 0.75,
      "text": "Bye for now.",
      "timestamp": "15:40"
    }
  ],
  "extraction_timestamp": "2025-07-17T17:19:25",
  "processing_type": "single_video"
}