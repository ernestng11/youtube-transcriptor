{
  "video_id": "QMh-gG40UGA",
  "video_title": "Donghao Huang - Optimizing and Benchmarking Open-Source LLMs - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=QMh-gG40UGA",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T06:27:00+00:00",
  "duration_seconds": null,
  "view_count": 102,
  "like_count": 2,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: Optimizing and Benchmarking Open-Source LLMs for Enterprise Applications\n\nSpeaker:\nDonghao Huang, VP, R&D @ Mastercard\n\nStage: WEKA Stage\n#superai #mastercard #benchmarks #enterpriseai #llms \n\nRecorded on 19 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 264,
    "aggregated_text": "so I'm going to share my name is Dona Huang currently I'm working as a masterard R&D team at um leading the global emerging technology for AI machine learning and generative AI so with 25 years of software development I hold 25 more than 25 uh granted patent and another more pending patent file pending and published 14 research papers uh beyond that I'm also pursuing a part-time degree in generative AI for enterprise applications at Singapore Management University which complements my uh graduate study from AUS Pin University and Siha University the combination of the academic foundation and my hands-on experience in fintech applications enable me to develop transformative innovations for at the intersection of emerging technologies and the financial applications just to start with the uh where we uh the house research starts so the this is the product uh um along launched we uh announced last year in 2020 many 2020 in October the we call Mascard product on boarding assistant and this year in April our Mascard has been awarded the 2025 excellence in custom service for technology of the year because of the the excellent per performance of this JI assistant for product on boarding so during the initial development of the um product GI product on boarding system we observe a very se severe problem uh from open source non models because when when we started we always use the open s open AI APIs and which worked very well with our initial codebase but once we tried to switch to open source models we find out uh some serious uh rep text repetition problem so let's show this the first one is talking about this when try I'm not going to try to just talk about the B model you can see the sentence and the electrons are in a state of energy levels are repeat repeat repeat until for a long time and for long sentences make the responses unreadable and the other case here is that you can see um um repetitive the new lines if you really switch in the kind of UI eye experience you can see the screen go up up with all blank content so which will deliver very bad user experience to address this problem uh inference in inference engines like hockey transformers they do introduce the repetition penalty parameters um but we during the experimentations we realized that there's a it's very difficult to set the right par value for the R repetition penalty parameter or RPP if you set it too low then you have a repetition problem if set too high then the sentence might just cut off and become a incomplete responses either way you render the user experience very bad badly so the this come to the core research problem we try to address so how do we achieve the balance between reducing excess of repetition while still maintaining the um coherence and the relevance of the output for by the open source larger language models to address this we develop a novel metric called repetition or web performance where we are trying to balance the repetition and performance uh in open source and IMF let let me explain with uh this example to see how our RAP works so if you ask a large language model say what currency is used in Panama the model actually output the correct answer the panomin pan many baloba and United US dollars but you can see very clearly here that US dollars actually repeated twice and then also um uh kind more dots nec non-necessary dots uh uh here so we we develop a efficient um repetition detection algorithm uh to classify the repetition probably into long word character like the dot dot dots and also the United and the tax repetition like you have United US United States dollars here and then eventually we calculate the repetition ratio to to define how severe the repetition in the response is in and then we apply a cubic penalty function to the original 100% of accuracy we get around 25% of rep so even if the even the results initial performance is 100% it will get down to 25% so which give us a way to optimize to um the RPP values later we can see so to benchmark all the open source models we we had developed we use the two different modes why is the we try to devel use use rack and we fix the retrieval component and just swap out all the open source models and so that we can tell the differences between the large order for the generation piece because we don't really care about the retrieval accuracy yet for for this specific purpose and we also try to um rem remove the retrieval component totally just try to see um direct answer from Language models so see whether the the repetition patterns are still there so we call it the wrong rack so we systematically change the repetition par parameter penalty parameters from one to 1.3 with the increments of 0.02 this systematic approach can allows us to map out the granular differences between the different parameters and across all the models we evaluated so to evaluate the the the models we also use the very two widely used the data set for the um natur le natural language and processing um one is a web QSP which is a widely used benchmark for knowledge based question answering tasks we use performance metrics F1 and the another one is from Microsoft Microsoft Marco data set is a comprehensive data set for rendering reading comprehension and question answering so we use a bird F1 as a performance metric so the key key keep because we know that we to fine-tune the the get the better performance we will need to do the prompt engineering so we also try to run different promping strategy the first one is we call it the rag generic prompt which is the default prompt engine uh template introduced by n in in nchen which is a widely used um open source framework for large language model application development so you can see it's just natural language without any templates so we when we try to do the long rag we notice okay that for each specific RM family actually they have a kind of so-called chat template we need to squeeze the date the text into the right template like this one so later we also we think about okay maybe maybe we can also do the same thing we call rag chat template putting the all the rack information into the this uh chat template for different RM so we will see how whether this rack temp chat template will get us better performance so here this is uh the the results i I don't want to get into the detail due to the time in diff time constraint but the very important observance observation here is that if we see we can see that react chat template I will consistently suppress the red text repetition 10 times smaller than the generic prompt so that's is the very first observation actually from this research findings when I talk to my our product development uh team and say hey you you you guys probably need to switch from the default um um nin prompt template to this chhat template and they initially they don't really believe so and uh then we we I I use that data set and use my methodology and prove that they they also have uh the repetition problem with the current the their current implementation and eventually the whole product development switch to this chat template approach another another observation is that if you the RPP normally will repress the temp repetition you can see the downward trend for almost all across all the performance metrics there but you might also uh notice that it it impact profetition as well as performance so to get the balance we need to tune the RPP so that we will have a balance so this is the way we are trying to do the balance between the performance center and repetition so the blue color and it shows that the best per original performance the RP value the yellow color shows the best the RP value for the best R value so if you see the just see the first subplot you can see that at the beginning um for the the blue color actually it has around 26% of the repetition um ratio is this which means a very high um repetition but if we choose the yellow color the RPP it change to the R change is suppressed to 2% which is a um decrement of uh 92% of the repetition uh behaviors but if you see the performance drop here is just 0.022 points so this with this we are we can systematically um find the balance uh the best RP for to to make the to balance the performance and the repetition to summarize we developed a repetition detection algorithm which to measure the repetition in generated text using uh repetition ratio and this algorithm we I don't have time to discuss the details and but it perform outperformed the uh previous versions in the literature by 20% in terms of the efficiency and we also introduce an repetition of web performance which allows us to do the optimization of the large language model while maintaining content uh quality so key takeaway here is that use chat template for whatever uh application you are going to build we we we talk about rag but in the our research paper also talk about translation we also you find some uh same pattern here and that we could see the difference in RP where not only impact the text repetition but also impact the performance so we need to make sure that have find the right RPP so that you can you don't uh sacrifice performance while you are trying to suppress the tax repetition so we our RP is a introduced a new way to balance the performance and repetition what the all these findings have have been published in local conference which was held in US last in from April 29 to May 4 so you can see the link here also is now public available in on online thank you for your attention if interested to discuss uh more with me and feel free to connect via LinkedIn or send me an email thank you",
    "text_length": 9684,
    "word_count": 1779
  },
  "segments": [
    {
      "start": 7.359,
      "duration": 2.24,
      "text": "so I'm going to share my name is Dona",
      "timestamp": "00:07"
    },
    {
      "start": 9.599,
      "duration": 1.681,
      "text": "Huang currently I'm working as a",
      "timestamp": "00:09"
    },
    {
      "start": 11.28,
      "duration": 3.2,
      "text": "masterard R&D team at",
      "timestamp": "00:11"
    },
    {
      "start": 14.48,
      "duration": 2.96,
      "text": "um leading the",
      "timestamp": "00:14"
    },
    {
      "start": 17.44,
      "duration": 2.72,
      "text": "global emerging technology for AI",
      "timestamp": "00:17"
    },
    {
      "start": 20.16,
      "duration": 2.48,
      "text": "machine learning and generative AI so",
      "timestamp": "00:20"
    },
    {
      "start": 22.64,
      "duration": 3.2,
      "text": "with 25 years of software development I",
      "timestamp": "00:22"
    },
    {
      "start": 25.84,
      "duration": 3.359,
      "text": "hold 25 more than 25 uh granted patent",
      "timestamp": "00:25"
    },
    {
      "start": 29.199,
      "duration": 3.121,
      "text": "and another more pending patent file",
      "timestamp": "00:29"
    },
    {
      "start": 32.32,
      "duration": 4.16,
      "text": "pending and published 14 research papers",
      "timestamp": "00:32"
    },
    {
      "start": 36.48,
      "duration": 2.8,
      "text": "uh beyond that I'm also pursuing a",
      "timestamp": "00:36"
    },
    {
      "start": 39.28,
      "duration": 2.48,
      "text": "part-time degree in generative AI for",
      "timestamp": "00:39"
    },
    {
      "start": 41.76,
      "duration": 2.799,
      "text": "enterprise applications at Singapore",
      "timestamp": "00:41"
    },
    {
      "start": 44.559,
      "duration": 2.561,
      "text": "Management University which complements",
      "timestamp": "00:44"
    },
    {
      "start": 47.12,
      "duration": 4.8,
      "text": "my uh graduate study from AUS Pin",
      "timestamp": "00:47"
    },
    {
      "start": 51.92,
      "duration": 2.56,
      "text": "University and Siha University the",
      "timestamp": "00:51"
    },
    {
      "start": 54.48,
      "duration": 3.44,
      "text": "combination of the academic foundation",
      "timestamp": "00:54"
    },
    {
      "start": 57.92,
      "duration": 3.04,
      "text": "and my hands-on experience in fintech",
      "timestamp": "00:57"
    },
    {
      "start": 60.96,
      "duration": 3.12,
      "text": "applications enable me to develop",
      "timestamp": "01:00"
    },
    {
      "start": 64.08,
      "duration": 1.92,
      "text": "transformative",
      "timestamp": "01:04"
    },
    {
      "start": 66.0,
      "duration": 2.88,
      "text": "innovations for at the intersection of",
      "timestamp": "01:06"
    },
    {
      "start": 68.88,
      "duration": 2.48,
      "text": "emerging technologies and the financial",
      "timestamp": "01:08"
    },
    {
      "start": 71.36,
      "duration": 2.48,
      "text": "applications",
      "timestamp": "01:11"
    },
    {
      "start": 73.84,
      "duration": 3.84,
      "text": "just to start with the uh where we uh",
      "timestamp": "01:13"
    },
    {
      "start": 77.68,
      "duration": 2.96,
      "text": "the house research starts so the this is",
      "timestamp": "01:17"
    },
    {
      "start": 80.64,
      "duration": 4.88,
      "text": "the product uh um along launched we uh",
      "timestamp": "01:20"
    },
    {
      "start": 85.52,
      "duration": 3.68,
      "text": "announced last year in 2020 many 2020 in",
      "timestamp": "01:25"
    },
    {
      "start": 89.2,
      "duration": 3.36,
      "text": "October the we call Mascard product on",
      "timestamp": "01:29"
    },
    {
      "start": 92.56,
      "duration": 2.559,
      "text": "boarding assistant and this year in",
      "timestamp": "01:32"
    },
    {
      "start": 95.119,
      "duration": 3.201,
      "text": "April our Mascard has been awarded the",
      "timestamp": "01:35"
    },
    {
      "start": 98.32,
      "duration": 3.2,
      "text": "2025 excellence in custom service for",
      "timestamp": "01:38"
    },
    {
      "start": 101.52,
      "duration": 2.0,
      "text": "technology of the year because of the",
      "timestamp": "01:41"
    },
    {
      "start": 103.52,
      "duration": 4.0,
      "text": "the excellent per performance of this JI",
      "timestamp": "01:43"
    },
    {
      "start": 107.52,
      "duration": 3.76,
      "text": "assistant for product on boarding",
      "timestamp": "01:47"
    },
    {
      "start": 111.28,
      "duration": 3.68,
      "text": "so during the initial development of the",
      "timestamp": "01:51"
    },
    {
      "start": 114.96,
      "duration": 2.96,
      "text": "um product GI product on boarding system",
      "timestamp": "01:54"
    },
    {
      "start": 117.92,
      "duration": 3.36,
      "text": "we observe a very se severe problem uh",
      "timestamp": "01:57"
    },
    {
      "start": 121.28,
      "duration": 2.799,
      "text": "from open source non models because when",
      "timestamp": "02:01"
    },
    {
      "start": 124.079,
      "duration": 2.561,
      "text": "when we started we always use the open s",
      "timestamp": "02:04"
    },
    {
      "start": 126.64,
      "duration": 3.2,
      "text": "open AI APIs and which worked very well",
      "timestamp": "02:06"
    },
    {
      "start": 129.84,
      "duration": 2.24,
      "text": "with our initial codebase but once we",
      "timestamp": "02:09"
    },
    {
      "start": 132.08,
      "duration": 2.879,
      "text": "tried to switch to open source models we",
      "timestamp": "02:12"
    },
    {
      "start": 134.959,
      "duration": 2.961,
      "text": "find out uh some serious uh rep text",
      "timestamp": "02:14"
    },
    {
      "start": 137.92,
      "duration": 2.959,
      "text": "repetition problem so let's show this",
      "timestamp": "02:17"
    },
    {
      "start": 140.879,
      "duration": 2.72,
      "text": "the first one is talking about this when",
      "timestamp": "02:20"
    },
    {
      "start": 143.599,
      "duration": 2.801,
      "text": "try I'm not going to try to just talk",
      "timestamp": "02:23"
    },
    {
      "start": 146.4,
      "duration": 1.919,
      "text": "about the B model you can see the",
      "timestamp": "02:26"
    },
    {
      "start": 148.319,
      "duration": 2.401,
      "text": "sentence and the electrons are in a",
      "timestamp": "02:28"
    },
    {
      "start": 150.72,
      "duration": 2.0,
      "text": "state of energy levels are repeat repeat",
      "timestamp": "02:30"
    },
    {
      "start": 152.72,
      "duration": 2.879,
      "text": "repeat until for a long time and for",
      "timestamp": "02:32"
    },
    {
      "start": 155.599,
      "duration": 3.201,
      "text": "long sentences make the responses",
      "timestamp": "02:35"
    },
    {
      "start": 158.8,
      "duration": 1.519,
      "text": "unreadable",
      "timestamp": "02:38"
    },
    {
      "start": 160.319,
      "duration": 2.56,
      "text": "and the other case here is that you can",
      "timestamp": "02:40"
    },
    {
      "start": 162.879,
      "duration": 3.601,
      "text": "see um um repetitive the new lines if",
      "timestamp": "02:42"
    },
    {
      "start": 166.48,
      "duration": 2.64,
      "text": "you really switch in the kind of UI eye",
      "timestamp": "02:46"
    },
    {
      "start": 169.12,
      "duration": 2.08,
      "text": "experience you can see the screen go up",
      "timestamp": "02:49"
    },
    {
      "start": 171.2,
      "duration": 2.88,
      "text": "up with all blank content so which will",
      "timestamp": "02:51"
    },
    {
      "start": 174.08,
      "duration": 2.72,
      "text": "deliver very bad user experience to",
      "timestamp": "02:54"
    },
    {
      "start": 176.8,
      "duration": 3.92,
      "text": "address this problem uh inference in",
      "timestamp": "02:56"
    },
    {
      "start": 180.72,
      "duration": 2.879,
      "text": "inference engines like hockey",
      "timestamp": "03:00"
    },
    {
      "start": 183.599,
      "duration": 2.881,
      "text": "transformers they do introduce the",
      "timestamp": "03:03"
    },
    {
      "start": 186.48,
      "duration": 2.64,
      "text": "repetition penalty parameters",
      "timestamp": "03:06"
    },
    {
      "start": 189.12,
      "duration": 4.24,
      "text": "um but we during the experimentations we",
      "timestamp": "03:09"
    },
    {
      "start": 193.36,
      "duration": 2.239,
      "text": "realized that there's a it's very",
      "timestamp": "03:13"
    },
    {
      "start": 195.599,
      "duration": 3.681,
      "text": "difficult to set the right par value for",
      "timestamp": "03:15"
    },
    {
      "start": 199.28,
      "duration": 2.48,
      "text": "the R repetition penalty parameter or",
      "timestamp": "03:19"
    },
    {
      "start": 201.76,
      "duration": 3.119,
      "text": "RPP if you set it too low then you have",
      "timestamp": "03:21"
    },
    {
      "start": 204.879,
      "duration": 2.481,
      "text": "a repetition problem if set too high",
      "timestamp": "03:24"
    },
    {
      "start": 207.36,
      "duration": 2.08,
      "text": "then the sentence might just cut off and",
      "timestamp": "03:27"
    },
    {
      "start": 209.44,
      "duration": 2.879,
      "text": "become a incomplete responses either way",
      "timestamp": "03:29"
    },
    {
      "start": 212.319,
      "duration": 2.241,
      "text": "you render the user experience very bad",
      "timestamp": "03:32"
    },
    {
      "start": 214.56,
      "duration": 3.679,
      "text": "badly so the this come to the core",
      "timestamp": "03:34"
    },
    {
      "start": 218.239,
      "duration": 2.881,
      "text": "research problem we try to address so",
      "timestamp": "03:38"
    },
    {
      "start": 221.12,
      "duration": 2.64,
      "text": "how do we achieve the balance between",
      "timestamp": "03:41"
    },
    {
      "start": 223.76,
      "duration": 2.24,
      "text": "reducing excess of repetition while",
      "timestamp": "03:43"
    },
    {
      "start": 226.0,
      "duration": 2.879,
      "text": "still maintaining the um coherence and",
      "timestamp": "03:46"
    },
    {
      "start": 228.879,
      "duration": 2.72,
      "text": "the relevance of the output for by the",
      "timestamp": "03:48"
    },
    {
      "start": 231.599,
      "duration": 2.801,
      "text": "open source larger language models to",
      "timestamp": "03:51"
    },
    {
      "start": 234.4,
      "duration": 4.399,
      "text": "address this we develop",
      "timestamp": "03:54"
    },
    {
      "start": 238.799,
      "duration": 3.201,
      "text": "a novel metric called repetition or web",
      "timestamp": "03:58"
    },
    {
      "start": 242.0,
      "duration": 2.799,
      "text": "performance where we are trying to",
      "timestamp": "04:02"
    },
    {
      "start": 244.799,
      "duration": 2.321,
      "text": "balance the repetition and performance",
      "timestamp": "04:04"
    },
    {
      "start": 247.12,
      "duration": 3.119,
      "text": "uh in open source and IMF let let me",
      "timestamp": "04:07"
    },
    {
      "start": 250.239,
      "duration": 2.961,
      "text": "explain with uh this example to see how",
      "timestamp": "04:10"
    },
    {
      "start": 253.2,
      "duration": 2.64,
      "text": "our RAP works so if you ask a large",
      "timestamp": "04:13"
    },
    {
      "start": 255.84,
      "duration": 2.639,
      "text": "language model say what currency is used",
      "timestamp": "04:15"
    },
    {
      "start": 258.479,
      "duration": 3.121,
      "text": "in Panama the model actually output the",
      "timestamp": "04:18"
    },
    {
      "start": 261.6,
      "duration": 3.28,
      "text": "correct answer the panomin pan many",
      "timestamp": "04:21"
    },
    {
      "start": 264.88,
      "duration": 4.319,
      "text": "baloba and United US dollars but you can",
      "timestamp": "04:24"
    },
    {
      "start": 269.199,
      "duration": 2.161,
      "text": "see very clearly here that US dollars",
      "timestamp": "04:29"
    },
    {
      "start": 271.36,
      "duration": 4.08,
      "text": "actually repeated twice and then also um",
      "timestamp": "04:31"
    },
    {
      "start": 275.44,
      "duration": 3.52,
      "text": "uh kind more dots nec non-necessary dots",
      "timestamp": "04:35"
    },
    {
      "start": 278.96,
      "duration": 5.12,
      "text": "uh uh here so we we develop a efficient",
      "timestamp": "04:38"
    },
    {
      "start": 284.08,
      "duration": 4.72,
      "text": "um repetition detection algorithm uh to",
      "timestamp": "04:44"
    },
    {
      "start": 288.8,
      "duration": 2.0,
      "text": "classify the repetition probably into",
      "timestamp": "04:48"
    },
    {
      "start": 290.8,
      "duration": 2.56,
      "text": "long word character like the dot dot",
      "timestamp": "04:50"
    },
    {
      "start": 293.36,
      "duration": 2.64,
      "text": "dots and also the United and the tax",
      "timestamp": "04:53"
    },
    {
      "start": 296.0,
      "duration": 1.52,
      "text": "repetition like you have United US",
      "timestamp": "04:56"
    },
    {
      "start": 297.52,
      "duration": 3.2,
      "text": "United States dollars here and then",
      "timestamp": "04:57"
    },
    {
      "start": 300.72,
      "duration": 2.0,
      "text": "eventually we calculate the repetition",
      "timestamp": "05:00"
    },
    {
      "start": 302.72,
      "duration": 3.68,
      "text": "ratio to to define how severe the",
      "timestamp": "05:02"
    },
    {
      "start": 306.4,
      "duration": 3.519,
      "text": "repetition in the response is in and",
      "timestamp": "05:06"
    },
    {
      "start": 309.919,
      "duration": 2.801,
      "text": "then we apply a cubic penalty function",
      "timestamp": "05:09"
    },
    {
      "start": 312.72,
      "duration": 3.36,
      "text": "to the original 100% of accuracy we get",
      "timestamp": "05:12"
    },
    {
      "start": 316.08,
      "duration": 3.28,
      "text": "around 25% of rep so even if the even",
      "timestamp": "05:16"
    },
    {
      "start": 319.36,
      "duration": 3.76,
      "text": "the results initial performance is 100%",
      "timestamp": "05:19"
    },
    {
      "start": 323.12,
      "duration": 3.28,
      "text": "it will get down to 25% so which give us",
      "timestamp": "05:23"
    },
    {
      "start": 326.4,
      "duration": 6.239,
      "text": "a way to optimize to um the RPP values",
      "timestamp": "05:26"
    },
    {
      "start": 332.639,
      "duration": 4.241,
      "text": "later we can see so to benchmark all the",
      "timestamp": "05:32"
    },
    {
      "start": 336.88,
      "duration": 3.36,
      "text": "open source models we we had developed",
      "timestamp": "05:36"
    },
    {
      "start": 340.24,
      "duration": 1.679,
      "text": "we use the two different modes why is",
      "timestamp": "05:40"
    },
    {
      "start": 341.919,
      "duration": 4.801,
      "text": "the we try to devel use use rack and we",
      "timestamp": "05:41"
    },
    {
      "start": 346.72,
      "duration": 2.8,
      "text": "fix the retrieval component and just",
      "timestamp": "05:46"
    },
    {
      "start": 349.52,
      "duration": 2.88,
      "text": "swap out all the open source models and",
      "timestamp": "05:49"
    },
    {
      "start": 352.4,
      "duration": 2.799,
      "text": "so that we can tell the differences",
      "timestamp": "05:52"
    },
    {
      "start": 355.199,
      "duration": 2.481,
      "text": "between the",
      "timestamp": "05:55"
    },
    {
      "start": 357.68,
      "duration": 2.0,
      "text": "large order for the generation piece",
      "timestamp": "05:57"
    },
    {
      "start": 359.68,
      "duration": 1.76,
      "text": "because we don't really care about the",
      "timestamp": "05:59"
    },
    {
      "start": 361.44,
      "duration": 2.56,
      "text": "retrieval accuracy yet for for this",
      "timestamp": "06:01"
    },
    {
      "start": 364.0,
      "duration": 4.16,
      "text": "specific purpose and we also try to um",
      "timestamp": "06:04"
    },
    {
      "start": 368.16,
      "duration": 2.479,
      "text": "rem remove the retrieval component",
      "timestamp": "06:08"
    },
    {
      "start": 370.639,
      "duration": 3.441,
      "text": "totally just try to see um direct answer",
      "timestamp": "06:10"
    },
    {
      "start": 374.08,
      "duration": 2.559,
      "text": "from Language models so see whether the",
      "timestamp": "06:14"
    },
    {
      "start": 376.639,
      "duration": 3.361,
      "text": "the repetition patterns are still there",
      "timestamp": "06:16"
    },
    {
      "start": 380.0,
      "duration": 2.72,
      "text": "so we call it the wrong rack so we",
      "timestamp": "06:20"
    },
    {
      "start": 382.72,
      "duration": 3.36,
      "text": "systematically change the repetition par",
      "timestamp": "06:22"
    },
    {
      "start": 386.08,
      "duration": 2.8,
      "text": "parameter penalty parameters from one to",
      "timestamp": "06:26"
    },
    {
      "start": 388.88,
      "duration": 4.0,
      "text": "1.3 with the increments of 0.02 this",
      "timestamp": "06:28"
    },
    {
      "start": 392.88,
      "duration": 3.12,
      "text": "systematic approach can allows us to map",
      "timestamp": "06:32"
    },
    {
      "start": 396.0,
      "duration": 3.44,
      "text": "out the granular differences between the",
      "timestamp": "06:36"
    },
    {
      "start": 399.44,
      "duration": 2.319,
      "text": "different parameters and across all the",
      "timestamp": "06:39"
    },
    {
      "start": 401.759,
      "duration": 2.801,
      "text": "models we evaluated",
      "timestamp": "06:41"
    },
    {
      "start": 404.56,
      "duration": 4.72,
      "text": "so to evaluate the the the",
      "timestamp": "06:44"
    },
    {
      "start": 409.28,
      "duration": 3.28,
      "text": "models we also use the very two widely",
      "timestamp": "06:49"
    },
    {
      "start": 412.56,
      "duration": 3.359,
      "text": "used the data set for the um natur le",
      "timestamp": "06:52"
    },
    {
      "start": 415.919,
      "duration": 3.761,
      "text": "natural language and processing um one",
      "timestamp": "06:55"
    },
    {
      "start": 419.68,
      "duration": 3.2,
      "text": "is a web QSP which is a widely used",
      "timestamp": "06:59"
    },
    {
      "start": 422.88,
      "duration": 2.08,
      "text": "benchmark for knowledge based question",
      "timestamp": "07:02"
    },
    {
      "start": 424.96,
      "duration": 2.239,
      "text": "answering tasks we use performance",
      "timestamp": "07:04"
    },
    {
      "start": 427.199,
      "duration": 3.44,
      "text": "metrics F1 and the another one is from",
      "timestamp": "07:07"
    },
    {
      "start": 430.639,
      "duration": 3.361,
      "text": "Microsoft Microsoft Marco data set is a",
      "timestamp": "07:10"
    },
    {
      "start": 434.0,
      "duration": 1.919,
      "text": "comprehensive data set for rendering",
      "timestamp": "07:14"
    },
    {
      "start": 435.919,
      "duration": 1.921,
      "text": "reading comprehension and question",
      "timestamp": "07:15"
    },
    {
      "start": 437.84,
      "duration": 3.12,
      "text": "answering so we use a bird F1 as a",
      "timestamp": "07:17"
    },
    {
      "start": 440.96,
      "duration": 2.799,
      "text": "performance metric",
      "timestamp": "07:20"
    },
    {
      "start": 443.759,
      "duration": 3.84,
      "text": "so the key key keep because we know that",
      "timestamp": "07:23"
    },
    {
      "start": 447.599,
      "duration": 3.121,
      "text": "we to fine-tune the the get the better",
      "timestamp": "07:27"
    },
    {
      "start": 450.72,
      "duration": 1.68,
      "text": "performance we will need to do the",
      "timestamp": "07:30"
    },
    {
      "start": 452.4,
      "duration": 3.04,
      "text": "prompt engineering so we also try to run",
      "timestamp": "07:32"
    },
    {
      "start": 455.44,
      "duration": 3.199,
      "text": "different promping strategy the first",
      "timestamp": "07:35"
    },
    {
      "start": 458.639,
      "duration": 2.96,
      "text": "one is we call it the rag generic prompt",
      "timestamp": "07:38"
    },
    {
      "start": 461.599,
      "duration": 4.481,
      "text": "which is the default prompt engine uh",
      "timestamp": "07:41"
    },
    {
      "start": 466.08,
      "duration": 3.44,
      "text": "template introduced by n in in nchen",
      "timestamp": "07:46"
    },
    {
      "start": 469.52,
      "duration": 3.92,
      "text": "which is a widely used um open source",
      "timestamp": "07:49"
    },
    {
      "start": 473.44,
      "duration": 2.08,
      "text": "framework for large language model",
      "timestamp": "07:53"
    },
    {
      "start": 475.52,
      "duration": 2.239,
      "text": "application development",
      "timestamp": "07:55"
    },
    {
      "start": 477.759,
      "duration": 1.681,
      "text": "so you can see it's just natural",
      "timestamp": "07:57"
    },
    {
      "start": 479.44,
      "duration": 2.879,
      "text": "language without any templates so we",
      "timestamp": "07:59"
    },
    {
      "start": 482.319,
      "duration": 2.241,
      "text": "when we try to do the long rag we notice",
      "timestamp": "08:02"
    },
    {
      "start": 484.56,
      "duration": 2.56,
      "text": "okay that for each specific RM family",
      "timestamp": "08:04"
    },
    {
      "start": 487.12,
      "duration": 1.359,
      "text": "actually they have a kind of so-called",
      "timestamp": "08:07"
    },
    {
      "start": 488.479,
      "duration": 2.0,
      "text": "chat template we need to squeeze the",
      "timestamp": "08:08"
    },
    {
      "start": 490.479,
      "duration": 2.481,
      "text": "date the text into the right template",
      "timestamp": "08:10"
    },
    {
      "start": 492.96,
      "duration": 2.56,
      "text": "like this one so later we also we think",
      "timestamp": "08:12"
    },
    {
      "start": 495.52,
      "duration": 2.0,
      "text": "about okay maybe maybe we can also do",
      "timestamp": "08:15"
    },
    {
      "start": 497.52,
      "duration": 2.959,
      "text": "the same thing we call rag chat template",
      "timestamp": "08:17"
    },
    {
      "start": 500.479,
      "duration": 2.641,
      "text": "putting the all the rack information",
      "timestamp": "08:20"
    },
    {
      "start": 503.12,
      "duration": 2.4,
      "text": "into the this uh chat template for",
      "timestamp": "08:23"
    },
    {
      "start": 505.52,
      "duration": 3.6,
      "text": "different RM so we will see how whether",
      "timestamp": "08:25"
    },
    {
      "start": 509.12,
      "duration": 2.16,
      "text": "this rack temp chat template will get us",
      "timestamp": "08:29"
    },
    {
      "start": 511.28,
      "duration": 3.28,
      "text": "better performance",
      "timestamp": "08:31"
    },
    {
      "start": 514.56,
      "duration": 3.919,
      "text": "so here this is uh the the results i I",
      "timestamp": "08:34"
    },
    {
      "start": 518.479,
      "duration": 2.0,
      "text": "don't want to get into the detail due to",
      "timestamp": "08:38"
    },
    {
      "start": 520.479,
      "duration": 3.36,
      "text": "the time in diff time constraint but the",
      "timestamp": "08:40"
    },
    {
      "start": 523.839,
      "duration": 3.521,
      "text": "very important observance observation",
      "timestamp": "08:43"
    },
    {
      "start": 527.36,
      "duration": 4.08,
      "text": "here is that if we see we can see that",
      "timestamp": "08:47"
    },
    {
      "start": 531.44,
      "duration": 4.0,
      "text": "react chat template I will consistently",
      "timestamp": "08:51"
    },
    {
      "start": 535.44,
      "duration": 2.88,
      "text": "suppress the red text repetition 10",
      "timestamp": "08:55"
    },
    {
      "start": 538.32,
      "duration": 4.72,
      "text": "times smaller than the generic prompt so",
      "timestamp": "08:58"
    },
    {
      "start": 543.04,
      "duration": 2.239,
      "text": "that's is the very first observation",
      "timestamp": "09:03"
    },
    {
      "start": 545.279,
      "duration": 1.761,
      "text": "actually from this research findings",
      "timestamp": "09:05"
    },
    {
      "start": 547.04,
      "duration": 2.16,
      "text": "when I talk to my our product",
      "timestamp": "09:07"
    },
    {
      "start": 549.2,
      "duration": 2.48,
      "text": "development uh team and say hey you you",
      "timestamp": "09:09"
    },
    {
      "start": 551.68,
      "duration": 1.839,
      "text": "you guys probably need to switch from",
      "timestamp": "09:11"
    },
    {
      "start": 553.519,
      "duration": 4.32,
      "text": "the default um um nin prompt template to",
      "timestamp": "09:13"
    },
    {
      "start": 557.839,
      "duration": 2.481,
      "text": "this chhat template and they initially",
      "timestamp": "09:17"
    },
    {
      "start": 560.32,
      "duration": 2.72,
      "text": "they don't really believe so and uh then",
      "timestamp": "09:20"
    },
    {
      "start": 563.04,
      "duration": 2.72,
      "text": "we we I I",
      "timestamp": "09:23"
    },
    {
      "start": 565.76,
      "duration": 2.48,
      "text": "use that data set and use my methodology",
      "timestamp": "09:25"
    },
    {
      "start": 568.24,
      "duration": 2.88,
      "text": "and prove that they they also have uh",
      "timestamp": "09:28"
    },
    {
      "start": 571.12,
      "duration": 2.64,
      "text": "the repetition problem with the current",
      "timestamp": "09:31"
    },
    {
      "start": 573.76,
      "duration": 1.84,
      "text": "the their current implementation and",
      "timestamp": "09:33"
    },
    {
      "start": 575.6,
      "duration": 2.56,
      "text": "eventually the whole product development",
      "timestamp": "09:35"
    },
    {
      "start": 578.16,
      "duration": 2.799,
      "text": "switch to this chat template approach",
      "timestamp": "09:38"
    },
    {
      "start": 580.959,
      "duration": 2.0,
      "text": "another another observation is that if",
      "timestamp": "09:40"
    },
    {
      "start": 582.959,
      "duration": 4.0,
      "text": "you the RPP normally will repress the",
      "timestamp": "09:42"
    },
    {
      "start": 586.959,
      "duration": 2.241,
      "text": "temp repetition you can see the downward",
      "timestamp": "09:46"
    },
    {
      "start": 589.2,
      "duration": 2.88,
      "text": "trend for almost all across all the",
      "timestamp": "09:49"
    },
    {
      "start": 592.08,
      "duration": 3.199,
      "text": "performance metrics there but you might",
      "timestamp": "09:52"
    },
    {
      "start": 595.279,
      "duration": 4.641,
      "text": "also uh notice that it it impact",
      "timestamp": "09:55"
    },
    {
      "start": 599.92,
      "duration": 2.08,
      "text": "profetition",
      "timestamp": "09:59"
    },
    {
      "start": 602.0,
      "duration": 2.8,
      "text": "as well as performance so to get the",
      "timestamp": "10:02"
    },
    {
      "start": 604.8,
      "duration": 2.56,
      "text": "balance we need to tune the RPP so that",
      "timestamp": "10:04"
    },
    {
      "start": 607.36,
      "duration": 3.599,
      "text": "we will have a balance so this is the",
      "timestamp": "10:07"
    },
    {
      "start": 610.959,
      "duration": 2.721,
      "text": "way we are trying to do the balance",
      "timestamp": "10:10"
    },
    {
      "start": 613.68,
      "duration": 1.279,
      "text": "between the performance center and",
      "timestamp": "10:13"
    },
    {
      "start": 614.959,
      "duration": 2.88,
      "text": "repetition so the blue color and it",
      "timestamp": "10:14"
    },
    {
      "start": 617.839,
      "duration": 2.881,
      "text": "shows that the best per original",
      "timestamp": "10:17"
    },
    {
      "start": 620.72,
      "duration": 2.96,
      "text": "performance the RP value the yellow",
      "timestamp": "10:20"
    },
    {
      "start": 623.68,
      "duration": 2.96,
      "text": "color shows the best the RP value for",
      "timestamp": "10:23"
    },
    {
      "start": 626.64,
      "duration": 3.04,
      "text": "the best R value so if you see the just",
      "timestamp": "10:26"
    },
    {
      "start": 629.68,
      "duration": 3.36,
      "text": "see the first subplot you can see that",
      "timestamp": "10:29"
    },
    {
      "start": 633.04,
      "duration": 2.4,
      "text": "at the beginning um for the the blue",
      "timestamp": "10:33"
    },
    {
      "start": 635.44,
      "duration": 4.079,
      "text": "color actually it has around 26% of the",
      "timestamp": "10:35"
    },
    {
      "start": 639.519,
      "duration": 4.081,
      "text": "repetition um ratio is this which means",
      "timestamp": "10:39"
    },
    {
      "start": 643.6,
      "duration": 3.919,
      "text": "a very high um repetition but if we",
      "timestamp": "10:43"
    },
    {
      "start": 647.519,
      "duration": 2.56,
      "text": "choose the yellow color the RPP it",
      "timestamp": "10:47"
    },
    {
      "start": 650.079,
      "duration": 3.2,
      "text": "change to the R change is suppressed to",
      "timestamp": "10:50"
    },
    {
      "start": 653.279,
      "duration": 6.721,
      "text": "2% which is a um decrement of uh 92% of",
      "timestamp": "10:53"
    },
    {
      "start": 660.0,
      "duration": 3.839,
      "text": "the repetition uh behaviors but if you",
      "timestamp": "11:00"
    },
    {
      "start": 663.839,
      "duration": 2.161,
      "text": "see the performance drop here is just",
      "timestamp": "11:03"
    },
    {
      "start": 666.0,
      "duration": 1.6,
      "text": "0.022",
      "timestamp": "11:06"
    },
    {
      "start": 667.6,
      "duration": 3.359,
      "text": "points so this with this we are we can",
      "timestamp": "11:07"
    },
    {
      "start": 670.959,
      "duration": 1.681,
      "text": "systematically",
      "timestamp": "11:10"
    },
    {
      "start": 672.64,
      "duration": 3.199,
      "text": "um find the balance uh the best RP for",
      "timestamp": "11:12"
    },
    {
      "start": 675.839,
      "duration": 2.401,
      "text": "to to make the to balance the",
      "timestamp": "11:15"
    },
    {
      "start": 678.24,
      "duration": 3.96,
      "text": "performance and the repetition",
      "timestamp": "11:18"
    },
    {
      "start": 683.68,
      "duration": 2.719,
      "text": "to summarize we developed a repetition",
      "timestamp": "11:23"
    },
    {
      "start": 686.399,
      "duration": 2.161,
      "text": "detection algorithm which to measure the",
      "timestamp": "11:26"
    },
    {
      "start": 688.56,
      "duration": 2.88,
      "text": "repetition in generated text using uh",
      "timestamp": "11:28"
    },
    {
      "start": 691.44,
      "duration": 3.68,
      "text": "repetition ratio and this algorithm we I",
      "timestamp": "11:31"
    },
    {
      "start": 695.12,
      "duration": 2.48,
      "text": "don't have time to discuss the details",
      "timestamp": "11:35"
    },
    {
      "start": 697.6,
      "duration": 3.679,
      "text": "and but it perform outperformed the uh",
      "timestamp": "11:37"
    },
    {
      "start": 701.279,
      "duration": 3.201,
      "text": "previous versions in the literature by",
      "timestamp": "11:41"
    },
    {
      "start": 704.48,
      "duration": 3.52,
      "text": "20% in terms of the efficiency and we",
      "timestamp": "11:44"
    },
    {
      "start": 708.0,
      "duration": 3.2,
      "text": "also introduce an repetition of web",
      "timestamp": "11:48"
    },
    {
      "start": 711.2,
      "duration": 2.48,
      "text": "performance which allows us to do the",
      "timestamp": "11:51"
    },
    {
      "start": 713.68,
      "duration": 3.279,
      "text": "optimization of the large language model",
      "timestamp": "11:53"
    },
    {
      "start": 716.959,
      "duration": 3.12,
      "text": "while maintaining content uh quality so",
      "timestamp": "11:56"
    },
    {
      "start": 720.079,
      "duration": 2.561,
      "text": "key takeaway here is that use chat",
      "timestamp": "12:00"
    },
    {
      "start": 722.64,
      "duration": 3.04,
      "text": "template for whatever uh application you",
      "timestamp": "12:02"
    },
    {
      "start": 725.68,
      "duration": 2.08,
      "text": "are going to build we we we talk about",
      "timestamp": "12:05"
    },
    {
      "start": 727.76,
      "duration": 2.24,
      "text": "rag but in the our research paper also",
      "timestamp": "12:07"
    },
    {
      "start": 730.0,
      "duration": 2.72,
      "text": "talk about translation we also you find",
      "timestamp": "12:10"
    },
    {
      "start": 732.72,
      "duration": 3.84,
      "text": "some uh same pattern here and that we",
      "timestamp": "12:12"
    },
    {
      "start": 736.56,
      "duration": 2.8,
      "text": "could see the difference in RP where not",
      "timestamp": "12:16"
    },
    {
      "start": 739.36,
      "duration": 2.8,
      "text": "only impact the text repetition but also",
      "timestamp": "12:19"
    },
    {
      "start": 742.16,
      "duration": 2.08,
      "text": "impact the performance so we need to",
      "timestamp": "12:22"
    },
    {
      "start": 744.24,
      "duration": 2.64,
      "text": "make sure that have find the right RPP",
      "timestamp": "12:24"
    },
    {
      "start": 746.88,
      "duration": 2.88,
      "text": "so that you can you don't uh sacrifice",
      "timestamp": "12:26"
    },
    {
      "start": 749.76,
      "duration": 1.759,
      "text": "performance while you are trying to",
      "timestamp": "12:29"
    },
    {
      "start": 751.519,
      "duration": 4.641,
      "text": "suppress the tax repetition so we our RP",
      "timestamp": "12:31"
    },
    {
      "start": 756.16,
      "duration": 5.2,
      "text": "is a introduced a new way to balance the",
      "timestamp": "12:36"
    },
    {
      "start": 761.36,
      "duration": 2.96,
      "text": "performance and repetition",
      "timestamp": "12:41"
    },
    {
      "start": 764.32,
      "duration": 2.4,
      "text": "what the all these findings have have",
      "timestamp": "12:44"
    },
    {
      "start": 766.72,
      "duration": 3.359,
      "text": "been published in local conference which",
      "timestamp": "12:46"
    },
    {
      "start": 770.079,
      "duration": 5.601,
      "text": "was held in US last in from April 29 to",
      "timestamp": "12:50"
    },
    {
      "start": 775.68,
      "duration": 2.719,
      "text": "May 4 so you can see the link here also",
      "timestamp": "12:55"
    },
    {
      "start": 778.399,
      "duration": 5.841,
      "text": "is now public available in on online",
      "timestamp": "12:58"
    },
    {
      "start": 784.24,
      "duration": 2.64,
      "text": "thank you for your attention if",
      "timestamp": "13:04"
    },
    {
      "start": 786.88,
      "duration": 2.959,
      "text": "interested to discuss uh more with me",
      "timestamp": "13:06"
    },
    {
      "start": 789.839,
      "duration": 2.881,
      "text": "and feel free to connect via LinkedIn or",
      "timestamp": "13:09"
    },
    {
      "start": 792.72,
      "duration": 2.559,
      "text": "send me an email",
      "timestamp": "13:12"
    },
    {
      "start": 795.279,
      "duration": 3.161,
      "text": "thank you",
      "timestamp": "13:15"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.393774",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}