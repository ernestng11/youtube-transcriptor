{
  "video_id": "8Utc50ROZzc",
  "video_title": "Scott Albin - Discover What's Possible With Fast AI Inference At Groq Speed - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=8Utc50ROZzc",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T13:48:56+00:00",
  "duration_seconds": null,
  "view_count": 30,
  "like_count": 1,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: Build Fast. Discover What's Possible With Fast AI Inference At Groq Speed\n\nSpeaker:\nScott Albin, GM, APAC @ Groq\n\nStage: WEKA Stage\n#superai #groq #ai #inference #benchmarks \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 314,
    "aggregated_text": "all right hi everyone that's Grock with a Q not Grock with a K just want to clarify that uh Grock is a company that was born out of frustration and a desire to change the world uh our founder Jonathan Ross he previously worked at Google and when he was at Google he invented a chip you might have heard of it's called the TPU that was a side project of his at the time he left Google ultimately and founded Grock and he wanted to bring AI to the world he saw what was happening at Google he thought that was inevitable it was going to happen everywhere in the world uh and he had a goal to drive the cost of AI down to zero kind of how search the marginal cost of search is z is zero he wanted to do the same thing for AI and he knew that would take a new type of technology a new type of chip a whole new set of software so I want to tell you about that a bit today but first I want to just tell you what's going on in the world probably nothing that you don't already know but let me show you some stats ai is eating work as we know it it's taking over every form of labor that we we know there's robots here on the floor that are entering the physical world there's AI eating the traditional digital services and all forms of a labor and what does that really mean well what is AI ai at its core is generating tokens or doing this process that we call inference and I'd like you to leave this talk recognizing that inference is the vast majority of the AI workload training is a very important part that has been talked about extensively for the last few years we've seen the rise of NVIDIA but when we actually talk about building applications building software powering these robotics that's all inference and so work is inference in the future we know there's a lot of infrastructure getting built also all over the world you know some of the stats are pretty astonishing more than $3 trillion is going to get spent building infrastructure the majority of that is going to AI and the majority of that AI infrastructure is powering inference not training models this is really important because this is going to be the future of work and so Grock was built for fast inference we design a chip we call it the LPU that chip is deployed all over the world it's manufactured in North America very unique i think we're the only advanced AI accelerator chip made in the north in North America we're headquartered in California but I would like you to also know that half of our global developers are here in Asia i live here in Singapore we're building a big team out here to support our customers i want to bring this to life for you in just a moment but first I want to tell you a little bit more about the chip we know about CPUs we know about GPUs i would like to argue that the LPU is going to be the next major architectural shift in AI in computing our LPU is substantially faster than GPUs four times faster oneif of the cost onethird of the power and this gap is growing substantially we can achieve this because we have a fundamentally different architecture it fundamentally looks different here's a picture of a GPU and on the right you see a picture of our LPU it just looks different it's actually designed around this concept of an assembly line like a factory floor data comes in one end tokens come out the other there's fundamental architectural differences about this chip about the memory architecture about the design i'll even let you know that the chip itself the LPU that you see pictured here is a 14 nanometer chip 10-year-old fabrication technology and it outperforms GPUs because of that fundamental difference in architecture and all of the software that defines how it works so I want to bring this to live for you so let's do a demo so one of the most important AI applications that is going to be ubiquitous all over the world is voice we want to talk to our phones we want the phones to talk back to us we want our cars to talk back to us we're going to get on the phone with customer support it's going to be voiceto if you saw the demo from Phone which is powered by Grock earlier today and that's a great example of voicetovoice interactions so I'm going to do a live demo i'm going to talk to a voice agent u let's see how it goes hello Super AI so excited to be talking to you hi you tell me about yourself it's great connect hi how can I help you today can you tell me about yourself yes I'm talking to you right now i'm a voice AI assistant and our conversation is happening in real time i'm here to help answer your questions or just chat great can you tell me about what technology you're built on i am built using Pipecat an open source framework for building voice AI applications and I'm powered by the Gro AI platform which provides high performance AI services that's great can you tell me why fast AI inference is important fast AI inference is crucial for applications that require real-time processing like voice assistants chat bots and other interactive system it enables seamless and responsive interactions making it possible to deliver high-quality user experiences gro's language processing units LPUs are designed to accelerate AI inference achieving industryleading speed and low latency that's great which is a key factor in making real Okay I'm glad you're interested we're we're going have to keep going thanks for the demo if you have any The fun doesn't stop here our audience should visit the Gro booth in the expo to learn more about Grock and how to leverage real time AI inference for their use cases from you and your team so there you go there's a there's a simple example of a voicetooice demo i'll tell you that the infrastructure running on that demo is in North America so we had to send that request all the way to the US we had to process it on LPUs we had to send it all the way back so you might have noticed a very small delay that was part of the network transit time uh ultimately that infrastructure is going to be ubiquitous everywhere and so these voicetooice agents are going to be everywhere i want to show you one more demo i had to record this one um but everyone always asks me well how is this different than a GPU what's the actual relative performance difference so I wanted to try to visualize that for you and so I'm going to play a video for you this video is going to show Grock powered by our Gro cloud platform running Llama 4 Scout compared to Nvidia GPUs compared to OpenAI we're going to just give it a prompt all three models all three services at the same time and I'm going to use a coding example uh here we go let's wait for this to load i'm going to ask it to code a simple simple game in Python so write me a Python script for the game Pac-Man the request is going to go to all three models simultaneously all three services and you're going to see the results so Grock is on the left it's done nvidia running the same model H100s it will finish and then OpenAI it's a bit more verbose a bit more chatty so it'll it'll finish soon but I want to draw your attention to what the the number at the very top if you can see that we were doing over 400 tokens per second 400 tokens per second most GPUs today do less than 100 G tokens per second now with new generations that will get a bit faster but we're at 500 and we're going to go way way past that this is again 14 nanometer technology 10 years old it's more than five times faster than GPUs so if you're an engineer you're a developer you're building co-pilots voice agents anything that requires low latency this is critical to power those applications and we think this is going to open up the next range of applications that's really going to transform the world okay so to recap Grock is producing a chip we call it the LPU but it's not doesn't stop there we like to say we build from the silicon to the cloud so we design the chips again manufactured in North America we package it all in North America we design the networking and we build the full cloud software service that allows developers to access AI models running on our Grock hardware so to show you a stat we launched Grock Cloud in March of 2024 just over a year ago we had zero users we were running one copy of Llama One or two at the time we now have over 1.6 million developers i think we're adding 50,000 a week people are coming to Groth because of the price performance difference that we're able to deliver uh in fast AI inference versel if you know Verscell they are one of the largest web developer app platforms in the world i think they have 50 million developers building web apps they ran a study recently and said who are you switching from uh when you in the context of AI and most people were switching from open AI or maybe one of the hypers scale cloud providers and Grock was number three this was now a little while ago i think that number is probably a bit higher but Grock is very rapidly from a standstill gaining tremendous mind share and market share in the fast AI inference and I believe that we'll be the largest provider of AI inference in the not too distant future we even got a shout out from Zuckerberg he's notoriously uh uh stingy with his shoutouts for third party companies but Grock is actually powering the Meta Enterprise Llama API you know we can read the quote the low latency advantage of Grock at the lower price is really the advantage we're really trying to drive the cost of AI to zero to enable you to build better and more amazing products but we're not stopping there i'm here in Singapore i'm at this conference with you i'm based here as I mentioned 10 years uh we're expanding internationally grock expanded first to the Middle East with our partnership with Aramco now Humane AI in the Middle East in in the Kingdom of Saudi Arabia where they're building the world's largest AI inference data center uh it's an incredible feat uh that's powered by Grock you know hundreds of thousands of our chips will eventually be powering this data center we've also announced a partnership with Bell Telco in Canada where we're building the sovereign AI infrastructure in Canada and we're having similar discussions all over the world to build localized infrastructure to power local AI models to enable developers like you and we're also releasing the latest and greatest AI models we just released Quinn 3 uh this past week we just launched on Hugging Face so if you're a developer you use hugging face you use open router versel you can find gro in all these platforms and you can deploy it so I'm going to leave you here uh just want to deliver a few key messages grock is growing very very fast we are going to be everywhere we're already here in Asia over half of our global developers are here we're supporting hundreds of thousands of you but I think many people in this room are actually uh users or customers of Gro which is fantastic we're going to continue to push the envelope and provide the world's fastest AI inference we're going to do that at the most competitive price and we're going to provide as many of the models uh that you need to build as many of the tools and enterprise features that you need to build so I hope that was useful i hope that was interesting for you if you have more questions there is a Grock booth just past the main stage please come say hello all right thank you",
    "text_length": 11378,
    "word_count": 2145
  },
  "segments": [
    {
      "start": 7.68,
      "duration": 1.999,
      "text": "all right hi everyone that's Grock with",
      "timestamp": "00:07"
    },
    {
      "start": 9.679,
      "duration": 2.561,
      "text": "a Q not Grock with a K just want to",
      "timestamp": "00:09"
    },
    {
      "start": 12.24,
      "duration": 3.92,
      "text": "clarify that uh Grock is a company that",
      "timestamp": "00:12"
    },
    {
      "start": 16.16,
      "duration": 2.959,
      "text": "was born out of frustration and a desire",
      "timestamp": "00:16"
    },
    {
      "start": 19.119,
      "duration": 2.561,
      "text": "to change the world uh our founder",
      "timestamp": "00:19"
    },
    {
      "start": 21.68,
      "duration": 2.0,
      "text": "Jonathan Ross he previously worked at",
      "timestamp": "00:21"
    },
    {
      "start": 23.68,
      "duration": 2.32,
      "text": "Google and when he was at Google he",
      "timestamp": "00:23"
    },
    {
      "start": 26.0,
      "duration": 1.519,
      "text": "invented a chip you might have heard of",
      "timestamp": "00:26"
    },
    {
      "start": 27.519,
      "duration": 2.0,
      "text": "it's called the TPU that was a side",
      "timestamp": "00:27"
    },
    {
      "start": 29.519,
      "duration": 2.801,
      "text": "project of his at the time he left",
      "timestamp": "00:29"
    },
    {
      "start": 32.32,
      "duration": 2.239,
      "text": "Google ultimately and founded Grock and",
      "timestamp": "00:32"
    },
    {
      "start": 34.559,
      "duration": 1.921,
      "text": "he wanted to bring AI to the world he",
      "timestamp": "00:34"
    },
    {
      "start": 36.48,
      "duration": 1.68,
      "text": "saw what was happening at Google he",
      "timestamp": "00:36"
    },
    {
      "start": 38.16,
      "duration": 1.44,
      "text": "thought that was inevitable it was going",
      "timestamp": "00:38"
    },
    {
      "start": 39.6,
      "duration": 2.16,
      "text": "to happen everywhere in the world uh and",
      "timestamp": "00:39"
    },
    {
      "start": 41.76,
      "duration": 2.16,
      "text": "he had a goal to drive the cost of AI",
      "timestamp": "00:41"
    },
    {
      "start": 43.92,
      "duration": 2.319,
      "text": "down to zero kind of how search the",
      "timestamp": "00:43"
    },
    {
      "start": 46.239,
      "duration": 2.241,
      "text": "marginal cost of search is z is zero he",
      "timestamp": "00:46"
    },
    {
      "start": 48.48,
      "duration": 1.759,
      "text": "wanted to do the same thing for AI and",
      "timestamp": "00:48"
    },
    {
      "start": 50.239,
      "duration": 1.441,
      "text": "he knew that would take a new type of",
      "timestamp": "00:50"
    },
    {
      "start": 51.68,
      "duration": 2.719,
      "text": "technology a new type of chip a whole",
      "timestamp": "00:51"
    },
    {
      "start": 54.399,
      "duration": 1.601,
      "text": "new set of software so I want to tell",
      "timestamp": "00:54"
    },
    {
      "start": 56.0,
      "duration": 2.399,
      "text": "you about that a bit today but first I",
      "timestamp": "00:56"
    },
    {
      "start": 58.399,
      "duration": 1.281,
      "text": "want to just tell you what's going on in",
      "timestamp": "00:58"
    },
    {
      "start": 59.68,
      "duration": 2.32,
      "text": "the world probably nothing that you",
      "timestamp": "00:59"
    },
    {
      "start": 62.0,
      "duration": 1.28,
      "text": "don't already know but let me show you",
      "timestamp": "01:02"
    },
    {
      "start": 63.28,
      "duration": 2.56,
      "text": "some stats",
      "timestamp": "01:03"
    },
    {
      "start": 65.84,
      "duration": 3.279,
      "text": "ai is eating work as we know it it's",
      "timestamp": "01:05"
    },
    {
      "start": 69.119,
      "duration": 3.601,
      "text": "taking over every form of labor that we",
      "timestamp": "01:09"
    },
    {
      "start": 72.72,
      "duration": 2.16,
      "text": "we know there's robots here on the floor",
      "timestamp": "01:12"
    },
    {
      "start": 74.88,
      "duration": 2.08,
      "text": "that are entering the physical world",
      "timestamp": "01:14"
    },
    {
      "start": 76.96,
      "duration": 2.08,
      "text": "there's AI eating the traditional",
      "timestamp": "01:16"
    },
    {
      "start": 79.04,
      "duration": 2.16,
      "text": "digital services and all forms of a",
      "timestamp": "01:19"
    },
    {
      "start": 81.2,
      "duration": 2.559,
      "text": "labor and what does that really mean",
      "timestamp": "01:21"
    },
    {
      "start": 83.759,
      "duration": 2.4,
      "text": "well what is AI ai at its core is",
      "timestamp": "01:23"
    },
    {
      "start": 86.159,
      "duration": 2.241,
      "text": "generating tokens or doing this process",
      "timestamp": "01:26"
    },
    {
      "start": 88.4,
      "duration": 2.16,
      "text": "that we call inference",
      "timestamp": "01:28"
    },
    {
      "start": 90.56,
      "duration": 2.96,
      "text": "and I'd like you to leave this talk",
      "timestamp": "01:30"
    },
    {
      "start": 93.52,
      "duration": 2.4,
      "text": "recognizing that inference is the vast",
      "timestamp": "01:33"
    },
    {
      "start": 95.92,
      "duration": 3.76,
      "text": "majority of the AI workload",
      "timestamp": "01:35"
    },
    {
      "start": 99.68,
      "duration": 2.079,
      "text": "training is a very important part that",
      "timestamp": "01:39"
    },
    {
      "start": 101.759,
      "duration": 1.921,
      "text": "has been talked about extensively for",
      "timestamp": "01:41"
    },
    {
      "start": 103.68,
      "duration": 1.68,
      "text": "the last few years we've seen the rise",
      "timestamp": "01:43"
    },
    {
      "start": 105.36,
      "duration": 1.759,
      "text": "of NVIDIA but when we actually talk",
      "timestamp": "01:45"
    },
    {
      "start": 107.119,
      "duration": 1.601,
      "text": "about building applications building",
      "timestamp": "01:47"
    },
    {
      "start": 108.72,
      "duration": 2.399,
      "text": "software powering these robotics that's",
      "timestamp": "01:48"
    },
    {
      "start": 111.119,
      "duration": 3.36,
      "text": "all inference and so work is inference",
      "timestamp": "01:51"
    },
    {
      "start": 114.479,
      "duration": 3.041,
      "text": "in the future",
      "timestamp": "01:54"
    },
    {
      "start": 117.52,
      "duration": 1.279,
      "text": "we know there's a lot of infrastructure",
      "timestamp": "01:57"
    },
    {
      "start": 118.799,
      "duration": 2.241,
      "text": "getting built also all over the world",
      "timestamp": "01:58"
    },
    {
      "start": 121.04,
      "duration": 1.2,
      "text": "you know some of the stats are pretty",
      "timestamp": "02:01"
    },
    {
      "start": 122.24,
      "duration": 2.559,
      "text": "astonishing more than $3 trillion is",
      "timestamp": "02:02"
    },
    {
      "start": 124.799,
      "duration": 1.28,
      "text": "going to get spent building",
      "timestamp": "02:04"
    },
    {
      "start": 126.079,
      "duration": 2.32,
      "text": "infrastructure the majority of that is",
      "timestamp": "02:06"
    },
    {
      "start": 128.399,
      "duration": 1.92,
      "text": "going to AI",
      "timestamp": "02:08"
    },
    {
      "start": 130.319,
      "duration": 1.521,
      "text": "and the majority of that AI",
      "timestamp": "02:10"
    },
    {
      "start": 131.84,
      "duration": 2.88,
      "text": "infrastructure is powering inference not",
      "timestamp": "02:11"
    },
    {
      "start": 134.72,
      "duration": 2.08,
      "text": "training models",
      "timestamp": "02:14"
    },
    {
      "start": 136.8,
      "duration": 1.84,
      "text": "this is really important because this is",
      "timestamp": "02:16"
    },
    {
      "start": 138.64,
      "duration": 2.959,
      "text": "going to be the future of work",
      "timestamp": "02:18"
    },
    {
      "start": 141.599,
      "duration": 2.64,
      "text": "and so Grock was built for fast",
      "timestamp": "02:21"
    },
    {
      "start": 144.239,
      "duration": 2.72,
      "text": "inference we design a chip we call it",
      "timestamp": "02:24"
    },
    {
      "start": 146.959,
      "duration": 3.761,
      "text": "the LPU that chip is deployed all over",
      "timestamp": "02:26"
    },
    {
      "start": 150.72,
      "duration": 1.76,
      "text": "the world",
      "timestamp": "02:30"
    },
    {
      "start": 152.48,
      "duration": 3.44,
      "text": "it's manufactured in North America very",
      "timestamp": "02:32"
    },
    {
      "start": 155.92,
      "duration": 2.16,
      "text": "unique i think we're the only advanced",
      "timestamp": "02:35"
    },
    {
      "start": 158.08,
      "duration": 3.04,
      "text": "AI accelerator chip made in the north in",
      "timestamp": "02:38"
    },
    {
      "start": 161.12,
      "duration": 2.479,
      "text": "North America we're headquartered in",
      "timestamp": "02:41"
    },
    {
      "start": 163.599,
      "duration": 2.801,
      "text": "California but I would like you to also",
      "timestamp": "02:43"
    },
    {
      "start": 166.4,
      "duration": 1.839,
      "text": "know that half of our global developers",
      "timestamp": "02:46"
    },
    {
      "start": 168.239,
      "duration": 2.08,
      "text": "are here in Asia i live here in",
      "timestamp": "02:48"
    },
    {
      "start": 170.319,
      "duration": 1.761,
      "text": "Singapore we're building a big team out",
      "timestamp": "02:50"
    },
    {
      "start": 172.08,
      "duration": 3.36,
      "text": "here to support our customers",
      "timestamp": "02:52"
    },
    {
      "start": 175.44,
      "duration": 2.879,
      "text": "i want to bring this to life for you in",
      "timestamp": "02:55"
    },
    {
      "start": 178.319,
      "duration": 1.761,
      "text": "just a moment but first I want to tell",
      "timestamp": "02:58"
    },
    {
      "start": 180.08,
      "duration": 2.239,
      "text": "you a little bit more about the chip we",
      "timestamp": "03:00"
    },
    {
      "start": 182.319,
      "duration": 2.161,
      "text": "know about CPUs we know about GPUs i",
      "timestamp": "03:02"
    },
    {
      "start": 184.48,
      "duration": 1.759,
      "text": "would like to argue that the LPU is",
      "timestamp": "03:04"
    },
    {
      "start": 186.239,
      "duration": 3.121,
      "text": "going to be the next major architectural",
      "timestamp": "03:06"
    },
    {
      "start": 189.36,
      "duration": 4.48,
      "text": "shift in AI in computing our LPU is",
      "timestamp": "03:09"
    },
    {
      "start": 193.84,
      "duration": 3.28,
      "text": "substantially faster than GPUs four",
      "timestamp": "03:13"
    },
    {
      "start": 197.12,
      "duration": 4.0,
      "text": "times faster oneif of the cost",
      "timestamp": "03:17"
    },
    {
      "start": 201.12,
      "duration": 2.08,
      "text": "onethird of the power and this gap is",
      "timestamp": "03:21"
    },
    {
      "start": 203.2,
      "duration": 2.56,
      "text": "growing substantially",
      "timestamp": "03:23"
    },
    {
      "start": 205.76,
      "duration": 1.68,
      "text": "we can achieve this because we have a",
      "timestamp": "03:25"
    },
    {
      "start": 207.44,
      "duration": 2.48,
      "text": "fundamentally different architecture it",
      "timestamp": "03:27"
    },
    {
      "start": 209.92,
      "duration": 1.92,
      "text": "fundamentally looks different here's a",
      "timestamp": "03:29"
    },
    {
      "start": 211.84,
      "duration": 2.64,
      "text": "picture of a GPU and on the right you",
      "timestamp": "03:31"
    },
    {
      "start": 214.48,
      "duration": 2.319,
      "text": "see a picture of our LPU it just looks",
      "timestamp": "03:34"
    },
    {
      "start": 216.799,
      "duration": 2.481,
      "text": "different it's actually designed around",
      "timestamp": "03:36"
    },
    {
      "start": 219.28,
      "duration": 1.92,
      "text": "this concept of an assembly line like a",
      "timestamp": "03:39"
    },
    {
      "start": 221.2,
      "duration": 2.8,
      "text": "factory floor data comes in one end",
      "timestamp": "03:41"
    },
    {
      "start": 224.0,
      "duration": 2.56,
      "text": "tokens come out the other",
      "timestamp": "03:44"
    },
    {
      "start": 226.56,
      "duration": 1.679,
      "text": "there's fundamental architectural",
      "timestamp": "03:46"
    },
    {
      "start": 228.239,
      "duration": 2.401,
      "text": "differences about this chip about the",
      "timestamp": "03:48"
    },
    {
      "start": 230.64,
      "duration": 2.56,
      "text": "memory architecture about the design",
      "timestamp": "03:50"
    },
    {
      "start": 233.2,
      "duration": 1.84,
      "text": "i'll even let you know that the chip",
      "timestamp": "03:53"
    },
    {
      "start": 235.04,
      "duration": 1.759,
      "text": "itself the LPU that you see pictured",
      "timestamp": "03:55"
    },
    {
      "start": 236.799,
      "duration": 3.921,
      "text": "here is a 14 nanometer chip 10-year-old",
      "timestamp": "03:56"
    },
    {
      "start": 240.72,
      "duration": 2.239,
      "text": "fabrication technology",
      "timestamp": "04:00"
    },
    {
      "start": 242.959,
      "duration": 2.881,
      "text": "and it outperforms GPUs because of that",
      "timestamp": "04:02"
    },
    {
      "start": 245.84,
      "duration": 2.88,
      "text": "fundamental difference in architecture",
      "timestamp": "04:05"
    },
    {
      "start": 248.72,
      "duration": 2.239,
      "text": "and all of the software that defines how",
      "timestamp": "04:08"
    },
    {
      "start": 250.959,
      "duration": 3.241,
      "text": "it works",
      "timestamp": "04:10"
    },
    {
      "start": 254.4,
      "duration": 2.399,
      "text": "so I want to bring this to live for you",
      "timestamp": "04:14"
    },
    {
      "start": 256.799,
      "duration": 3.201,
      "text": "so let's do a demo",
      "timestamp": "04:16"
    },
    {
      "start": 260.0,
      "duration": 3.36,
      "text": "so one of the most important",
      "timestamp": "04:20"
    },
    {
      "start": 263.36,
      "duration": 2.32,
      "text": "AI applications that is going to be",
      "timestamp": "04:23"
    },
    {
      "start": 265.68,
      "duration": 2.959,
      "text": "ubiquitous all over the world is voice",
      "timestamp": "04:25"
    },
    {
      "start": 268.639,
      "duration": 1.761,
      "text": "we want to talk to our phones we want",
      "timestamp": "04:28"
    },
    {
      "start": 270.4,
      "duration": 1.519,
      "text": "the phones to talk back to us we want",
      "timestamp": "04:30"
    },
    {
      "start": 271.919,
      "duration": 1.84,
      "text": "our cars to talk back to us we're going",
      "timestamp": "04:31"
    },
    {
      "start": 273.759,
      "duration": 1.041,
      "text": "to get on the phone with customer",
      "timestamp": "04:33"
    },
    {
      "start": 274.8,
      "duration": 2.399,
      "text": "support it's going to be voiceto if you",
      "timestamp": "04:34"
    },
    {
      "start": 277.199,
      "duration": 2.161,
      "text": "saw the demo from Phone which is powered",
      "timestamp": "04:37"
    },
    {
      "start": 279.36,
      "duration": 2.32,
      "text": "by Grock earlier today and that's a",
      "timestamp": "04:39"
    },
    {
      "start": 281.68,
      "duration": 1.28,
      "text": "great example of voicetovoice",
      "timestamp": "04:41"
    },
    {
      "start": 282.96,
      "duration": 1.76,
      "text": "interactions so I'm going to do a live",
      "timestamp": "04:42"
    },
    {
      "start": 284.72,
      "duration": 2.16,
      "text": "demo i'm going to talk to a voice agent",
      "timestamp": "04:44"
    },
    {
      "start": 286.88,
      "duration": 4.12,
      "text": "u let's see how it goes",
      "timestamp": "04:46"
    },
    {
      "start": 298.16,
      "duration": 2.72,
      "text": "hello Super AI so excited to be talking",
      "timestamp": "04:58"
    },
    {
      "start": 300.88,
      "duration": 2.08,
      "text": "to you",
      "timestamp": "05:00"
    },
    {
      "start": 302.96,
      "duration": 1.76,
      "text": "hi you tell me about yourself\n it's great",
      "timestamp": "05:02"
    },
    {
      "start": 304.72,
      "duration": 1.84,
      "text": "connect",
      "timestamp": "05:04"
    },
    {
      "start": 306.56,
      "duration": 3.68,
      "text": "hi how can I help you today\n can you tell",
      "timestamp": "05:06"
    },
    {
      "start": 310.24,
      "duration": 2.72,
      "text": "me about yourself",
      "timestamp": "05:10"
    },
    {
      "start": 312.96,
      "duration": 2.32,
      "text": "yes I'm talking to you right now i'm a",
      "timestamp": "05:12"
    },
    {
      "start": 315.28,
      "duration": 2.32,
      "text": "voice AI assistant and our conversation",
      "timestamp": "05:15"
    },
    {
      "start": 317.6,
      "duration": 2.56,
      "text": "is happening in real time i'm here to",
      "timestamp": "05:17"
    },
    {
      "start": 320.16,
      "duration": 3.28,
      "text": "help answer your questions or just chat",
      "timestamp": "05:20"
    },
    {
      "start": 323.44,
      "duration": 1.12,
      "text": "great can you tell me about what",
      "timestamp": "05:23"
    },
    {
      "start": 324.56,
      "duration": 3.96,
      "text": "technology you're built on",
      "timestamp": "05:24"
    },
    {
      "start": 328.8,
      "duration": 2.239,
      "text": "i am built using Pipecat an open source",
      "timestamp": "05:28"
    },
    {
      "start": 331.039,
      "duration": 1.921,
      "text": "framework for building voice AI",
      "timestamp": "05:31"
    },
    {
      "start": 332.96,
      "duration": 2.64,
      "text": "applications and I'm powered by the Gro",
      "timestamp": "05:32"
    },
    {
      "start": 335.6,
      "duration": 2.08,
      "text": "AI platform which provides high",
      "timestamp": "05:35"
    },
    {
      "start": 337.68,
      "duration": 3.04,
      "text": "performance AI services",
      "timestamp": "05:37"
    },
    {
      "start": 340.72,
      "duration": 2.96,
      "text": "that's great can you tell me why fast AI",
      "timestamp": "05:40"
    },
    {
      "start": 343.68,
      "duration": 4.04,
      "text": "inference is important",
      "timestamp": "05:43"
    },
    {
      "start": 349.28,
      "duration": 1.919,
      "text": "fast AI inference is crucial for",
      "timestamp": "05:49"
    },
    {
      "start": 351.199,
      "duration": 1.761,
      "text": "applications that require real-time",
      "timestamp": "05:51"
    },
    {
      "start": 352.96,
      "duration": 2.4,
      "text": "processing like voice assistants chat",
      "timestamp": "05:52"
    },
    {
      "start": 355.36,
      "duration": 2.48,
      "text": "bots and other interactive system it",
      "timestamp": "05:55"
    },
    {
      "start": 357.84,
      "duration": 1.76,
      "text": "enables seamless and responsive",
      "timestamp": "05:57"
    },
    {
      "start": 359.6,
      "duration": 2.159,
      "text": "interactions making it possible to",
      "timestamp": "05:59"
    },
    {
      "start": 361.759,
      "duration": 2.16,
      "text": "deliver high-quality user experiences",
      "timestamp": "06:01"
    },
    {
      "start": 363.919,
      "duration": 2.801,
      "text": "gro's language processing units LPUs are",
      "timestamp": "06:03"
    },
    {
      "start": 366.72,
      "duration": 2.08,
      "text": "designed to accelerate AI inference",
      "timestamp": "06:06"
    },
    {
      "start": 368.8,
      "duration": 2.239,
      "text": "achieving industryleading speed and low",
      "timestamp": "06:08"
    },
    {
      "start": 371.039,
      "duration": 1.841,
      "text": "latency\n that's great\n which is a key",
      "timestamp": "06:11"
    },
    {
      "start": 372.88,
      "duration": 2.8,
      "text": "factor in making real\n Okay\n I'm glad",
      "timestamp": "06:12"
    },
    {
      "start": 375.68,
      "duration": 1.6,
      "text": "you're interested we're we're going have",
      "timestamp": "06:15"
    },
    {
      "start": 377.28,
      "duration": 1.44,
      "text": "to keep going thanks for the demo\n if you",
      "timestamp": "06:17"
    },
    {
      "start": 378.72,
      "duration": 2.56,
      "text": "have any",
      "timestamp": "06:18"
    },
    {
      "start": 381.28,
      "duration": 2.32,
      "text": "The fun doesn't stop here our audience",
      "timestamp": "06:21"
    },
    {
      "start": 383.6,
      "duration": 3.12,
      "text": "should visit the Gro booth in the expo",
      "timestamp": "06:23"
    },
    {
      "start": 386.72,
      "duration": 2.16,
      "text": "to learn more about Grock and how to",
      "timestamp": "06:26"
    },
    {
      "start": 388.88,
      "duration": 2.08,
      "text": "leverage real time AI inference for",
      "timestamp": "06:28"
    },
    {
      "start": 390.96,
      "duration": 3.679,
      "text": "their use cases from you and your team",
      "timestamp": "06:30"
    },
    {
      "start": 394.639,
      "duration": 1.441,
      "text": "so there you go there's a there's a",
      "timestamp": "06:34"
    },
    {
      "start": 396.08,
      "duration": 2.08,
      "text": "simple example of a voicetooice demo",
      "timestamp": "06:36"
    },
    {
      "start": 398.16,
      "duration": 1.36,
      "text": "i'll tell you that the infrastructure",
      "timestamp": "06:38"
    },
    {
      "start": 399.52,
      "duration": 3.76,
      "text": "running on that demo is in North America",
      "timestamp": "06:39"
    },
    {
      "start": 403.28,
      "duration": 1.919,
      "text": "so we had to send that request all the",
      "timestamp": "06:43"
    },
    {
      "start": 405.199,
      "duration": 2.241,
      "text": "way to the US we had to process it on",
      "timestamp": "06:45"
    },
    {
      "start": 407.44,
      "duration": 2.4,
      "text": "LPUs we had to send it all the way back",
      "timestamp": "06:47"
    },
    {
      "start": 409.84,
      "duration": 1.44,
      "text": "so you might have noticed a very small",
      "timestamp": "06:49"
    },
    {
      "start": 411.28,
      "duration": 1.68,
      "text": "delay that was part of the network",
      "timestamp": "06:51"
    },
    {
      "start": 412.96,
      "duration": 2.079,
      "text": "transit time uh ultimately that",
      "timestamp": "06:52"
    },
    {
      "start": 415.039,
      "duration": 1.44,
      "text": "infrastructure is going to be ubiquitous",
      "timestamp": "06:55"
    },
    {
      "start": 416.479,
      "duration": 1.681,
      "text": "everywhere and so these voicetooice",
      "timestamp": "06:56"
    },
    {
      "start": 418.16,
      "duration": 2.879,
      "text": "agents are going to be everywhere",
      "timestamp": "06:58"
    },
    {
      "start": 421.039,
      "duration": 1.761,
      "text": "i want to show you one more demo i had",
      "timestamp": "07:01"
    },
    {
      "start": 422.8,
      "duration": 2.16,
      "text": "to record this one um but everyone",
      "timestamp": "07:02"
    },
    {
      "start": 424.96,
      "duration": 1.6,
      "text": "always asks me well how is this",
      "timestamp": "07:04"
    },
    {
      "start": 426.56,
      "duration": 2.32,
      "text": "different than a GPU what's the actual",
      "timestamp": "07:06"
    },
    {
      "start": 428.88,
      "duration": 1.52,
      "text": "relative performance difference so I",
      "timestamp": "07:08"
    },
    {
      "start": 430.4,
      "duration": 2.239,
      "text": "wanted to try to visualize that for you",
      "timestamp": "07:10"
    },
    {
      "start": 432.639,
      "duration": 2.321,
      "text": "and so I'm going to play a video for you",
      "timestamp": "07:12"
    },
    {
      "start": 434.96,
      "duration": 1.519,
      "text": "this video is going to show Grock",
      "timestamp": "07:14"
    },
    {
      "start": 436.479,
      "duration": 1.601,
      "text": "powered by our Gro cloud platform",
      "timestamp": "07:16"
    },
    {
      "start": 438.08,
      "duration": 2.48,
      "text": "running Llama 4 Scout compared to Nvidia",
      "timestamp": "07:18"
    },
    {
      "start": 440.56,
      "duration": 2.72,
      "text": "GPUs compared to OpenAI we're going to",
      "timestamp": "07:20"
    },
    {
      "start": 443.28,
      "duration": 1.84,
      "text": "just give it a prompt all three models",
      "timestamp": "07:23"
    },
    {
      "start": 445.12,
      "duration": 1.919,
      "text": "all three services at the same time and",
      "timestamp": "07:25"
    },
    {
      "start": 447.039,
      "duration": 2.72,
      "text": "I'm going to use a coding example uh",
      "timestamp": "07:27"
    },
    {
      "start": 449.759,
      "duration": 3.041,
      "text": "here we go let's wait for this to load",
      "timestamp": "07:29"
    },
    {
      "start": 452.8,
      "duration": 2.16,
      "text": "i'm going to ask it to code a simple",
      "timestamp": "07:32"
    },
    {
      "start": 454.96,
      "duration": 3.28,
      "text": "simple game in Python so write me a",
      "timestamp": "07:34"
    },
    {
      "start": 458.24,
      "duration": 3.359,
      "text": "Python script for the game Pac-Man the",
      "timestamp": "07:38"
    },
    {
      "start": 461.599,
      "duration": 1.121,
      "text": "request is going to go to all three",
      "timestamp": "07:41"
    },
    {
      "start": 462.72,
      "duration": 2.08,
      "text": "models simultaneously all three services",
      "timestamp": "07:42"
    },
    {
      "start": 464.8,
      "duration": 2.48,
      "text": "and you're going to see the results",
      "timestamp": "07:44"
    },
    {
      "start": 467.28,
      "duration": 3.12,
      "text": "so Grock is on the left it's done nvidia",
      "timestamp": "07:47"
    },
    {
      "start": 470.4,
      "duration": 1.84,
      "text": "running the same model H100s it will",
      "timestamp": "07:50"
    },
    {
      "start": 472.24,
      "duration": 2.0,
      "text": "finish and then OpenAI it's a bit more",
      "timestamp": "07:52"
    },
    {
      "start": 474.24,
      "duration": 2.079,
      "text": "verbose a bit more chatty so it'll it'll",
      "timestamp": "07:54"
    },
    {
      "start": 476.319,
      "duration": 2.081,
      "text": "finish soon but I want to draw your",
      "timestamp": "07:56"
    },
    {
      "start": 478.4,
      "duration": 1.6,
      "text": "attention to what the the number at the",
      "timestamp": "07:58"
    },
    {
      "start": 480.0,
      "duration": 2.16,
      "text": "very top if you can see that we were",
      "timestamp": "08:00"
    },
    {
      "start": 482.16,
      "duration": 3.36,
      "text": "doing over 400 tokens per second",
      "timestamp": "08:02"
    },
    {
      "start": 485.52,
      "duration": 3.119,
      "text": "400 tokens per second most GPUs today do",
      "timestamp": "08:05"
    },
    {
      "start": 488.639,
      "duration": 2.641,
      "text": "less than 100 G tokens per second now",
      "timestamp": "08:08"
    },
    {
      "start": 491.28,
      "duration": 1.68,
      "text": "with new generations that will get a bit",
      "timestamp": "08:11"
    },
    {
      "start": 492.96,
      "duration": 2.639,
      "text": "faster but we're at 500 and we're going",
      "timestamp": "08:12"
    },
    {
      "start": 495.599,
      "duration": 3.04,
      "text": "to go way way past that this is again 14",
      "timestamp": "08:15"
    },
    {
      "start": 498.639,
      "duration": 2.4,
      "text": "nanometer technology 10 years old it's",
      "timestamp": "08:18"
    },
    {
      "start": 501.039,
      "duration": 2.88,
      "text": "more than five times faster than GPUs so",
      "timestamp": "08:21"
    },
    {
      "start": 503.919,
      "duration": 2.081,
      "text": "if you're an engineer you're a developer",
      "timestamp": "08:23"
    },
    {
      "start": 506.0,
      "duration": 2.08,
      "text": "you're building co-pilots voice agents",
      "timestamp": "08:26"
    },
    {
      "start": 508.08,
      "duration": 2.64,
      "text": "anything that requires low latency this",
      "timestamp": "08:28"
    },
    {
      "start": 510.72,
      "duration": 2.64,
      "text": "is critical to power those applications",
      "timestamp": "08:30"
    },
    {
      "start": 513.36,
      "duration": 1.359,
      "text": "and we think this is going to open up",
      "timestamp": "08:33"
    },
    {
      "start": 514.719,
      "duration": 2.88,
      "text": "the next range of applications that's",
      "timestamp": "08:34"
    },
    {
      "start": 517.599,
      "duration": 3.761,
      "text": "really going to transform the world",
      "timestamp": "08:37"
    },
    {
      "start": 521.36,
      "duration": 3.599,
      "text": "okay so to recap Grock is producing a",
      "timestamp": "08:41"
    },
    {
      "start": 524.959,
      "duration": 2.481,
      "text": "chip we call it the LPU but it's not",
      "timestamp": "08:44"
    },
    {
      "start": 527.44,
      "duration": 1.92,
      "text": "doesn't stop there we like to say we",
      "timestamp": "08:47"
    },
    {
      "start": 529.36,
      "duration": 2.96,
      "text": "build from the silicon to the cloud so",
      "timestamp": "08:49"
    },
    {
      "start": 532.32,
      "duration": 2.079,
      "text": "we design the chips again manufactured",
      "timestamp": "08:52"
    },
    {
      "start": 534.399,
      "duration": 2.0,
      "text": "in North America we package it all in",
      "timestamp": "08:54"
    },
    {
      "start": 536.399,
      "duration": 3.201,
      "text": "North America we design the networking",
      "timestamp": "08:56"
    },
    {
      "start": 539.6,
      "duration": 2.0,
      "text": "and we build the full cloud software",
      "timestamp": "08:59"
    },
    {
      "start": 541.6,
      "duration": 3.28,
      "text": "service that allows developers to access",
      "timestamp": "09:01"
    },
    {
      "start": 544.88,
      "duration": 3.28,
      "text": "AI models running on our Grock hardware",
      "timestamp": "09:04"
    },
    {
      "start": 548.16,
      "duration": 2.56,
      "text": "so to show you a stat",
      "timestamp": "09:08"
    },
    {
      "start": 550.72,
      "duration": 3.44,
      "text": "we launched Grock Cloud in March of 2024",
      "timestamp": "09:10"
    },
    {
      "start": 554.16,
      "duration": 4.16,
      "text": "just over a year ago we had zero users",
      "timestamp": "09:14"
    },
    {
      "start": 558.32,
      "duration": 2.72,
      "text": "we were running one copy of Llama One or",
      "timestamp": "09:18"
    },
    {
      "start": 561.04,
      "duration": 3.44,
      "text": "two at the time we now have over 1.6",
      "timestamp": "09:21"
    },
    {
      "start": 564.48,
      "duration": 1.6,
      "text": "million developers i think we're adding",
      "timestamp": "09:24"
    },
    {
      "start": 566.08,
      "duration": 3.12,
      "text": "50,000 a week people are coming to Groth",
      "timestamp": "09:26"
    },
    {
      "start": 569.2,
      "duration": 1.28,
      "text": "because of the price performance",
      "timestamp": "09:29"
    },
    {
      "start": 570.48,
      "duration": 2.72,
      "text": "difference that we're able to deliver uh",
      "timestamp": "09:30"
    },
    {
      "start": 573.2,
      "duration": 3.96,
      "text": "in fast AI inference",
      "timestamp": "09:33"
    },
    {
      "start": 577.36,
      "duration": 1.919,
      "text": "versel if you know Verscell they are one",
      "timestamp": "09:37"
    },
    {
      "start": 579.279,
      "duration": 1.841,
      "text": "of the largest web developer app",
      "timestamp": "09:39"
    },
    {
      "start": 581.12,
      "duration": 1.76,
      "text": "platforms in the world i think they have",
      "timestamp": "09:41"
    },
    {
      "start": 582.88,
      "duration": 2.8,
      "text": "50 million developers building web apps",
      "timestamp": "09:42"
    },
    {
      "start": 585.68,
      "duration": 2.159,
      "text": "they ran a study recently and said who",
      "timestamp": "09:45"
    },
    {
      "start": 587.839,
      "duration": 2.801,
      "text": "are you switching from uh when you in",
      "timestamp": "09:47"
    },
    {
      "start": 590.64,
      "duration": 3.12,
      "text": "the context of AI and most people were",
      "timestamp": "09:50"
    },
    {
      "start": 593.76,
      "duration": 2.56,
      "text": "switching from open AI or maybe one of",
      "timestamp": "09:53"
    },
    {
      "start": 596.32,
      "duration": 2.0,
      "text": "the hypers scale cloud providers and",
      "timestamp": "09:56"
    },
    {
      "start": 598.32,
      "duration": 1.84,
      "text": "Grock was number three this was now a",
      "timestamp": "09:58"
    },
    {
      "start": 600.16,
      "duration": 1.2,
      "text": "little while ago i think that number is",
      "timestamp": "10:00"
    },
    {
      "start": 601.36,
      "duration": 2.32,
      "text": "probably a bit higher but Grock is very",
      "timestamp": "10:01"
    },
    {
      "start": 603.68,
      "duration": 2.4,
      "text": "rapidly from a standstill gaining",
      "timestamp": "10:03"
    },
    {
      "start": 606.08,
      "duration": 1.84,
      "text": "tremendous mind share and market share",
      "timestamp": "10:06"
    },
    {
      "start": 607.92,
      "duration": 2.479,
      "text": "in the fast AI inference and I believe",
      "timestamp": "10:07"
    },
    {
      "start": 610.399,
      "duration": 2.721,
      "text": "that we'll be the largest provider of AI",
      "timestamp": "10:10"
    },
    {
      "start": 613.12,
      "duration": 3.92,
      "text": "inference in the not too distant future",
      "timestamp": "10:13"
    },
    {
      "start": 617.04,
      "duration": 2.08,
      "text": "we even got a shout out from Zuckerberg",
      "timestamp": "10:17"
    },
    {
      "start": 619.12,
      "duration": 3.279,
      "text": "he's notoriously uh uh stingy with his",
      "timestamp": "10:19"
    },
    {
      "start": 622.399,
      "duration": 2.081,
      "text": "shoutouts for third party companies but",
      "timestamp": "10:22"
    },
    {
      "start": 624.48,
      "duration": 2.0,
      "text": "Grock is actually powering the Meta",
      "timestamp": "10:24"
    },
    {
      "start": 626.48,
      "duration": 2.72,
      "text": "Enterprise Llama API",
      "timestamp": "10:26"
    },
    {
      "start": 629.2,
      "duration": 2.319,
      "text": "you know we can read the quote the low",
      "timestamp": "10:29"
    },
    {
      "start": 631.519,
      "duration": 2.641,
      "text": "latency advantage of Grock at the lower",
      "timestamp": "10:31"
    },
    {
      "start": 634.16,
      "duration": 2.239,
      "text": "price is really the advantage we're",
      "timestamp": "10:34"
    },
    {
      "start": 636.399,
      "duration": 1.761,
      "text": "really trying to drive the cost of AI to",
      "timestamp": "10:36"
    },
    {
      "start": 638.16,
      "duration": 2.48,
      "text": "zero to enable you to build better and",
      "timestamp": "10:38"
    },
    {
      "start": 640.64,
      "duration": 3.639,
      "text": "more amazing products",
      "timestamp": "10:40"
    },
    {
      "start": 645.6,
      "duration": 2.08,
      "text": "but we're not stopping there i'm here in",
      "timestamp": "10:45"
    },
    {
      "start": 647.68,
      "duration": 1.279,
      "text": "Singapore i'm at this conference with",
      "timestamp": "10:47"
    },
    {
      "start": 648.959,
      "duration": 2.161,
      "text": "you i'm based here as I mentioned 10",
      "timestamp": "10:48"
    },
    {
      "start": 651.12,
      "duration": 2.959,
      "text": "years uh we're expanding internationally",
      "timestamp": "10:51"
    },
    {
      "start": 654.079,
      "duration": 2.161,
      "text": "grock expanded first to the Middle East",
      "timestamp": "10:54"
    },
    {
      "start": 656.24,
      "duration": 2.159,
      "text": "with our partnership with Aramco now",
      "timestamp": "10:56"
    },
    {
      "start": 658.399,
      "duration": 2.0,
      "text": "Humane AI in the Middle East in in the",
      "timestamp": "10:58"
    },
    {
      "start": 660.399,
      "duration": 1.68,
      "text": "Kingdom of Saudi Arabia where they're",
      "timestamp": "11:00"
    },
    {
      "start": 662.079,
      "duration": 1.76,
      "text": "building the world's largest AI",
      "timestamp": "11:02"
    },
    {
      "start": 663.839,
      "duration": 2.881,
      "text": "inference data center uh it's an",
      "timestamp": "11:03"
    },
    {
      "start": 666.72,
      "duration": 2.72,
      "text": "incredible feat uh that's powered by",
      "timestamp": "11:06"
    },
    {
      "start": 669.44,
      "duration": 2.32,
      "text": "Grock you know hundreds of thousands of",
      "timestamp": "11:09"
    },
    {
      "start": 671.76,
      "duration": 1.519,
      "text": "our chips will eventually be powering",
      "timestamp": "11:11"
    },
    {
      "start": 673.279,
      "duration": 2.321,
      "text": "this data center",
      "timestamp": "11:13"
    },
    {
      "start": 675.6,
      "duration": 1.84,
      "text": "we've also announced a partnership with",
      "timestamp": "11:15"
    },
    {
      "start": 677.44,
      "duration": 2.32,
      "text": "Bell Telco in Canada where we're",
      "timestamp": "11:17"
    },
    {
      "start": 679.76,
      "duration": 1.759,
      "text": "building the sovereign AI infrastructure",
      "timestamp": "11:19"
    },
    {
      "start": 681.519,
      "duration": 2.721,
      "text": "in Canada and we're having similar",
      "timestamp": "11:21"
    },
    {
      "start": 684.24,
      "duration": 2.32,
      "text": "discussions all over the world to build",
      "timestamp": "11:24"
    },
    {
      "start": 686.56,
      "duration": 2.24,
      "text": "localized infrastructure to power local",
      "timestamp": "11:26"
    },
    {
      "start": 688.8,
      "duration": 3.76,
      "text": "AI models to enable developers like you",
      "timestamp": "11:28"
    },
    {
      "start": 692.56,
      "duration": 1.68,
      "text": "and we're also releasing the latest and",
      "timestamp": "11:32"
    },
    {
      "start": 694.24,
      "duration": 2.08,
      "text": "greatest AI models we just released",
      "timestamp": "11:34"
    },
    {
      "start": 696.32,
      "duration": 3.92,
      "text": "Quinn 3 uh this past week we just",
      "timestamp": "11:36"
    },
    {
      "start": 700.24,
      "duration": 2.159,
      "text": "launched on Hugging Face so if you're a",
      "timestamp": "11:40"
    },
    {
      "start": 702.399,
      "duration": 1.841,
      "text": "developer you use hugging face you use",
      "timestamp": "11:42"
    },
    {
      "start": 704.24,
      "duration": 3.039,
      "text": "open router versel you can find gro in",
      "timestamp": "11:44"
    },
    {
      "start": 707.279,
      "duration": 1.68,
      "text": "all these platforms and you can deploy",
      "timestamp": "11:47"
    },
    {
      "start": 708.959,
      "duration": 3.0,
      "text": "it",
      "timestamp": "11:48"
    },
    {
      "start": 712.399,
      "duration": 3.361,
      "text": "so I'm going to leave you here uh just",
      "timestamp": "11:52"
    },
    {
      "start": 715.76,
      "duration": 3.04,
      "text": "want to deliver a few key messages",
      "timestamp": "11:55"
    },
    {
      "start": 718.8,
      "duration": 3.76,
      "text": "grock is growing very very fast we are",
      "timestamp": "11:58"
    },
    {
      "start": 722.56,
      "duration": 1.92,
      "text": "going to be everywhere we're already",
      "timestamp": "12:02"
    },
    {
      "start": 724.48,
      "duration": 2.24,
      "text": "here in Asia over half of our global",
      "timestamp": "12:04"
    },
    {
      "start": 726.72,
      "duration": 1.919,
      "text": "developers are here we're supporting",
      "timestamp": "12:06"
    },
    {
      "start": 728.639,
      "duration": 1.681,
      "text": "hundreds of thousands of you but I think",
      "timestamp": "12:08"
    },
    {
      "start": 730.32,
      "duration": 1.759,
      "text": "many people in this room are actually uh",
      "timestamp": "12:10"
    },
    {
      "start": 732.079,
      "duration": 1.601,
      "text": "users or customers of Gro which is",
      "timestamp": "12:12"
    },
    {
      "start": 733.68,
      "duration": 2.08,
      "text": "fantastic we're going to continue to",
      "timestamp": "12:13"
    },
    {
      "start": 735.76,
      "duration": 1.44,
      "text": "push the envelope and provide the",
      "timestamp": "12:15"
    },
    {
      "start": 737.2,
      "duration": 2.24,
      "text": "world's fastest AI inference we're going",
      "timestamp": "12:17"
    },
    {
      "start": 739.44,
      "duration": 2.48,
      "text": "to do that at the most competitive price",
      "timestamp": "12:19"
    },
    {
      "start": 741.92,
      "duration": 1.28,
      "text": "and we're going to provide as many of",
      "timestamp": "12:21"
    },
    {
      "start": 743.2,
      "duration": 2.56,
      "text": "the models uh that you need to build as",
      "timestamp": "12:23"
    },
    {
      "start": 745.76,
      "duration": 1.12,
      "text": "many of the tools and enterprise",
      "timestamp": "12:25"
    },
    {
      "start": 746.88,
      "duration": 2.48,
      "text": "features that you need to build",
      "timestamp": "12:26"
    },
    {
      "start": 749.36,
      "duration": 1.84,
      "text": "so I hope that was useful i hope that",
      "timestamp": "12:29"
    },
    {
      "start": 751.2,
      "duration": 1.759,
      "text": "was interesting for you if you have more",
      "timestamp": "12:31"
    },
    {
      "start": 752.959,
      "duration": 1.921,
      "text": "questions there is a Grock booth just",
      "timestamp": "12:32"
    },
    {
      "start": 754.88,
      "duration": 1.84,
      "text": "past the main stage please come say",
      "timestamp": "12:34"
    },
    {
      "start": 756.72,
      "duration": 4.159,
      "text": "hello all right thank you",
      "timestamp": "12:36"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.363407",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}