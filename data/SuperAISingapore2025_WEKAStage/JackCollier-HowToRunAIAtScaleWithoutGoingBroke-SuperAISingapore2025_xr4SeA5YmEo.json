{
  "video_id": "xr4SeA5YmEo",
  "video_title": "Jack Collier - How To Run AI At Scale Without Going Broke - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=xr4SeA5YmEo",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T13:03:30+00:00",
  "duration_seconds": null,
  "view_count": 18,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: How To Run AI At Scale Without Going Broke (Or Waiting Months)\n\nSpeaker:\nJack Collier, CMO @ io.net\n\nStage: WEKA Stage\n#superai #ionet #aihardware #depin #gpus \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 323,
    "aggregated_text": "so today I'm going to give you a bit of a look under the hood at the tech at IO.net And also I really want to sort of spend some time going into the philosophy about what we're building and why we're building it As we all know AI is moving really fast And what we're starting to see is that the traditional infrastructure that has got us to this point is starting to crack It's starting to show its weaknesses and it's starting to slow people down Coupled with the fact that we're also starting to sleepwalk into what potentially could be uh a dystopian future whereby uh the power of the LLMs are actually kept behind closed doors with uh with the few as opposed to the many And that's very much sort of the philosophy that we try and uh take to heart at IO.NET when we're building our product and our service And we really sort of try and put accessibility flexibility cost effectiveness and indeed fairness and empowerment into the products that we build And so today I'm just going to give you a bit of a look under the hood about the problem but also how we're trying to address it So AI is scaling of course we see it here in the conference but under the hood that infrastructure really isn't We see the demand for compute exploding So you can see from the right here we see by 2030 we'll have an exponential increase in the amount of compute power required to power all of the needs in AI as the models are getting more complicated and as application usage goes up Really the underlying infrastructure is really struggling to keep up A stat that really keeps me awake at night is we need to double our GPU power every five or six months just to keep up with the current level of demand And so while everyone talks about deploying agents and leveraging rag etc nobody really talks about where to run it or how and when someone tries to actually come and do that the answer is increasingly become well nowhere easily uh and and nowhere really fast compound that problem 65% of compute power is actually uh owned by three companies And so that creates an over centralization problem uh which we're all facing as well And in that uh vein we also see that LLM usage um is actually pretty closed And we talk about AI being in benefit of everybody Um but yeah you know 70% of of LLM uses comes through closed APIs and only three comes from 30% comes from open source APIs And the stat that really gets me is 83% of teams actually would prefer to use open-source models but the actual infrastructure uh and ability to productize those open source APIs just isn't there It's not simple enough It's not intuitive enough intuitive enough And that's something again at IO.net that we really take to heart when building the products and services uh that we provide And the last thing is like the hyperscalers really have got us to where we are today Um and they power most of of of AI as we know it but they're starting to show their flaws as I've mentioned Often very costly often very opaque and often people have to wait a long time to be able to access those tools And so again that's something that we believe we want to change We believe that we that infrastructure should be more open It should be more distributed and flexible and it should be fairer for people And so that's the foundations that we're building on at IO So I'm just going to give you a little look under the hood at uh at the io.net uh stack And so we are the intelligent stack for AI workloads Uh we're made up of uh three layers of our stack At the bottom we have IO cloud which is our decentralized compute product So you'll be able to spin up a GPU cluster within two minutes just at the click of a few buttons and I'll show you how in a minute We then have our IO intelligence layer which is our uh AI infrastructure layer where you can leverage a whole host of open source models um at the click of a button uh be able to use rag and uh be able to leverage um our agent builder basically all the tools that you need to sort of interact with AI all in one place in one unified API And then the top layer is our IO application layer And that is just a showcase of all of the applications and customers that are sort of leveraging the stack that we have And what I'm going to show you today is how we can actually do this thanks to uh distributed systems and decentralization How you can do it in a cost-ffective way Whether you're just a developer who wants to just spin up uh one GPU unit to do fine training or whatever all the way through to an enterprise who needs you know much much bigger scale um our stack is flexible enough to be able to deal with that So let's start at the bottom with our IO cloud product So these are the three simple steps to be able to spin up a cloud on a GPU on IO.NET You first select the setup whether that is a container whether that is Ray Megaay or just a bare metal machine You decide which uh which setup you want You select the type of GPU device that you want the chipset that you want whether you want high performing devices like H200s H100s or whether you want something a little bit more cost-ffective that's there at the click of a button Um we have a variety of different locations that you can spin up your device in um over 138 different countries and then you configure your device add your payment details and click deploy And within two minutes you've got access to high performing GPU um GPU uh clusters within under two minutes And that's all without submitting a support ticket without speaking to a human and with no lockin You pay for what you use And so you're all probably thinking okay great Jack How does this actually work Surely too good to be true right Um and it's actually because of the nature of how we've built our product um that we can actually do this This is what we call decentralized physical infrastructure in action or deep in in the uh web3 space It's where we can tap into the idle GPU use GPU usage across the world as part and as part of our network people can volunteer that GPU onto the network stake um IO tokens to be able to guarantee the performance of those devices and then we can show those devices to um users who might want to use them um and they can obviously spin up and and use the IO.net product to sort of deploy the GPU from there So we're tapping into that 90% of the world's idle GPU processors to be able to do that So we don't own the data centers but we coordinate uh the different supply to meet the demands that the customers have Very novel way to solve the problem Right Let's go up the stack The second layer is our IO intelligence layer which is our open-source AI studio And as I sort of alluded to earlier um it's one single place with all of our AI tools and agents which are all unified through a single API API And what we find from speaking to engineers is that they're spending up to 50% of their time juggling different tools from different providers Whether that is using an API for inference whether that's some another product for rag another another for orchestration And what we're trying to do with IO intelligence is to remove um that complexity within the system and restore time back into the hands of developers and engineers so that they can actually get the value from those uh from the tools that they have as opposed to just you know having to deal with the coordination of it all And we break the product down into three simple buckets We have our inference and finetuning tool where you can run open source models There's a whole host of open source models um at the touch of a few buttons Uh you can actually uh use our retrieval engine or our rag product to be able to ground those agents within your own data and your own context And like I say that's all unified through one single deployment API We then have our agent and model marketplace where you can leverage a whole host of custom agents that have been built to leverage as part of your products or service with no real configure no real setup costs and it's fully configurable to your needs And then lastly we have this is a product coming soon very very soon We have our agent builder product It's a basically a gentic workflow product where you can build your own custom agents with custom logic custom tools so that you can adapt the models to be able to meet the needs for you uh in a more cost-effective and simple way And like I say this entire infrastructure runs on the IO cloud platform which means we're able to offer these services with um incredibly low latency and incredibly low cost So removing the in for complexity and also saving you money And then I touched on the open and closed uh LLM problem at the start and actually IO intelligence really is a product that tries to overcome that problem and make open-source models more accessible and easier to integrate for people So you're no longer stuck by some of the problems that you have with closed source LLMs and you can move into one unified product where you can actually be more visible in how these models are making decisions You can be more uh open in how the innovation is being made and uh be easier to be able to flip between different models to get different types of output based on whatever whatever it is you're trying to achieve And then just a few things by the numbers on IO We've been live almost a year now and on our IO cloud platform um we've had almost well over $14 million worth of daily network earnings put through the uh put through the network and like I say because we're a decentralized uh network of GPUs You can actually uh you can actually see how much revenue is coming into the into the network and how much suppliers are actually using It's all open and verifiable and so that nobody can hide behind uh you know numbers or figures where we put all of ours out in the open And it's the same with our supply We have over 10,000 active GPUs on our network uh today And in terms of our IO intelligence uh product we have over 1 billion tokens being used per week on our open source models Um and so that's real usage um that people are sort of gaining value from the our products and service and along with over 5,000 different enterprises startups and developers leveraging that product as well It's not a prototype it's real technology in action And here's just a list of some of the partners that we work with who leverage uh the IO uh.NET full stack And so what's coming next So like the customers that we have who are constantly innovating using our technology we're always innovating too Um and we have an exciting road map ahead including training as a service obviously the agent builder that I've talked about uh multimodal AI support and then um orchestration with manage VMs coming uh early next year And so I mentioned I wouldn't just talk about the tech and the product but I'd also talk about philosophy This is our vision statement that we um we live and breathe by at IO.NET net when we're building our products and that is that we want to make it easy for anyone anywhere to access the tools to develop AI through reliable affordable and performant platforms and we always keep that to heart as we as we uh as we build So I encourage you guys to try us out Don't just take my word for it Um you know experience it for yourself So you can do that by going to io.net net and just like I say spinning up a cluster within 2 minutes um or deploying an agent on your product through our single deployment API using IO intelligence We've just launched applications for our hackathon our inaugural hackathon today uh which you can find out more details by scanning the QR code or going to io.net And lastly we've just launched a grants program for anybody who's just starting out in the industry and wants to benefit from um free startup uh compute credits to be able to power their organization Thank you very much and uh thank you Cheers",
    "text_length": 11920,
    "word_count": 2255
  },
  "segments": [
    {
      "start": 7.6,
      "duration": 1.44,
      "text": "so today I'm going to give you a bit of",
      "timestamp": "00:07"
    },
    {
      "start": 9.04,
      "duration": 2.08,
      "text": "a look under the hood at the tech at",
      "timestamp": "00:09"
    },
    {
      "start": 11.12,
      "duration": 1.599,
      "text": "IO.net",
      "timestamp": "00:11"
    },
    {
      "start": 12.719,
      "duration": 1.441,
      "text": "And also I really want to sort of spend",
      "timestamp": "00:12"
    },
    {
      "start": 14.16,
      "duration": 2.08,
      "text": "some time going into the philosophy",
      "timestamp": "00:14"
    },
    {
      "start": 16.24,
      "duration": 1.76,
      "text": "about what we're building and why we're",
      "timestamp": "00:16"
    },
    {
      "start": 18.0,
      "duration": 1.76,
      "text": "building it",
      "timestamp": "00:18"
    },
    {
      "start": 19.76,
      "duration": 3.12,
      "text": "As we all know AI is moving really fast",
      "timestamp": "00:19"
    },
    {
      "start": 22.88,
      "duration": 2.399,
      "text": "And what we're starting to see is that",
      "timestamp": "00:22"
    },
    {
      "start": 25.279,
      "duration": 1.92,
      "text": "the traditional infrastructure that has",
      "timestamp": "00:25"
    },
    {
      "start": 27.199,
      "duration": 2.561,
      "text": "got us to this point is starting to",
      "timestamp": "00:27"
    },
    {
      "start": 29.76,
      "duration": 2.24,
      "text": "crack It's starting to show its",
      "timestamp": "00:29"
    },
    {
      "start": 32.0,
      "duration": 2.32,
      "text": "weaknesses and it's starting to slow",
      "timestamp": "00:32"
    },
    {
      "start": 34.32,
      "duration": 4.0,
      "text": "people down Coupled with the fact that",
      "timestamp": "00:34"
    },
    {
      "start": 38.32,
      "duration": 2.239,
      "text": "we're also starting to sleepwalk into",
      "timestamp": "00:38"
    },
    {
      "start": 40.559,
      "duration": 2.481,
      "text": "what potentially could be uh a dystopian",
      "timestamp": "00:40"
    },
    {
      "start": 43.04,
      "duration": 3.039,
      "text": "future whereby uh the power of the LLMs",
      "timestamp": "00:43"
    },
    {
      "start": 46.079,
      "duration": 2.081,
      "text": "are actually kept behind closed doors",
      "timestamp": "00:46"
    },
    {
      "start": 48.16,
      "duration": 2.239,
      "text": "with uh with the few as opposed to the",
      "timestamp": "00:48"
    },
    {
      "start": 50.399,
      "duration": 2.32,
      "text": "many And that's very much sort of the",
      "timestamp": "00:50"
    },
    {
      "start": 52.719,
      "duration": 1.68,
      "text": "philosophy that we try and uh take to",
      "timestamp": "00:52"
    },
    {
      "start": 54.399,
      "duration": 1.68,
      "text": "heart at IO.NET when we're building our",
      "timestamp": "00:54"
    },
    {
      "start": 56.079,
      "duration": 2.32,
      "text": "product and our service And we really",
      "timestamp": "00:56"
    },
    {
      "start": 58.399,
      "duration": 2.561,
      "text": "sort of try and put accessibility",
      "timestamp": "00:58"
    },
    {
      "start": 60.96,
      "duration": 2.399,
      "text": "flexibility cost effectiveness and",
      "timestamp": "01:00"
    },
    {
      "start": 63.359,
      "duration": 2.08,
      "text": "indeed fairness and empowerment into the",
      "timestamp": "01:03"
    },
    {
      "start": 65.439,
      "duration": 1.921,
      "text": "products that we build And so today I'm",
      "timestamp": "01:05"
    },
    {
      "start": 67.36,
      "duration": 0.96,
      "text": "just going to give you a bit of a look",
      "timestamp": "01:07"
    },
    {
      "start": 68.32,
      "duration": 1.28,
      "text": "under the hood about the problem but",
      "timestamp": "01:08"
    },
    {
      "start": 69.6,
      "duration": 4.68,
      "text": "also how we're trying to address it",
      "timestamp": "01:09"
    },
    {
      "start": 74.72,
      "duration": 2.96,
      "text": "So AI is scaling of course we see it",
      "timestamp": "01:14"
    },
    {
      "start": 77.68,
      "duration": 1.92,
      "text": "here in the conference but under the",
      "timestamp": "01:17"
    },
    {
      "start": 79.6,
      "duration": 2.879,
      "text": "hood that infrastructure really isn't We",
      "timestamp": "01:19"
    },
    {
      "start": 82.479,
      "duration": 2.96,
      "text": "see the demand for compute exploding So",
      "timestamp": "01:22"
    },
    {
      "start": 85.439,
      "duration": 1.841,
      "text": "you can see from the right here we see",
      "timestamp": "01:25"
    },
    {
      "start": 87.28,
      "duration": 2.0,
      "text": "by 2030 we'll have an exponential",
      "timestamp": "01:27"
    },
    {
      "start": 89.28,
      "duration": 1.68,
      "text": "increase in the amount of compute power",
      "timestamp": "01:29"
    },
    {
      "start": 90.96,
      "duration": 3.28,
      "text": "required to power all of the needs in AI",
      "timestamp": "01:30"
    },
    {
      "start": 94.24,
      "duration": 1.28,
      "text": "as the models are getting more",
      "timestamp": "01:34"
    },
    {
      "start": 95.52,
      "duration": 2.16,
      "text": "complicated and as application usage",
      "timestamp": "01:35"
    },
    {
      "start": 97.68,
      "duration": 1.84,
      "text": "goes up Really the underlying",
      "timestamp": "01:37"
    },
    {
      "start": 99.52,
      "duration": 1.44,
      "text": "infrastructure is really struggling to",
      "timestamp": "01:39"
    },
    {
      "start": 100.96,
      "duration": 2.4,
      "text": "keep up A stat that really keeps me",
      "timestamp": "01:40"
    },
    {
      "start": 103.36,
      "duration": 2.48,
      "text": "awake at night is we need to double our",
      "timestamp": "01:43"
    },
    {
      "start": 105.84,
      "duration": 3.2,
      "text": "GPU power every five or six months just",
      "timestamp": "01:45"
    },
    {
      "start": 109.04,
      "duration": 1.359,
      "text": "to keep up with the current level of",
      "timestamp": "01:49"
    },
    {
      "start": 110.399,
      "duration": 3.441,
      "text": "demand And so while everyone talks about",
      "timestamp": "01:50"
    },
    {
      "start": 113.84,
      "duration": 3.76,
      "text": "deploying agents and leveraging rag etc",
      "timestamp": "01:53"
    },
    {
      "start": 117.6,
      "duration": 1.68,
      "text": "nobody really talks about where to run",
      "timestamp": "01:57"
    },
    {
      "start": 119.28,
      "duration": 1.839,
      "text": "it or how and when someone tries to",
      "timestamp": "01:59"
    },
    {
      "start": 121.119,
      "duration": 2.241,
      "text": "actually come and do that the answer is",
      "timestamp": "02:01"
    },
    {
      "start": 123.36,
      "duration": 2.96,
      "text": "increasingly become well nowhere easily",
      "timestamp": "02:03"
    },
    {
      "start": 126.32,
      "duration": 2.88,
      "text": "uh and and nowhere really fast",
      "timestamp": "02:06"
    },
    {
      "start": 129.2,
      "duration": 3.039,
      "text": "compound that problem 65% of compute",
      "timestamp": "02:09"
    },
    {
      "start": 132.239,
      "duration": 2.481,
      "text": "power is actually uh owned by three",
      "timestamp": "02:12"
    },
    {
      "start": 134.72,
      "duration": 2.239,
      "text": "companies And so that creates an over",
      "timestamp": "02:14"
    },
    {
      "start": 136.959,
      "duration": 2.081,
      "text": "centralization problem uh which we're",
      "timestamp": "02:16"
    },
    {
      "start": 139.04,
      "duration": 3.68,
      "text": "all facing as well And in that uh vein",
      "timestamp": "02:19"
    },
    {
      "start": 142.72,
      "duration": 3.12,
      "text": "we also see that LLM usage um is",
      "timestamp": "02:22"
    },
    {
      "start": 145.84,
      "duration": 2.479,
      "text": "actually pretty closed And we talk about",
      "timestamp": "02:25"
    },
    {
      "start": 148.319,
      "duration": 2.961,
      "text": "AI being in benefit of everybody Um but",
      "timestamp": "02:28"
    },
    {
      "start": 151.28,
      "duration": 2.72,
      "text": "yeah you know 70% of of LLM uses comes",
      "timestamp": "02:31"
    },
    {
      "start": 154.0,
      "duration": 1.92,
      "text": "through closed APIs and only three comes",
      "timestamp": "02:34"
    },
    {
      "start": 155.92,
      "duration": 3.92,
      "text": "from 30% comes from open source APIs And",
      "timestamp": "02:35"
    },
    {
      "start": 159.84,
      "duration": 3.52,
      "text": "the stat that really gets me is 83% of",
      "timestamp": "02:39"
    },
    {
      "start": 163.36,
      "duration": 2.239,
      "text": "teams actually would prefer to use",
      "timestamp": "02:43"
    },
    {
      "start": 165.599,
      "duration": 3.201,
      "text": "open-source models but the actual",
      "timestamp": "02:45"
    },
    {
      "start": 168.8,
      "duration": 2.719,
      "text": "infrastructure uh and ability to",
      "timestamp": "02:48"
    },
    {
      "start": 171.519,
      "duration": 2.241,
      "text": "productize those open source APIs just",
      "timestamp": "02:51"
    },
    {
      "start": 173.76,
      "duration": 2.0,
      "text": "isn't there It's not simple enough It's",
      "timestamp": "02:53"
    },
    {
      "start": 175.76,
      "duration": 2.16,
      "text": "not intuitive enough intuitive enough",
      "timestamp": "02:55"
    },
    {
      "start": 177.92,
      "duration": 1.679,
      "text": "And that's something again at IO.net",
      "timestamp": "02:57"
    },
    {
      "start": 179.599,
      "duration": 1.521,
      "text": "that we really take to heart when",
      "timestamp": "02:59"
    },
    {
      "start": 181.12,
      "duration": 1.36,
      "text": "building the products and services uh",
      "timestamp": "03:01"
    },
    {
      "start": 182.48,
      "duration": 2.399,
      "text": "that we provide",
      "timestamp": "03:02"
    },
    {
      "start": 184.879,
      "duration": 2.08,
      "text": "And the last thing is like the",
      "timestamp": "03:04"
    },
    {
      "start": 186.959,
      "duration": 1.601,
      "text": "hyperscalers really have got us to where",
      "timestamp": "03:06"
    },
    {
      "start": 188.56,
      "duration": 2.56,
      "text": "we are today Um and they power most of",
      "timestamp": "03:08"
    },
    {
      "start": 191.12,
      "duration": 3.039,
      "text": "of of AI as we know it but they're",
      "timestamp": "03:11"
    },
    {
      "start": 194.159,
      "duration": 1.44,
      "text": "starting to show their flaws as I've",
      "timestamp": "03:14"
    },
    {
      "start": 195.599,
      "duration": 2.961,
      "text": "mentioned Often very costly often very",
      "timestamp": "03:15"
    },
    {
      "start": 198.56,
      "duration": 2.319,
      "text": "opaque and often people have to wait a",
      "timestamp": "03:18"
    },
    {
      "start": 200.879,
      "duration": 1.761,
      "text": "long time to be able to access those",
      "timestamp": "03:20"
    },
    {
      "start": 202.64,
      "duration": 2.64,
      "text": "tools And so again that's something that",
      "timestamp": "03:22"
    },
    {
      "start": 205.28,
      "duration": 1.52,
      "text": "we believe we want to change We believe",
      "timestamp": "03:25"
    },
    {
      "start": 206.8,
      "duration": 1.92,
      "text": "that we that infrastructure should be",
      "timestamp": "03:26"
    },
    {
      "start": 208.72,
      "duration": 1.84,
      "text": "more open It should be more distributed",
      "timestamp": "03:28"
    },
    {
      "start": 210.56,
      "duration": 2.16,
      "text": "and flexible and it should be fairer for",
      "timestamp": "03:30"
    },
    {
      "start": 212.72,
      "duration": 2.48,
      "text": "people And so that's the foundations",
      "timestamp": "03:32"
    },
    {
      "start": 215.2,
      "duration": 2.24,
      "text": "that we're building on at IO So I'm just",
      "timestamp": "03:35"
    },
    {
      "start": 217.44,
      "duration": 1.439,
      "text": "going to give you a little look under",
      "timestamp": "03:37"
    },
    {
      "start": 218.879,
      "duration": 4.08,
      "text": "the hood at uh at the io.net uh stack",
      "timestamp": "03:38"
    },
    {
      "start": 222.959,
      "duration": 2.321,
      "text": "And so we are the intelligent stack for",
      "timestamp": "03:42"
    },
    {
      "start": 225.28,
      "duration": 2.879,
      "text": "AI workloads Uh we're made up of uh",
      "timestamp": "03:45"
    },
    {
      "start": 228.159,
      "duration": 2.481,
      "text": "three layers of our stack At the bottom",
      "timestamp": "03:48"
    },
    {
      "start": 230.64,
      "duration": 2.56,
      "text": "we have IO cloud which is our",
      "timestamp": "03:50"
    },
    {
      "start": 233.2,
      "duration": 2.64,
      "text": "decentralized compute product So you'll",
      "timestamp": "03:53"
    },
    {
      "start": 235.84,
      "duration": 2.24,
      "text": "be able to spin up a GPU cluster within",
      "timestamp": "03:55"
    },
    {
      "start": 238.08,
      "duration": 1.439,
      "text": "two minutes just at the click of a few",
      "timestamp": "03:58"
    },
    {
      "start": 239.519,
      "duration": 1.36,
      "text": "buttons and I'll show you how in a",
      "timestamp": "03:59"
    },
    {
      "start": 240.879,
      "duration": 2.961,
      "text": "minute We then have our IO intelligence",
      "timestamp": "04:00"
    },
    {
      "start": 243.84,
      "duration": 2.56,
      "text": "layer which is our uh AI infrastructure",
      "timestamp": "04:03"
    },
    {
      "start": 246.4,
      "duration": 1.52,
      "text": "layer where you can leverage a whole",
      "timestamp": "04:06"
    },
    {
      "start": 247.92,
      "duration": 2.64,
      "text": "host of open source models um at the",
      "timestamp": "04:07"
    },
    {
      "start": 250.56,
      "duration": 2.48,
      "text": "click of a button uh be able to use rag",
      "timestamp": "04:10"
    },
    {
      "start": 253.04,
      "duration": 2.479,
      "text": "and uh be able to leverage um our agent",
      "timestamp": "04:13"
    },
    {
      "start": 255.519,
      "duration": 1.84,
      "text": "builder basically all the tools that you",
      "timestamp": "04:15"
    },
    {
      "start": 257.359,
      "duration": 2.481,
      "text": "need to sort of interact with AI all in",
      "timestamp": "04:17"
    },
    {
      "start": 259.84,
      "duration": 3.04,
      "text": "one place in one unified API",
      "timestamp": "04:19"
    },
    {
      "start": 262.88,
      "duration": 1.84,
      "text": "And then the top layer is our IO",
      "timestamp": "04:22"
    },
    {
      "start": 264.72,
      "duration": 1.84,
      "text": "application layer And that is just a",
      "timestamp": "04:24"
    },
    {
      "start": 266.56,
      "duration": 2.16,
      "text": "showcase of all of the applications and",
      "timestamp": "04:26"
    },
    {
      "start": 268.72,
      "duration": 1.12,
      "text": "customers that are sort of leveraging",
      "timestamp": "04:28"
    },
    {
      "start": 269.84,
      "duration": 3.2,
      "text": "the stack that we have And what I'm",
      "timestamp": "04:29"
    },
    {
      "start": 273.04,
      "duration": 1.2,
      "text": "going to show you today is how we can",
      "timestamp": "04:33"
    },
    {
      "start": 274.24,
      "duration": 2.08,
      "text": "actually do this thanks to uh",
      "timestamp": "04:34"
    },
    {
      "start": 276.32,
      "duration": 2.159,
      "text": "distributed systems and decentralization",
      "timestamp": "04:36"
    },
    {
      "start": 278.479,
      "duration": 2.16,
      "text": "How you can do it in a cost-ffective way",
      "timestamp": "04:38"
    },
    {
      "start": 280.639,
      "duration": 1.521,
      "text": "Whether you're just a developer who",
      "timestamp": "04:40"
    },
    {
      "start": 282.16,
      "duration": 2.64,
      "text": "wants to just spin up uh one GPU unit to",
      "timestamp": "04:42"
    },
    {
      "start": 284.8,
      "duration": 1.76,
      "text": "do fine training or whatever all the way",
      "timestamp": "04:44"
    },
    {
      "start": 286.56,
      "duration": 2.0,
      "text": "through to an enterprise who needs you",
      "timestamp": "04:46"
    },
    {
      "start": 288.56,
      "duration": 2.48,
      "text": "know much much bigger scale um our stack",
      "timestamp": "04:48"
    },
    {
      "start": 291.04,
      "duration": 1.36,
      "text": "is flexible enough to be able to deal",
      "timestamp": "04:51"
    },
    {
      "start": 292.4,
      "duration": 2.56,
      "text": "with that",
      "timestamp": "04:52"
    },
    {
      "start": 294.96,
      "duration": 2.16,
      "text": "So let's start at the bottom with our IO",
      "timestamp": "04:54"
    },
    {
      "start": 297.12,
      "duration": 3.44,
      "text": "cloud product So these are the three",
      "timestamp": "04:57"
    },
    {
      "start": 300.56,
      "duration": 1.84,
      "text": "simple steps to be able to spin up a",
      "timestamp": "05:00"
    },
    {
      "start": 302.4,
      "duration": 3.12,
      "text": "cloud on a GPU on IO.NET You first",
      "timestamp": "05:02"
    },
    {
      "start": 305.52,
      "duration": 2.239,
      "text": "select the setup whether that is a",
      "timestamp": "05:05"
    },
    {
      "start": 307.759,
      "duration": 2.16,
      "text": "container whether that is Ray Megaay or",
      "timestamp": "05:07"
    },
    {
      "start": 309.919,
      "duration": 2.081,
      "text": "just a bare metal machine You decide",
      "timestamp": "05:09"
    },
    {
      "start": 312.0,
      "duration": 2.56,
      "text": "which uh which setup you want You select",
      "timestamp": "05:12"
    },
    {
      "start": 314.56,
      "duration": 1.919,
      "text": "the type of GPU device that you want the",
      "timestamp": "05:14"
    },
    {
      "start": 316.479,
      "duration": 1.44,
      "text": "chipset that you want whether you want",
      "timestamp": "05:16"
    },
    {
      "start": 317.919,
      "duration": 2.801,
      "text": "high performing devices like H200s H100s",
      "timestamp": "05:17"
    },
    {
      "start": 320.72,
      "duration": 1.12,
      "text": "or whether you want something a little",
      "timestamp": "05:20"
    },
    {
      "start": 321.84,
      "duration": 2.0,
      "text": "bit more cost-ffective that's there at",
      "timestamp": "05:21"
    },
    {
      "start": 323.84,
      "duration": 2.48,
      "text": "the click of a button Um we have a",
      "timestamp": "05:23"
    },
    {
      "start": 326.32,
      "duration": 1.76,
      "text": "variety of different locations that you",
      "timestamp": "05:26"
    },
    {
      "start": 328.08,
      "duration": 2.88,
      "text": "can spin up your device in um over 138",
      "timestamp": "05:28"
    },
    {
      "start": 330.96,
      "duration": 1.92,
      "text": "different countries and then you",
      "timestamp": "05:30"
    },
    {
      "start": 332.88,
      "duration": 2.08,
      "text": "configure your device add your payment",
      "timestamp": "05:32"
    },
    {
      "start": 334.96,
      "duration": 3.04,
      "text": "details and click deploy And within two",
      "timestamp": "05:34"
    },
    {
      "start": 338.0,
      "duration": 1.759,
      "text": "minutes you've got access to high",
      "timestamp": "05:38"
    },
    {
      "start": 339.759,
      "duration": 4.241,
      "text": "performing GPU um GPU uh clusters within",
      "timestamp": "05:39"
    },
    {
      "start": 344.0,
      "duration": 2.56,
      "text": "under two minutes And that's all without",
      "timestamp": "05:44"
    },
    {
      "start": 346.56,
      "duration": 1.52,
      "text": "submitting a support ticket without",
      "timestamp": "05:46"
    },
    {
      "start": 348.08,
      "duration": 2.48,
      "text": "speaking to a human and with no lockin",
      "timestamp": "05:48"
    },
    {
      "start": 350.56,
      "duration": 3.88,
      "text": "You pay for what you use",
      "timestamp": "05:50"
    },
    {
      "start": 354.72,
      "duration": 2.16,
      "text": "And so you're all probably thinking okay",
      "timestamp": "05:54"
    },
    {
      "start": 356.88,
      "duration": 2.64,
      "text": "great Jack How does this actually work",
      "timestamp": "05:56"
    },
    {
      "start": 359.52,
      "duration": 2.64,
      "text": "Surely too good to be true right Um and",
      "timestamp": "05:59"
    },
    {
      "start": 362.16,
      "duration": 1.599,
      "text": "it's actually because of the nature of",
      "timestamp": "06:02"
    },
    {
      "start": 363.759,
      "duration": 2.0,
      "text": "how we've built our product um that we",
      "timestamp": "06:03"
    },
    {
      "start": 365.759,
      "duration": 1.841,
      "text": "can actually do this This is what we",
      "timestamp": "06:05"
    },
    {
      "start": 367.6,
      "duration": 1.76,
      "text": "call decentralized physical",
      "timestamp": "06:07"
    },
    {
      "start": 369.36,
      "duration": 2.399,
      "text": "infrastructure in action or deep in in",
      "timestamp": "06:09"
    },
    {
      "start": 371.759,
      "duration": 3.041,
      "text": "the uh web3 space It's where we can tap",
      "timestamp": "06:11"
    },
    {
      "start": 374.8,
      "duration": 3.119,
      "text": "into the idle GPU use GPU usage across",
      "timestamp": "06:14"
    },
    {
      "start": 377.919,
      "duration": 2.241,
      "text": "the world as part and as part of our",
      "timestamp": "06:17"
    },
    {
      "start": 380.16,
      "duration": 2.159,
      "text": "network people can volunteer that GPU",
      "timestamp": "06:20"
    },
    {
      "start": 382.319,
      "duration": 3.121,
      "text": "onto the network stake um IO tokens to",
      "timestamp": "06:22"
    },
    {
      "start": 385.44,
      "duration": 1.52,
      "text": "be able to guarantee the performance of",
      "timestamp": "06:25"
    },
    {
      "start": 386.96,
      "duration": 2.4,
      "text": "those devices and then we can show those",
      "timestamp": "06:26"
    },
    {
      "start": 389.36,
      "duration": 2.48,
      "text": "devices to um users who might want to",
      "timestamp": "06:29"
    },
    {
      "start": 391.84,
      "duration": 1.919,
      "text": "use them um and they can obviously spin",
      "timestamp": "06:31"
    },
    {
      "start": 393.759,
      "duration": 1.921,
      "text": "up and and use the IO.net product to",
      "timestamp": "06:33"
    },
    {
      "start": 395.68,
      "duration": 1.76,
      "text": "sort of deploy the GPU from there So",
      "timestamp": "06:35"
    },
    {
      "start": 397.44,
      "duration": 2.0,
      "text": "we're tapping into that 90% of the",
      "timestamp": "06:37"
    },
    {
      "start": 399.44,
      "duration": 2.4,
      "text": "world's idle GPU processors to be able",
      "timestamp": "06:39"
    },
    {
      "start": 401.84,
      "duration": 1.919,
      "text": "to do that So we don't own the data",
      "timestamp": "06:41"
    },
    {
      "start": 403.759,
      "duration": 1.841,
      "text": "centers but we coordinate uh the",
      "timestamp": "06:43"
    },
    {
      "start": 405.6,
      "duration": 1.92,
      "text": "different supply to meet the demands",
      "timestamp": "06:45"
    },
    {
      "start": 407.52,
      "duration": 2.08,
      "text": "that the customers have Very novel way",
      "timestamp": "06:47"
    },
    {
      "start": 409.6,
      "duration": 3.2,
      "text": "to solve the problem",
      "timestamp": "06:49"
    },
    {
      "start": 412.8,
      "duration": 2.32,
      "text": "Right Let's go up the stack The second",
      "timestamp": "06:52"
    },
    {
      "start": 415.12,
      "duration": 2.079,
      "text": "layer is our IO intelligence layer which",
      "timestamp": "06:55"
    },
    {
      "start": 417.199,
      "duration": 3.681,
      "text": "is our open-source AI studio And as I",
      "timestamp": "06:57"
    },
    {
      "start": 420.88,
      "duration": 2.319,
      "text": "sort of alluded to earlier um it's one",
      "timestamp": "07:00"
    },
    {
      "start": 423.199,
      "duration": 1.84,
      "text": "single place with all of our AI tools",
      "timestamp": "07:03"
    },
    {
      "start": 425.039,
      "duration": 1.921,
      "text": "and agents which are all unified through",
      "timestamp": "07:05"
    },
    {
      "start": 426.96,
      "duration": 2.639,
      "text": "a single API API",
      "timestamp": "07:06"
    },
    {
      "start": 429.599,
      "duration": 2.241,
      "text": "And what we find from speaking to",
      "timestamp": "07:09"
    },
    {
      "start": 431.84,
      "duration": 3.52,
      "text": "engineers is that they're spending up to",
      "timestamp": "07:11"
    },
    {
      "start": 435.36,
      "duration": 2.559,
      "text": "50% of their time juggling different",
      "timestamp": "07:15"
    },
    {
      "start": 437.919,
      "duration": 2.0,
      "text": "tools from different providers Whether",
      "timestamp": "07:17"
    },
    {
      "start": 439.919,
      "duration": 2.641,
      "text": "that is using an API for inference",
      "timestamp": "07:19"
    },
    {
      "start": 442.56,
      "duration": 1.52,
      "text": "whether that's some another product for",
      "timestamp": "07:22"
    },
    {
      "start": 444.08,
      "duration": 2.8,
      "text": "rag another another for orchestration",
      "timestamp": "07:24"
    },
    {
      "start": 446.88,
      "duration": 1.759,
      "text": "And what we're trying to do with IO",
      "timestamp": "07:26"
    },
    {
      "start": 448.639,
      "duration": 2.721,
      "text": "intelligence is to remove um that",
      "timestamp": "07:28"
    },
    {
      "start": 451.36,
      "duration": 2.08,
      "text": "complexity within the system and restore",
      "timestamp": "07:31"
    },
    {
      "start": 453.44,
      "duration": 2.56,
      "text": "time back into the hands of developers",
      "timestamp": "07:33"
    },
    {
      "start": 456.0,
      "duration": 1.84,
      "text": "and engineers so that they can actually",
      "timestamp": "07:36"
    },
    {
      "start": 457.84,
      "duration": 2.0,
      "text": "get the value from those uh from the",
      "timestamp": "07:37"
    },
    {
      "start": 459.84,
      "duration": 2.079,
      "text": "tools that they have as opposed to just",
      "timestamp": "07:39"
    },
    {
      "start": 461.919,
      "duration": 1.28,
      "text": "you know having to deal with the",
      "timestamp": "07:41"
    },
    {
      "start": 463.199,
      "duration": 2.481,
      "text": "coordination of it all",
      "timestamp": "07:43"
    },
    {
      "start": 465.68,
      "duration": 1.919,
      "text": "And we break the product down into three",
      "timestamp": "07:45"
    },
    {
      "start": 467.599,
      "duration": 1.761,
      "text": "simple buckets",
      "timestamp": "07:47"
    },
    {
      "start": 469.36,
      "duration": 2.0,
      "text": "We have our inference and finetuning",
      "timestamp": "07:49"
    },
    {
      "start": 471.36,
      "duration": 2.0,
      "text": "tool where you can run open source",
      "timestamp": "07:51"
    },
    {
      "start": 473.36,
      "duration": 1.279,
      "text": "models There's a whole host of open",
      "timestamp": "07:53"
    },
    {
      "start": 474.639,
      "duration": 2.721,
      "text": "source models um at the touch of a few",
      "timestamp": "07:54"
    },
    {
      "start": 477.36,
      "duration": 2.239,
      "text": "buttons Uh you can actually uh use our",
      "timestamp": "07:57"
    },
    {
      "start": 479.599,
      "duration": 1.841,
      "text": "retrieval engine or our rag product to",
      "timestamp": "07:59"
    },
    {
      "start": 481.44,
      "duration": 1.84,
      "text": "be able to ground those agents within",
      "timestamp": "08:01"
    },
    {
      "start": 483.28,
      "duration": 2.24,
      "text": "your own data and your own context And",
      "timestamp": "08:03"
    },
    {
      "start": 485.52,
      "duration": 1.76,
      "text": "like I say that's all unified through",
      "timestamp": "08:05"
    },
    {
      "start": 487.28,
      "duration": 2.96,
      "text": "one single deployment API",
      "timestamp": "08:07"
    },
    {
      "start": 490.24,
      "duration": 1.679,
      "text": "We then have our agent and model",
      "timestamp": "08:10"
    },
    {
      "start": 491.919,
      "duration": 1.921,
      "text": "marketplace where you can leverage a",
      "timestamp": "08:11"
    },
    {
      "start": 493.84,
      "duration": 1.52,
      "text": "whole host of custom agents that have",
      "timestamp": "08:13"
    },
    {
      "start": 495.36,
      "duration": 1.76,
      "text": "been built to leverage as part of your",
      "timestamp": "08:15"
    },
    {
      "start": 497.12,
      "duration": 2.0,
      "text": "products or service with no real",
      "timestamp": "08:17"
    },
    {
      "start": 499.12,
      "duration": 2.32,
      "text": "configure no real setup costs and it's",
      "timestamp": "08:19"
    },
    {
      "start": 501.44,
      "duration": 2.4,
      "text": "fully configurable to your needs",
      "timestamp": "08:21"
    },
    {
      "start": 503.84,
      "duration": 1.44,
      "text": "And then lastly we have this is a",
      "timestamp": "08:23"
    },
    {
      "start": 505.28,
      "duration": 1.599,
      "text": "product coming soon very very soon We",
      "timestamp": "08:25"
    },
    {
      "start": 506.879,
      "duration": 1.841,
      "text": "have our agent builder product It's a",
      "timestamp": "08:26"
    },
    {
      "start": 508.72,
      "duration": 2.0,
      "text": "basically a gentic workflow product",
      "timestamp": "08:28"
    },
    {
      "start": 510.72,
      "duration": 1.759,
      "text": "where you can build your own custom",
      "timestamp": "08:30"
    },
    {
      "start": 512.479,
      "duration": 2.641,
      "text": "agents with custom logic custom tools so",
      "timestamp": "08:32"
    },
    {
      "start": 515.12,
      "duration": 1.76,
      "text": "that you can adapt the models to be able",
      "timestamp": "08:35"
    },
    {
      "start": 516.88,
      "duration": 2.48,
      "text": "to meet the needs for you uh in a more",
      "timestamp": "08:36"
    },
    {
      "start": 519.36,
      "duration": 2.799,
      "text": "cost-effective and simple way And like I",
      "timestamp": "08:39"
    },
    {
      "start": 522.159,
      "duration": 2.081,
      "text": "say this entire infrastructure runs on",
      "timestamp": "08:42"
    },
    {
      "start": 524.24,
      "duration": 2.4,
      "text": "the IO cloud platform which means we're",
      "timestamp": "08:44"
    },
    {
      "start": 526.64,
      "duration": 2.4,
      "text": "able to offer these services with um",
      "timestamp": "08:46"
    },
    {
      "start": 529.04,
      "duration": 2.56,
      "text": "incredibly low latency and incredibly",
      "timestamp": "08:49"
    },
    {
      "start": 531.6,
      "duration": 2.32,
      "text": "low cost So removing the in for",
      "timestamp": "08:51"
    },
    {
      "start": 533.92,
      "duration": 4.16,
      "text": "complexity and also saving you money",
      "timestamp": "08:53"
    },
    {
      "start": 538.08,
      "duration": 2.4,
      "text": "And then I touched on the open and",
      "timestamp": "08:58"
    },
    {
      "start": 540.48,
      "duration": 3.359,
      "text": "closed uh LLM problem at the start and",
      "timestamp": "09:00"
    },
    {
      "start": 543.839,
      "duration": 2.481,
      "text": "actually IO intelligence really is a",
      "timestamp": "09:03"
    },
    {
      "start": 546.32,
      "duration": 1.92,
      "text": "product that tries to overcome that",
      "timestamp": "09:06"
    },
    {
      "start": 548.24,
      "duration": 2.159,
      "text": "problem and make open-source models more",
      "timestamp": "09:08"
    },
    {
      "start": 550.399,
      "duration": 2.081,
      "text": "accessible and easier to integrate for",
      "timestamp": "09:10"
    },
    {
      "start": 552.48,
      "duration": 2.56,
      "text": "people So you're no longer stuck by some",
      "timestamp": "09:12"
    },
    {
      "start": 555.04,
      "duration": 1.2,
      "text": "of the problems that you have with",
      "timestamp": "09:15"
    },
    {
      "start": 556.24,
      "duration": 2.24,
      "text": "closed source LLMs and you can move into",
      "timestamp": "09:16"
    },
    {
      "start": 558.48,
      "duration": 2.32,
      "text": "one unified product where you can",
      "timestamp": "09:18"
    },
    {
      "start": 560.8,
      "duration": 1.44,
      "text": "actually be more visible in how these",
      "timestamp": "09:20"
    },
    {
      "start": 562.24,
      "duration": 2.0,
      "text": "models are making decisions You can be",
      "timestamp": "09:22"
    },
    {
      "start": 564.24,
      "duration": 2.08,
      "text": "more uh open in how the innovation is",
      "timestamp": "09:24"
    },
    {
      "start": 566.32,
      "duration": 2.32,
      "text": "being made and uh be easier to be able",
      "timestamp": "09:26"
    },
    {
      "start": 568.64,
      "duration": 1.92,
      "text": "to flip between different models to get",
      "timestamp": "09:28"
    },
    {
      "start": 570.56,
      "duration": 1.2,
      "text": "different types of output based on",
      "timestamp": "09:30"
    },
    {
      "start": 571.76,
      "duration": 1.6,
      "text": "whatever whatever it is you're trying to",
      "timestamp": "09:31"
    },
    {
      "start": 573.36,
      "duration": 3.0,
      "text": "achieve",
      "timestamp": "09:33"
    },
    {
      "start": 576.64,
      "duration": 1.92,
      "text": "And then just a few things by the",
      "timestamp": "09:36"
    },
    {
      "start": 578.56,
      "duration": 1.76,
      "text": "numbers on IO We've been live almost a",
      "timestamp": "09:38"
    },
    {
      "start": 580.32,
      "duration": 2.959,
      "text": "year now and on our IO cloud platform um",
      "timestamp": "09:40"
    },
    {
      "start": 583.279,
      "duration": 2.401,
      "text": "we've had almost well over $14 million",
      "timestamp": "09:43"
    },
    {
      "start": 585.68,
      "duration": 1.68,
      "text": "worth of daily network earnings put",
      "timestamp": "09:45"
    },
    {
      "start": 587.36,
      "duration": 2.08,
      "text": "through the uh put through the network",
      "timestamp": "09:47"
    },
    {
      "start": 589.44,
      "duration": 1.2,
      "text": "and like I say because we're a",
      "timestamp": "09:49"
    },
    {
      "start": 590.64,
      "duration": 3.04,
      "text": "decentralized uh network of GPUs You can",
      "timestamp": "09:50"
    },
    {
      "start": 593.68,
      "duration": 2.4,
      "text": "actually uh you can actually see how",
      "timestamp": "09:53"
    },
    {
      "start": 596.08,
      "duration": 1.759,
      "text": "much revenue is coming into the into the",
      "timestamp": "09:56"
    },
    {
      "start": 597.839,
      "duration": 1.601,
      "text": "network and how much suppliers are",
      "timestamp": "09:57"
    },
    {
      "start": 599.44,
      "duration": 2.079,
      "text": "actually using It's all open and",
      "timestamp": "09:59"
    },
    {
      "start": 601.519,
      "duration": 2.32,
      "text": "verifiable and so that nobody can hide",
      "timestamp": "10:01"
    },
    {
      "start": 603.839,
      "duration": 1.841,
      "text": "behind uh you know numbers or figures",
      "timestamp": "10:03"
    },
    {
      "start": 605.68,
      "duration": 2.159,
      "text": "where we put all of ours out in the open",
      "timestamp": "10:05"
    },
    {
      "start": 607.839,
      "duration": 1.601,
      "text": "And it's the same with our supply We",
      "timestamp": "10:07"
    },
    {
      "start": 609.44,
      "duration": 2.399,
      "text": "have over 10,000 active GPUs on our",
      "timestamp": "10:09"
    },
    {
      "start": 611.839,
      "duration": 2.401,
      "text": "network uh today",
      "timestamp": "10:11"
    },
    {
      "start": 614.24,
      "duration": 2.48,
      "text": "And in terms of our IO intelligence uh",
      "timestamp": "10:14"
    },
    {
      "start": 616.72,
      "duration": 2.48,
      "text": "product we have over 1 billion tokens",
      "timestamp": "10:16"
    },
    {
      "start": 619.2,
      "duration": 2.56,
      "text": "being used per week on our open source",
      "timestamp": "10:19"
    },
    {
      "start": 621.76,
      "duration": 3.28,
      "text": "models Um and so that's real usage um",
      "timestamp": "10:21"
    },
    {
      "start": 625.04,
      "duration": 1.919,
      "text": "that people are sort of gaining value",
      "timestamp": "10:25"
    },
    {
      "start": 626.959,
      "duration": 2.081,
      "text": "from the our products and service and",
      "timestamp": "10:26"
    },
    {
      "start": 629.04,
      "duration": 1.6,
      "text": "along with over 5,000 different",
      "timestamp": "10:29"
    },
    {
      "start": 630.64,
      "duration": 2.48,
      "text": "enterprises startups and developers",
      "timestamp": "10:30"
    },
    {
      "start": 633.12,
      "duration": 2.959,
      "text": "leveraging that product as well",
      "timestamp": "10:33"
    },
    {
      "start": 636.079,
      "duration": 2.161,
      "text": "It's not a prototype it's real",
      "timestamp": "10:36"
    },
    {
      "start": 638.24,
      "duration": 2.24,
      "text": "technology in action And here's just a",
      "timestamp": "10:38"
    },
    {
      "start": 640.48,
      "duration": 1.2,
      "text": "list of some of the partners that we",
      "timestamp": "10:40"
    },
    {
      "start": 641.68,
      "duration": 3.52,
      "text": "work with who leverage uh the IO uh.NET",
      "timestamp": "10:41"
    },
    {
      "start": 645.2,
      "duration": 3.199,
      "text": "full stack",
      "timestamp": "10:45"
    },
    {
      "start": 648.399,
      "duration": 2.401,
      "text": "And so what's coming next So like the",
      "timestamp": "10:48"
    },
    {
      "start": 650.8,
      "duration": 1.039,
      "text": "customers that we have who are",
      "timestamp": "10:50"
    },
    {
      "start": 651.839,
      "duration": 1.361,
      "text": "constantly innovating using our",
      "timestamp": "10:51"
    },
    {
      "start": 653.2,
      "duration": 2.319,
      "text": "technology we're always innovating too",
      "timestamp": "10:53"
    },
    {
      "start": 655.519,
      "duration": 1.601,
      "text": "Um and we have an exciting road map",
      "timestamp": "10:55"
    },
    {
      "start": 657.12,
      "duration": 2.32,
      "text": "ahead including training as a service",
      "timestamp": "10:57"
    },
    {
      "start": 659.44,
      "duration": 1.44,
      "text": "obviously the agent builder that I've",
      "timestamp": "10:59"
    },
    {
      "start": 660.88,
      "duration": 2.8,
      "text": "talked about uh multimodal AI support",
      "timestamp": "11:00"
    },
    {
      "start": 663.68,
      "duration": 1.92,
      "text": "and then um orchestration with manage",
      "timestamp": "11:03"
    },
    {
      "start": 665.6,
      "duration": 4.76,
      "text": "VMs coming uh early next year",
      "timestamp": "11:05"
    },
    {
      "start": 670.64,
      "duration": 2.4,
      "text": "And so I mentioned I wouldn't just talk",
      "timestamp": "11:10"
    },
    {
      "start": 673.04,
      "duration": 1.359,
      "text": "about the tech and the product but I'd",
      "timestamp": "11:13"
    },
    {
      "start": 674.399,
      "duration": 1.761,
      "text": "also talk about philosophy This is our",
      "timestamp": "11:14"
    },
    {
      "start": 676.16,
      "duration": 2.88,
      "text": "vision statement that we um we live and",
      "timestamp": "11:16"
    },
    {
      "start": 679.04,
      "duration": 1.6,
      "text": "breathe by at IO.NET net when we're",
      "timestamp": "11:19"
    },
    {
      "start": 680.64,
      "duration": 1.84,
      "text": "building our products and that is that",
      "timestamp": "11:20"
    },
    {
      "start": 682.48,
      "duration": 2.32,
      "text": "we want to make it easy for anyone",
      "timestamp": "11:22"
    },
    {
      "start": 684.8,
      "duration": 2.96,
      "text": "anywhere to access the tools to develop",
      "timestamp": "11:24"
    },
    {
      "start": 687.76,
      "duration": 3.84,
      "text": "AI through reliable affordable and",
      "timestamp": "11:27"
    },
    {
      "start": 691.6,
      "duration": 2.56,
      "text": "performant platforms and we always keep",
      "timestamp": "11:31"
    },
    {
      "start": 694.16,
      "duration": 4.16,
      "text": "that to heart as we as we uh as we build",
      "timestamp": "11:34"
    },
    {
      "start": 698.32,
      "duration": 2.4,
      "text": "So I encourage you guys to try us out",
      "timestamp": "11:38"
    },
    {
      "start": 700.72,
      "duration": 2.239,
      "text": "Don't just take my word for it Um you",
      "timestamp": "11:40"
    },
    {
      "start": 702.959,
      "duration": 2.241,
      "text": "know experience it for yourself So you",
      "timestamp": "11:42"
    },
    {
      "start": 705.2,
      "duration": 2.079,
      "text": "can do that by going to io.net net and",
      "timestamp": "11:45"
    },
    {
      "start": 707.279,
      "duration": 1.761,
      "text": "just like I say spinning up a cluster",
      "timestamp": "11:47"
    },
    {
      "start": 709.04,
      "duration": 2.0,
      "text": "within 2 minutes um or deploying an",
      "timestamp": "11:49"
    },
    {
      "start": 711.04,
      "duration": 1.52,
      "text": "agent on your product through our single",
      "timestamp": "11:51"
    },
    {
      "start": 712.56,
      "duration": 2.8,
      "text": "deployment API using IO intelligence",
      "timestamp": "11:52"
    },
    {
      "start": 715.36,
      "duration": 2.0,
      "text": "We've just launched applications for our",
      "timestamp": "11:55"
    },
    {
      "start": 717.36,
      "duration": 2.24,
      "text": "hackathon our inaugural hackathon today",
      "timestamp": "11:57"
    },
    {
      "start": 719.6,
      "duration": 1.359,
      "text": "uh which you can find out more details",
      "timestamp": "11:59"
    },
    {
      "start": 720.959,
      "duration": 2.0,
      "text": "by scanning the QR code or going to",
      "timestamp": "12:00"
    },
    {
      "start": 722.959,
      "duration": 2.641,
      "text": "io.net And lastly we've just launched a",
      "timestamp": "12:02"
    },
    {
      "start": 725.6,
      "duration": 2.4,
      "text": "grants program for anybody who's just",
      "timestamp": "12:05"
    },
    {
      "start": 728.0,
      "duration": 2.0,
      "text": "starting out in the industry and wants",
      "timestamp": "12:08"
    },
    {
      "start": 730.0,
      "duration": 2.72,
      "text": "to benefit from um free startup uh",
      "timestamp": "12:10"
    },
    {
      "start": 732.72,
      "duration": 1.44,
      "text": "compute credits to be able to power",
      "timestamp": "12:12"
    },
    {
      "start": 734.16,
      "duration": 2.08,
      "text": "their organization Thank you very much",
      "timestamp": "12:14"
    },
    {
      "start": 736.24,
      "duration": 4.039,
      "text": "and uh thank you Cheers",
      "timestamp": "12:16"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.346946",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}