{
  "video_id": "mNnUhzEpkUI",
  "video_title": "Govind Wathan - Genesis Startup Competition Pitch - Literal Labs - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=mNnUhzEpkUI",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T14:24:53+00:00",
  "duration_seconds": null,
  "view_count": 11,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nGenesis Startup Competition Pitch - Literal Labs\n\nSpeaker:\nGovind Wathan, Business Advisor @ Literal Labs\n\nStage: WEKA Stage\n#superai #literallabs #startups #competition #ai\n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 206,
    "aggregated_text": "most of us here have the sense that AI has the potential to change the world for the better and to push all of humanity forward to realize this potential AI needs to be able to run at the embedded edge wherever it is needed and it needs to be able to do this independently and to meet society's needs AI needs to be explainable it needs to be fast and it needs to be economically and energy efficient good afternoon i'm Goind Wadin and we are Literal Labs and we are on a mission to create a new AI architecture that is fast ultra low power and explainable for the edge implementing AI at the edge can bring significant value to a multitude of applications be it medical or industrial mckenzie puts this at upwards of 500 billion US dollars of additional annual economic value today AI for the edge is typically solved using neural networks but this does not solve all issues our customers are telling us that the high amounts of investment that is needed in order to implement AI at the edge is slowing down adoption and this has a lot to do with the compute complexity of neural networks the cost of new and expensive hardware the reliance on network availability and the need for AI experts to train and deploy model are some of the factors that is limiting the adoption of AI at the edge and we want to change that through years of collaborative research between the University of Newcastle in the UK and the Center of AI research in Norway a new technique to machine learning emerged one that uses a combination of data binarization propositional logic and settling machines the resulting logic- based AI algorithms drastically reduces the compute complexity and this leads to higher throughput ultra low power and more explainable AI for the edge and the best part of all these solutions can be retrofitted into existing silicon chips and over the last decade we have moved from promising theory to realizable potential and here are some of our recent results we first start with XG Boost xub Boost is a popular AI algorithm it uses decision trees in order to run regression as well as classification tasks it is a good choice for a number of applications such as fraud detection as well as risk assessment when we ran our models against Xuboost our models outperformed Xuboost by being 250 times faster at the same level of accuracy next up we have a fully connected auto encoder and this is a neural network it's good at feature extraction as well as anomaly detection and when we ran our models against the FC autoenccoder on the same hardware same conditions we were 54 times faster while being 52 times lower power now this changes the game and makes AI a lot more accessible for the embedded itch now those were mind-boggling numbers but where our models are becoming really impressive is when we look at realworld scenarios so if you look at the numbers here these are measured results of our logic based models when applied to a water flow prediction system for a water utility company in the UK notice the tiny memory footprint that our models occupy on device and our ultra low power means that we're able to run one prediction every 5.8 seconds for the next 10 years on a coin cell battery or one prediction every second for the next 10 years on a double A battery and so this is really making AI a lot more accessible for these highly constrained edge embedded devices now I've talked about the tech and you've seen some of our numbers let's dive into the product itself at the center of our product offering is a training tool the training tool takes in customer data sets through a customizable API and we then booleanize the data and iteratively analyze and refine the data and what we end up with is a number of models these are all logic based models and what the tool then does is it selects the best fit model based on the customer's criteria and this model is then deployed at the customer's preferred silicon and since our models are tiny they are able to run on existing silicon and also um they're able to um run on the CPU directly without the need of GPUs or accelerators our training tools are provided through a subscription model and we license out our models uh for um on a royalty fee we estimate our SAM to be at $18 billion and that is considering the embedded edge AI market looking at markets such as drones um and medical devices today we have got customer engagements with a water utility company in the UK a large automotive tier one company in Europe as well as a large food supply chain company and each of these are doing something different with the model and we're seeing very strong traction the team is comprised of highly experienced members at the helm is Noel Hurley the former GM and VP of the CPU group at ARM and we also have a highly experienced group of founders and advisers we recently closed our preede where we raised4.6 million pounds and this takes us through getting the product out into the market scaling the team and completing our product market fit um activities with that I thank you for your time and I welcome any questions hey let me start with the first one um very cool to see the inference speed really going up and doing it that at the edge my only question here is is it primarily you showed uh XG boost there does it primarily just work with this kind of decision tree so you're also looking at some of the other AI models that you could compress and do faster thank you so XG Boost the reason we chose XG Boost is because it's a highly optimized model and when you look at age AI um and solutions in age AI the primary AI models that are used today are neural networks and that's about roughly 80% of embedded age solutions decision trees are another and decision decision trees are smaller they're more efficient and that's what XG Boost is and that's the primary reason we chose it is because we wanted to go after something that is highly optimized fast efficient and has a good ability to handle large data sets and that's why we we we chose that but we also pit ourselves up against a neural network as shown um on the next graph okay so um thank you uh so I heard you mention a few customers that you already have um in the deck i noticed that you were also talking a lot about technical problems that you're solving but for a customer to pay you for this there's a business problem that you're solving so can you talk a little bit through what business problems that you're solving sure thing so when you look at the embedded edge um a lot of companies understand that they can gain a huge competitive advantage if they were to adopt AI the problem that they face is the high cap acts it's a lot to do with capital investment and less to do with operating costs if you think about capital investment by the year 2030 it is estimated that we will spend 6.7 trillion US on new hardware and new infrastructure just to enable AI to work at the edge and this is the problem that we're solving by enabling customers to utilize our models that are fast efficient and uh explainable in existing hardware without the need of mainline power and without the need for additional network infrastructure",
    "text_length": 7177,
    "word_count": 1319
  },
  "segments": [
    {
      "start": 7.759,
      "duration": 3.201,
      "text": "most of us here have the sense that AI",
      "timestamp": "00:07"
    },
    {
      "start": 10.96,
      "duration": 2.32,
      "text": "has the potential to change the world",
      "timestamp": "00:10"
    },
    {
      "start": 13.28,
      "duration": 3.12,
      "text": "for the better and to push all of",
      "timestamp": "00:13"
    },
    {
      "start": 16.4,
      "duration": 2.799,
      "text": "humanity forward",
      "timestamp": "00:16"
    },
    {
      "start": 19.199,
      "duration": 3.84,
      "text": "to realize this potential AI needs to be",
      "timestamp": "00:19"
    },
    {
      "start": 23.039,
      "duration": 3.281,
      "text": "able to run at the embedded edge",
      "timestamp": "00:23"
    },
    {
      "start": 26.32,
      "duration": 2.32,
      "text": "wherever it is needed and it needs to be",
      "timestamp": "00:26"
    },
    {
      "start": 28.64,
      "duration": 3.2,
      "text": "able to do this independently",
      "timestamp": "00:28"
    },
    {
      "start": 31.84,
      "duration": 3.36,
      "text": "and to meet society's needs AI needs to",
      "timestamp": "00:31"
    },
    {
      "start": 35.2,
      "duration": 3.84,
      "text": "be explainable it needs to be fast and",
      "timestamp": "00:35"
    },
    {
      "start": 39.04,
      "duration": 2.72,
      "text": "it needs to be economically and energy",
      "timestamp": "00:39"
    },
    {
      "start": 41.76,
      "duration": 1.84,
      "text": "efficient",
      "timestamp": "00:41"
    },
    {
      "start": 43.6,
      "duration": 3.119,
      "text": "good afternoon i'm Goind Wadin and we",
      "timestamp": "00:43"
    },
    {
      "start": 46.719,
      "duration": 3.52,
      "text": "are Literal Labs and we are on a mission",
      "timestamp": "00:46"
    },
    {
      "start": 50.239,
      "duration": 3.84,
      "text": "to create a new AI architecture that is",
      "timestamp": "00:50"
    },
    {
      "start": 54.079,
      "duration": 4.081,
      "text": "fast ultra low power and explainable for",
      "timestamp": "00:54"
    },
    {
      "start": 58.16,
      "duration": 3.16,
      "text": "the edge",
      "timestamp": "00:58"
    },
    {
      "start": 61.92,
      "duration": 3.68,
      "text": "implementing AI at the edge can bring",
      "timestamp": "01:01"
    },
    {
      "start": 65.6,
      "duration": 2.32,
      "text": "significant value to a multitude of",
      "timestamp": "01:05"
    },
    {
      "start": 67.92,
      "duration": 5.44,
      "text": "applications be it medical or industrial",
      "timestamp": "01:07"
    },
    {
      "start": 73.36,
      "duration": 3.36,
      "text": "mckenzie puts this at upwards of 500",
      "timestamp": "01:13"
    },
    {
      "start": 76.72,
      "duration": 3.2,
      "text": "billion US dollars of additional annual",
      "timestamp": "01:16"
    },
    {
      "start": 79.92,
      "duration": 2.559,
      "text": "economic value",
      "timestamp": "01:19"
    },
    {
      "start": 82.479,
      "duration": 2.881,
      "text": "today AI for the edge is typically",
      "timestamp": "01:22"
    },
    {
      "start": 85.36,
      "duration": 3.36,
      "text": "solved using neural networks but this",
      "timestamp": "01:25"
    },
    {
      "start": 88.72,
      "duration": 3.359,
      "text": "does not solve all issues",
      "timestamp": "01:28"
    },
    {
      "start": 92.079,
      "duration": 2.961,
      "text": "our customers are telling us that the",
      "timestamp": "01:32"
    },
    {
      "start": 95.04,
      "duration": 2.32,
      "text": "high amounts of investment that is",
      "timestamp": "01:35"
    },
    {
      "start": 97.36,
      "duration": 2.32,
      "text": "needed in order to implement AI at the",
      "timestamp": "01:37"
    },
    {
      "start": 99.68,
      "duration": 3.6,
      "text": "edge is slowing down adoption",
      "timestamp": "01:39"
    },
    {
      "start": 103.28,
      "duration": 2.479,
      "text": "and this has a lot to do with the",
      "timestamp": "01:43"
    },
    {
      "start": 105.759,
      "duration": 3.921,
      "text": "compute complexity of neural networks",
      "timestamp": "01:45"
    },
    {
      "start": 109.68,
      "duration": 4.88,
      "text": "the cost of new and expensive hardware",
      "timestamp": "01:49"
    },
    {
      "start": 114.56,
      "duration": 3.919,
      "text": "the reliance on network availability",
      "timestamp": "01:54"
    },
    {
      "start": 118.479,
      "duration": 3.041,
      "text": "and the need for AI experts to train and",
      "timestamp": "01:58"
    },
    {
      "start": 121.52,
      "duration": 2.879,
      "text": "deploy model are some of the factors",
      "timestamp": "02:01"
    },
    {
      "start": 124.399,
      "duration": 2.881,
      "text": "that is limiting the adoption of AI at",
      "timestamp": "02:04"
    },
    {
      "start": 127.28,
      "duration": 4.319,
      "text": "the edge and we want to change that",
      "timestamp": "02:07"
    },
    {
      "start": 131.599,
      "duration": 3.041,
      "text": "through years of collaborative research",
      "timestamp": "02:11"
    },
    {
      "start": 134.64,
      "duration": 2.319,
      "text": "between the University of Newcastle in",
      "timestamp": "02:14"
    },
    {
      "start": 136.959,
      "duration": 3.36,
      "text": "the UK and the Center of AI research in",
      "timestamp": "02:16"
    },
    {
      "start": 140.319,
      "duration": 3.201,
      "text": "Norway a new technique to machine",
      "timestamp": "02:20"
    },
    {
      "start": 143.52,
      "duration": 2.24,
      "text": "learning emerged",
      "timestamp": "02:23"
    },
    {
      "start": 145.76,
      "duration": 3.119,
      "text": "one that uses a combination of data",
      "timestamp": "02:25"
    },
    {
      "start": 148.879,
      "duration": 1.681,
      "text": "binarization",
      "timestamp": "02:28"
    },
    {
      "start": 150.56,
      "duration": 2.24,
      "text": "propositional logic and settling",
      "timestamp": "02:30"
    },
    {
      "start": 152.8,
      "duration": 1.76,
      "text": "machines",
      "timestamp": "02:32"
    },
    {
      "start": 154.56,
      "duration": 3.36,
      "text": "the resulting logic- based AI algorithms",
      "timestamp": "02:34"
    },
    {
      "start": 157.92,
      "duration": 2.48,
      "text": "drastically reduces the compute",
      "timestamp": "02:37"
    },
    {
      "start": 160.4,
      "duration": 1.6,
      "text": "complexity",
      "timestamp": "02:40"
    },
    {
      "start": 162.0,
      "duration": 3.519,
      "text": "and this leads to higher throughput",
      "timestamp": "02:42"
    },
    {
      "start": 165.519,
      "duration": 3.681,
      "text": "ultra low power and more explainable AI",
      "timestamp": "02:45"
    },
    {
      "start": 169.2,
      "duration": 3.119,
      "text": "for the edge and the best part of all",
      "timestamp": "02:49"
    },
    {
      "start": 172.319,
      "duration": 2.64,
      "text": "these solutions can be retrofitted into",
      "timestamp": "02:52"
    },
    {
      "start": 174.959,
      "duration": 2.881,
      "text": "existing silicon chips",
      "timestamp": "02:54"
    },
    {
      "start": 177.84,
      "duration": 1.759,
      "text": "and over the last decade we have moved",
      "timestamp": "02:57"
    },
    {
      "start": 179.599,
      "duration": 3.28,
      "text": "from promising theory to realizable",
      "timestamp": "02:59"
    },
    {
      "start": 182.879,
      "duration": 2.321,
      "text": "potential and here are some of our",
      "timestamp": "03:02"
    },
    {
      "start": 185.2,
      "duration": 2.16,
      "text": "recent results",
      "timestamp": "03:05"
    },
    {
      "start": 187.36,
      "duration": 3.04,
      "text": "we first start with XG Boost xub Boost",
      "timestamp": "03:07"
    },
    {
      "start": 190.4,
      "duration": 3.839,
      "text": "is a popular AI algorithm it uses",
      "timestamp": "03:10"
    },
    {
      "start": 194.239,
      "duration": 2.64,
      "text": "decision trees in order to run",
      "timestamp": "03:14"
    },
    {
      "start": 196.879,
      "duration": 2.161,
      "text": "regression as well as classification",
      "timestamp": "03:16"
    },
    {
      "start": 199.04,
      "duration": 1.76,
      "text": "tasks",
      "timestamp": "03:19"
    },
    {
      "start": 200.8,
      "duration": 1.68,
      "text": "it is a good choice for a number of",
      "timestamp": "03:20"
    },
    {
      "start": 202.48,
      "duration": 2.8,
      "text": "applications such as fraud detection as",
      "timestamp": "03:22"
    },
    {
      "start": 205.28,
      "duration": 2.319,
      "text": "well as risk assessment",
      "timestamp": "03:25"
    },
    {
      "start": 207.599,
      "duration": 3.92,
      "text": "when we ran our models against Xuboost",
      "timestamp": "03:27"
    },
    {
      "start": 211.519,
      "duration": 3.201,
      "text": "our models outperformed Xuboost by being",
      "timestamp": "03:31"
    },
    {
      "start": 214.72,
      "duration": 4.4,
      "text": "250 times faster at the same level of",
      "timestamp": "03:34"
    },
    {
      "start": 219.12,
      "duration": 1.759,
      "text": "accuracy",
      "timestamp": "03:39"
    },
    {
      "start": 220.879,
      "duration": 3.841,
      "text": "next up we have a fully connected auto",
      "timestamp": "03:40"
    },
    {
      "start": 224.72,
      "duration": 3.36,
      "text": "encoder and this is a neural network",
      "timestamp": "03:44"
    },
    {
      "start": 228.08,
      "duration": 2.4,
      "text": "it's good at feature extraction as well",
      "timestamp": "03:48"
    },
    {
      "start": 230.48,
      "duration": 2.479,
      "text": "as anomaly detection",
      "timestamp": "03:50"
    },
    {
      "start": 232.959,
      "duration": 3.441,
      "text": "and when we ran our models against the",
      "timestamp": "03:52"
    },
    {
      "start": 236.4,
      "duration": 3.039,
      "text": "FC autoenccoder on the same hardware",
      "timestamp": "03:56"
    },
    {
      "start": 239.439,
      "duration": 4.241,
      "text": "same conditions we were 54 times faster",
      "timestamp": "03:59"
    },
    {
      "start": 243.68,
      "duration": 5.04,
      "text": "while being 52 times lower power",
      "timestamp": "04:03"
    },
    {
      "start": 248.72,
      "duration": 3.12,
      "text": "now this changes the game and makes AI a",
      "timestamp": "04:08"
    },
    {
      "start": 251.84,
      "duration": 3.039,
      "text": "lot more accessible for the embedded",
      "timestamp": "04:11"
    },
    {
      "start": 254.879,
      "duration": 3.0,
      "text": "itch",
      "timestamp": "04:14"
    },
    {
      "start": 258.16,
      "duration": 3.28,
      "text": "now those were mind-boggling numbers but",
      "timestamp": "04:18"
    },
    {
      "start": 261.44,
      "duration": 2.24,
      "text": "where our models are becoming really",
      "timestamp": "04:21"
    },
    {
      "start": 263.68,
      "duration": 2.56,
      "text": "impressive is when we look at realworld",
      "timestamp": "04:23"
    },
    {
      "start": 266.24,
      "duration": 2.56,
      "text": "scenarios so if you look at the numbers",
      "timestamp": "04:26"
    },
    {
      "start": 268.8,
      "duration": 3.6,
      "text": "here these are measured results of our",
      "timestamp": "04:28"
    },
    {
      "start": 272.4,
      "duration": 3.44,
      "text": "logic based models when applied to a",
      "timestamp": "04:32"
    },
    {
      "start": 275.84,
      "duration": 3.04,
      "text": "water flow prediction system for a water",
      "timestamp": "04:35"
    },
    {
      "start": 278.88,
      "duration": 2.72,
      "text": "utility company in the UK",
      "timestamp": "04:38"
    },
    {
      "start": 281.6,
      "duration": 2.879,
      "text": "notice the tiny memory footprint that",
      "timestamp": "04:41"
    },
    {
      "start": 284.479,
      "duration": 3.601,
      "text": "our models occupy on device and our",
      "timestamp": "04:44"
    },
    {
      "start": 288.08,
      "duration": 2.559,
      "text": "ultra low power means that we're able to",
      "timestamp": "04:48"
    },
    {
      "start": 290.639,
      "duration": 4.481,
      "text": "run one prediction every 5.8 seconds for",
      "timestamp": "04:50"
    },
    {
      "start": 295.12,
      "duration": 4.56,
      "text": "the next 10 years on a coin cell battery",
      "timestamp": "04:55"
    },
    {
      "start": 299.68,
      "duration": 3.6,
      "text": "or one prediction every second for the",
      "timestamp": "04:59"
    },
    {
      "start": 303.28,
      "duration": 4.16,
      "text": "next 10 years on a double A battery and",
      "timestamp": "05:03"
    },
    {
      "start": 307.44,
      "duration": 2.64,
      "text": "so this is really making AI a lot more",
      "timestamp": "05:07"
    },
    {
      "start": 310.08,
      "duration": 3.44,
      "text": "accessible for these highly constrained",
      "timestamp": "05:10"
    },
    {
      "start": 313.52,
      "duration": 3.36,
      "text": "edge embedded devices",
      "timestamp": "05:13"
    },
    {
      "start": 316.88,
      "duration": 2.8,
      "text": "now I've talked about the tech and",
      "timestamp": "05:16"
    },
    {
      "start": 319.68,
      "duration": 2.64,
      "text": "you've seen some of our numbers let's",
      "timestamp": "05:19"
    },
    {
      "start": 322.32,
      "duration": 2.48,
      "text": "dive into the product itself at the",
      "timestamp": "05:22"
    },
    {
      "start": 324.8,
      "duration": 2.399,
      "text": "center of our product offering is a",
      "timestamp": "05:24"
    },
    {
      "start": 327.199,
      "duration": 3.121,
      "text": "training tool the training tool takes in",
      "timestamp": "05:27"
    },
    {
      "start": 330.32,
      "duration": 1.599,
      "text": "customer data sets through a",
      "timestamp": "05:30"
    },
    {
      "start": 331.919,
      "duration": 3.12,
      "text": "customizable API",
      "timestamp": "05:31"
    },
    {
      "start": 335.039,
      "duration": 2.88,
      "text": "and we then booleanize the data and",
      "timestamp": "05:35"
    },
    {
      "start": 337.919,
      "duration": 4.081,
      "text": "iteratively analyze and refine the data",
      "timestamp": "05:37"
    },
    {
      "start": 342.0,
      "duration": 3.28,
      "text": "and what we end up with is a number of",
      "timestamp": "05:42"
    },
    {
      "start": 345.28,
      "duration": 3.44,
      "text": "models these are all logic based models",
      "timestamp": "05:45"
    },
    {
      "start": 348.72,
      "duration": 1.759,
      "text": "and what the tool then does is it",
      "timestamp": "05:48"
    },
    {
      "start": 350.479,
      "duration": 2.801,
      "text": "selects the best fit model based on the",
      "timestamp": "05:50"
    },
    {
      "start": 353.28,
      "duration": 2.24,
      "text": "customer's criteria and this model is",
      "timestamp": "05:53"
    },
    {
      "start": 355.52,
      "duration": 2.239,
      "text": "then deployed at the customer's",
      "timestamp": "05:55"
    },
    {
      "start": 357.759,
      "duration": 2.801,
      "text": "preferred silicon and since our models",
      "timestamp": "05:57"
    },
    {
      "start": 360.56,
      "duration": 2.8,
      "text": "are tiny they are able to run on",
      "timestamp": "06:00"
    },
    {
      "start": 363.36,
      "duration": 2.32,
      "text": "existing silicon",
      "timestamp": "06:03"
    },
    {
      "start": 365.68,
      "duration": 2.0,
      "text": "and also",
      "timestamp": "06:05"
    },
    {
      "start": 367.68,
      "duration": 2.56,
      "text": "um they're able to",
      "timestamp": "06:07"
    },
    {
      "start": 370.24,
      "duration": 3.519,
      "text": "um run on the CPU directly without the",
      "timestamp": "06:10"
    },
    {
      "start": 373.759,
      "duration": 4.641,
      "text": "need of GPUs or accelerators",
      "timestamp": "06:13"
    },
    {
      "start": 378.4,
      "duration": 2.16,
      "text": "our training tools are provided through",
      "timestamp": "06:18"
    },
    {
      "start": 380.56,
      "duration": 3.04,
      "text": "a subscription model and we license out",
      "timestamp": "06:20"
    },
    {
      "start": 383.6,
      "duration": 4.64,
      "text": "our models uh for um on a royalty fee",
      "timestamp": "06:23"
    },
    {
      "start": 388.24,
      "duration": 4.32,
      "text": "we estimate our SAM to be at $18 billion",
      "timestamp": "06:28"
    },
    {
      "start": 392.56,
      "duration": 2.72,
      "text": "and that is considering the embedded",
      "timestamp": "06:32"
    },
    {
      "start": 395.28,
      "duration": 3.12,
      "text": "edge AI market looking at markets such",
      "timestamp": "06:35"
    },
    {
      "start": 398.4,
      "duration": 1.68,
      "text": "as drones",
      "timestamp": "06:38"
    },
    {
      "start": 400.08,
      "duration": 2.72,
      "text": "um and medical devices",
      "timestamp": "06:40"
    },
    {
      "start": 402.8,
      "duration": 1.839,
      "text": "today we have got customer engagements",
      "timestamp": "06:42"
    },
    {
      "start": 404.639,
      "duration": 3.441,
      "text": "with a water utility company in the UK a",
      "timestamp": "06:44"
    },
    {
      "start": 408.08,
      "duration": 2.399,
      "text": "large automotive tier one company in",
      "timestamp": "06:48"
    },
    {
      "start": 410.479,
      "duration": 2.801,
      "text": "Europe as well as a large food supply",
      "timestamp": "06:50"
    },
    {
      "start": 413.28,
      "duration": 1.759,
      "text": "chain company and each of these are",
      "timestamp": "06:53"
    },
    {
      "start": 415.039,
      "duration": 2.0,
      "text": "doing something different with the model",
      "timestamp": "06:55"
    },
    {
      "start": 417.039,
      "duration": 4.081,
      "text": "and we're seeing very strong traction",
      "timestamp": "06:57"
    },
    {
      "start": 421.12,
      "duration": 2.72,
      "text": "the team is comprised of highly",
      "timestamp": "07:01"
    },
    {
      "start": 423.84,
      "duration": 3.12,
      "text": "experienced members at the helm is Noel",
      "timestamp": "07:03"
    },
    {
      "start": 426.96,
      "duration": 4.079,
      "text": "Hurley the former GM and VP of the CPU",
      "timestamp": "07:06"
    },
    {
      "start": 431.039,
      "duration": 2.641,
      "text": "group at ARM and we also have a highly",
      "timestamp": "07:11"
    },
    {
      "start": 433.68,
      "duration": 2.4,
      "text": "experienced group of founders and",
      "timestamp": "07:13"
    },
    {
      "start": 436.08,
      "duration": 1.519,
      "text": "advisers",
      "timestamp": "07:16"
    },
    {
      "start": 437.599,
      "duration": 2.72,
      "text": "we recently closed our preede where we",
      "timestamp": "07:17"
    },
    {
      "start": 440.319,
      "duration": 3.28,
      "text": "raised4.6 million pounds and this takes",
      "timestamp": "07:20"
    },
    {
      "start": 443.599,
      "duration": 2.241,
      "text": "us through getting the product out into",
      "timestamp": "07:23"
    },
    {
      "start": 445.84,
      "duration": 2.24,
      "text": "the market scaling the team and",
      "timestamp": "07:25"
    },
    {
      "start": 448.08,
      "duration": 4.64,
      "text": "completing our product market fit um",
      "timestamp": "07:28"
    },
    {
      "start": 452.72,
      "duration": 1.68,
      "text": "activities",
      "timestamp": "07:32"
    },
    {
      "start": 454.4,
      "duration": 2.88,
      "text": "with that I thank you for your time and",
      "timestamp": "07:34"
    },
    {
      "start": 457.28,
      "duration": 2.88,
      "text": "I welcome any questions\n hey let me start",
      "timestamp": "07:37"
    },
    {
      "start": 460.16,
      "duration": 1.759,
      "text": "with the first one um very cool to see",
      "timestamp": "07:40"
    },
    {
      "start": 461.919,
      "duration": 1.921,
      "text": "the inference speed really going up and",
      "timestamp": "07:41"
    },
    {
      "start": 463.84,
      "duration": 1.759,
      "text": "doing it that at the edge my only",
      "timestamp": "07:43"
    },
    {
      "start": 465.599,
      "duration": 1.841,
      "text": "question here is is it primarily you",
      "timestamp": "07:45"
    },
    {
      "start": 467.44,
      "duration": 2.64,
      "text": "showed uh XG boost there does it",
      "timestamp": "07:47"
    },
    {
      "start": 470.08,
      "duration": 1.44,
      "text": "primarily just work with this kind of",
      "timestamp": "07:50"
    },
    {
      "start": 471.52,
      "duration": 1.76,
      "text": "decision tree so you're also looking at",
      "timestamp": "07:51"
    },
    {
      "start": 473.28,
      "duration": 1.68,
      "text": "some of the other AI models that you",
      "timestamp": "07:53"
    },
    {
      "start": 474.96,
      "duration": 3.679,
      "text": "could compress and do faster\n thank you",
      "timestamp": "07:54"
    },
    {
      "start": 478.639,
      "duration": 2.481,
      "text": "so XG Boost the reason we chose XG Boost",
      "timestamp": "07:58"
    },
    {
      "start": 481.12,
      "duration": 3.12,
      "text": "is because it's a highly optimized model",
      "timestamp": "08:01"
    },
    {
      "start": 484.24,
      "duration": 2.72,
      "text": "and when you look at age AI um and",
      "timestamp": "08:04"
    },
    {
      "start": 486.96,
      "duration": 3.76,
      "text": "solutions in age AI the primary AI",
      "timestamp": "08:06"
    },
    {
      "start": 490.72,
      "duration": 1.44,
      "text": "models that are used today are neural",
      "timestamp": "08:10"
    },
    {
      "start": 492.16,
      "duration": 3.36,
      "text": "networks and that's about roughly 80% of",
      "timestamp": "08:12"
    },
    {
      "start": 495.52,
      "duration": 2.88,
      "text": "embedded age solutions decision trees",
      "timestamp": "08:15"
    },
    {
      "start": 498.4,
      "duration": 2.56,
      "text": "are another and decision decision trees",
      "timestamp": "08:18"
    },
    {
      "start": 500.96,
      "duration": 1.84,
      "text": "are smaller they're more efficient and",
      "timestamp": "08:20"
    },
    {
      "start": 502.8,
      "duration": 2.32,
      "text": "that's what XG Boost is and that's the",
      "timestamp": "08:22"
    },
    {
      "start": 505.12,
      "duration": 1.84,
      "text": "primary reason we chose it is because we",
      "timestamp": "08:25"
    },
    {
      "start": 506.96,
      "duration": 1.76,
      "text": "wanted to go after something that is",
      "timestamp": "08:26"
    },
    {
      "start": 508.72,
      "duration": 3.12,
      "text": "highly optimized fast efficient and has",
      "timestamp": "08:28"
    },
    {
      "start": 511.84,
      "duration": 3.04,
      "text": "a good ability to handle large data sets",
      "timestamp": "08:31"
    },
    {
      "start": 514.88,
      "duration": 2.638,
      "text": "and that's why we we we chose that but",
      "timestamp": "08:34"
    },
    {
      "start": 517.519,
      "duration": 1.681,
      "text": "we also pit ourselves up against a",
      "timestamp": "08:37"
    },
    {
      "start": 519.2,
      "duration": 3.44,
      "text": "neural network as shown um on the next",
      "timestamp": "08:39"
    },
    {
      "start": 522.64,
      "duration": 5.04,
      "text": "graph\n okay so um thank you uh so I heard",
      "timestamp": "08:42"
    },
    {
      "start": 527.68,
      "duration": 1.76,
      "text": "you mention a few customers that you",
      "timestamp": "08:47"
    },
    {
      "start": 529.44,
      "duration": 2.56,
      "text": "already have um in the deck i noticed",
      "timestamp": "08:49"
    },
    {
      "start": 532.0,
      "duration": 1.279,
      "text": "that you were also talking a lot about",
      "timestamp": "08:52"
    },
    {
      "start": 533.279,
      "duration": 1.841,
      "text": "technical problems that you're solving",
      "timestamp": "08:53"
    },
    {
      "start": 535.12,
      "duration": 2.48,
      "text": "but for a customer to pay you for this",
      "timestamp": "08:55"
    },
    {
      "start": 537.6,
      "duration": 1.52,
      "text": "there's a business problem that you're",
      "timestamp": "08:57"
    },
    {
      "start": 539.12,
      "duration": 1.36,
      "text": "solving so can you talk a little bit",
      "timestamp": "08:59"
    },
    {
      "start": 540.48,
      "duration": 1.52,
      "text": "through what business problems that",
      "timestamp": "09:00"
    },
    {
      "start": 542.0,
      "duration": 2.8,
      "text": "you're solving\n sure thing so when you",
      "timestamp": "09:02"
    },
    {
      "start": 544.8,
      "duration": 3.44,
      "text": "look at the embedded edge um a lot of",
      "timestamp": "09:04"
    },
    {
      "start": 548.24,
      "duration": 3.12,
      "text": "companies understand that they can gain",
      "timestamp": "09:08"
    },
    {
      "start": 551.36,
      "duration": 2.56,
      "text": "a huge competitive advantage if they",
      "timestamp": "09:11"
    },
    {
      "start": 553.92,
      "duration": 2.72,
      "text": "were to adopt AI the problem that they",
      "timestamp": "09:13"
    },
    {
      "start": 556.64,
      "duration": 2.72,
      "text": "face is the high cap acts it's a lot to",
      "timestamp": "09:16"
    },
    {
      "start": 559.36,
      "duration": 1.919,
      "text": "do with capital investment and less to",
      "timestamp": "09:19"
    },
    {
      "start": 561.279,
      "duration": 2.481,
      "text": "do with operating costs if you think",
      "timestamp": "09:21"
    },
    {
      "start": 563.76,
      "duration": 2.72,
      "text": "about capital investment by the year",
      "timestamp": "09:23"
    },
    {
      "start": 566.48,
      "duration": 3.359,
      "text": "2030 it is estimated that we will spend",
      "timestamp": "09:26"
    },
    {
      "start": 569.839,
      "duration": 5.041,
      "text": "6.7 trillion US on new hardware and new",
      "timestamp": "09:29"
    },
    {
      "start": 574.88,
      "duration": 1.6,
      "text": "infrastructure",
      "timestamp": "09:34"
    },
    {
      "start": 576.48,
      "duration": 3.52,
      "text": "just to enable AI to work at the edge",
      "timestamp": "09:36"
    },
    {
      "start": 580.0,
      "duration": 1.44,
      "text": "and this is the problem that we're",
      "timestamp": "09:40"
    },
    {
      "start": 581.44,
      "duration": 3.519,
      "text": "solving by enabling customers to utilize",
      "timestamp": "09:41"
    },
    {
      "start": 584.959,
      "duration": 2.961,
      "text": "our models that are fast efficient and",
      "timestamp": "09:44"
    },
    {
      "start": 587.92,
      "duration": 2.96,
      "text": "uh explainable in existing hardware",
      "timestamp": "09:47"
    },
    {
      "start": 590.88,
      "duration": 3.28,
      "text": "without the need of mainline power and",
      "timestamp": "09:50"
    },
    {
      "start": 594.16,
      "duration": 1.6,
      "text": "without the need for additional network",
      "timestamp": "09:54"
    },
    {
      "start": 595.76,
      "duration": 3.0,
      "text": "infrastructure",
      "timestamp": "09:55"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.376479",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}