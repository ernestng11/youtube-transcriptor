{
  "video_id": "o56-WV8bXz8",
  "video_title": "Wendy Gonzalez - How Varied Perspectives Drive AI Excellence - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=o56-WV8bXz8",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T16:20:29+00:00",
  "duration_seconds": null,
  "view_count": 14,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: Diversity as Innovation: How Varied Perspectives Drive AI Excellence\n\nSpeaker:\nWendy Gonzalez, CEO @ Sama\n\nStage: WEKA Stage\n#superai #sama #genai #aimodels #aidevelopment \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 296,
    "aggregated_text": "all right so I'm here to talk a little bit about responsible AI and diversity as innovation so I think it comes as no surprise that AI is being globally adopted at an extremely high rate so none of this is uh is news to you all uh but so have AI incidents um as you can see to the right there's a hugely large spike and that's primarily from Gen AI and the adoption of Gen AI um this is actually data that's collected by the OECD and uh in this example this is an example of London's mayor um as as a deep fake um that caused quite a bit of uh challenge and concern but beyond that um inference challenges and bias are becoming a much more common situation and uh these are actually recent uh data points but nearly 70% of all data and actually uh 60% uh this just came out in 2025 of AI deployments still fail due to risk in building these systems misinformation is a large risk reputational risk which is incredibly costly bias and inaccuracies and last but not least security and data privacy what's also really interesting is that Gartner does a top 10 predictions every year and one of their predictions is that up to 10% of company AI spend in the course of the next decade is going to be spent on addressing misinformation that's actually a really staggering number so what's been happening governments standard bodies and corporations have come to develop responsible AI frameworks you may have seen some of them nist the National Institute for Standards and Technology uh is one the global OECD has created um their own and then there have also been um corporations that have developed their own uh responsible AI frameworks like Microsoft Google Meta and even the Partnership on AI which is a nonprofit that brings together experts as well as corporations to develop these standards uh but there isn't really a one-sizefits-all approach uh I think we can all agree that nobody wants to stifle innovation yet at the same time we recognize that as more a the more that AI gets adopted the more important it is to have some level of guard rails or frameworks in which developers can build so the EU um who uh they actually passed the uh EU artificial intelligence act u by show of hands is anybody familiar with that okay yeah exactly and that's really based off of uh four core pillars now this compliance actually passed unanimously like twothirds to one so the first is on data governance how do you know that the data sets that you're using are fit for the purpose of your application how do how do you do that how do you build that to the best uh extent possible that you've got the right data that is free of errors and that is complete the second component of this is human oversight right how do you ensure that higher risk systems whether it be financial services insurance uh things of that nature um have some level of human validation the third is model evaluation right how do you establish a quality measurement system so new models are being developed all the time right uh Llama 4.0 was dropped there's going to be a new version and another version we speak to clients often lots of different companies who know that their technology may change but that doesn't mean the purpose of your model changes so you need to have a common framework in which you're managing and measuring this and then last but not least is data privacy uh but compliance uh as I mentioned before has been identified as a way to uh you know to stifle innovation so there have been plenty of bills that are overreaching that are extremely challenging to ensure that you get to these frameworks so one that's been very interesting um that just came about is California bill SB813 and this is actually a bill that is a public public private partnership where instead of trying to drive this through compliance it's actually about incentivizing developers to adopt responsible AI best practices so these developers who can get certified by third parties actually get indemnification from legal action if they have been certified in these practices so it's an interesting uh very interesting uh concept about how to bring together the best of both worlds because at the end of the day engineers who are building these models right and creating an incentivization program is really quite interesting all of these frameworks they really have these things in common one it's about fairness transparency ultimately accountability right so how do you how do you define those AI outcomes privacy how effective is this model is it robust and then ultimately human human oversight so why do this why does it matter right responsible AI is better performing AI at the end of the day right to do this uh we need to recognize that this technology is ubiquitous right I started this conversation by sharing that AI is a global phenomenon global adoption well so are products and you want your products to work across borders um and across geographies across people uh but at the end of the day it's all about whether these u these models understand both the cost of failure and whether they're adopted so one example I'd like to raise that has done an incredible job of This is is Whimo so they publish a report they publish a report that shows that um after 10 million rides and something like 50 million miles their cars beat human drivers if you will or have better uh safety records from an accident standpoint by literally 10 to one okay 10 to one these these uh these uh autos are driving more miles with less accidents than humans and this is the kind of trust that is really necessary for companies to adopt AI the standard for AI performance is a lot higher than human performance okay so why does this matter um let's see actually let me go back one is that uh at the end of the day many of these models and most models are now being developed off of foundation models and those foundation models are trained on the internet the internet isn't exactly completely representative of everybody and everything right so that's why you have to include specialized annotated data and at the end of the day that data needs to be mo uh needs to be validated right and I just like this picture because it's kind of funny but the at the end of the day right without the right data and model valuation um practices in place models can hallucinate right and this is a a good example of of hallucination so why why does human context matter okay why can't we just synthetically generate everything um well the challenge is is that models that are trained on models can carry forward their bias and at the end of the day you need to understand what you are training for so I know there's a lot on this slide but it's a pretty simple example if you're building a model right and here's this model with parents pushing strollers well if you synthetically generate this you may end up in situations where for example does this look right would you see a stroller without a baby in the middle of the street right would you see a a stroller without a baby and these are some of the challenges is that as humans we recognize that context so oftent times even if synthetic data is being used to train models there's actually a human validation component that's necessary to ensure that the models that the synthetic data that's being created actually has the appropriate context and um ultimately central to developing responsible AI beyond human insight is really data diversity so that data diversity typically goes around demographics environment language edge cases and modalities so in terms of uh uh different environments as an example if you're building a self-driving car you need to know if you're in Singapore if there you know drivers sit on the uh right side versus for example um you know in China on the left side so understanding that is really important because um you need to reduce you want to reduce the overfitting of applications and to improve generalization that works across multiple regions so one of the things that uh we've done as a company is we're actually building an uh an Africacentric data set and the reason we're doing this for research purposes is because it's a region that has been consistently under reppresented in the world so by building this additional data set we hope that people for example can train their models to recognize um and um ultimately build applications that are more representative of of where we live and where we live globally the last thing I want to note I know this is a little bit of a goofy slide is that diversity is important not only in terms of data diversity but it's also about who builds your AI one of the practices that we have as a company is we not only hire at least 50% uh women we actually carry that all the way through up to the top and the reason being is that different people and different ideas breed you know better and more rich results uh if anybody here is an engineer when did you ever get it right and all the requirements the first time probably never right requirements are iterative they continue to build and so having people with a diversity of thinking diversity of backgrounds is an incredible help because we are ultimately training AI that needs to learn from experience and that's going to change over time uh last but not least I just wanted to put a couple of points in here around responsible AI learnings there are many different checklists that are out there and I'd certainly be happy to share a bunch of different um frameworks and references for how to develop responsible AI but I think the point that I really wanted to share is ultimately this responsible AI is it's a development framework okay it's not about necessarily just ethics or morality it's really about how do you build a model that is performant to your application and that really starts with practices that you think about when you begin to build a model okay so if anybody has again built models built models or built software if you think about security as an afterthought it's really challenging right so the idea here is think with these parameters in mind first the first is what you know what uh what AI do you want to develop how do you measure the effectiveness of that AI do you actually have a way in which you can measure it because the world changes it changes constantly how do you know if your model is right if you change your model um you know the model that you're using how do you ensure that again the model the uh the application you're building is performant so when you build this and you come up with these frameworks and you educate your developers on this upfront the ability to build models that are more performant more accurate um is is much much higher but it's really difficult to do if you do this midway through your process so my recommendation is measure twice cut once have a plan the tech is going to change but the evaluation is not thank you everybody",
    "text_length": 10938,
    "word_count": 1981
  },
  "segments": [
    {
      "start": 7.6,
      "duration": 1.28,
      "text": "all right so I'm here to talk a little",
      "timestamp": "00:07"
    },
    {
      "start": 8.88,
      "duration": 1.759,
      "text": "bit about responsible AI and diversity",
      "timestamp": "00:08"
    },
    {
      "start": 10.639,
      "duration": 3.361,
      "text": "as innovation so I think it comes as no",
      "timestamp": "00:10"
    },
    {
      "start": 14.0,
      "duration": 2.0,
      "text": "surprise that AI is being globally",
      "timestamp": "00:14"
    },
    {
      "start": 16.0,
      "duration": 2.24,
      "text": "adopted at an extremely high rate so",
      "timestamp": "00:16"
    },
    {
      "start": 18.24,
      "duration": 3.199,
      "text": "none of this is uh is news to you all",
      "timestamp": "00:18"
    },
    {
      "start": 21.439,
      "duration": 2.961,
      "text": "uh but so have AI incidents um as you",
      "timestamp": "00:21"
    },
    {
      "start": 24.4,
      "duration": 2.0,
      "text": "can see to the right there's a hugely",
      "timestamp": "00:24"
    },
    {
      "start": 26.4,
      "duration": 2.4,
      "text": "large spike and that's primarily from",
      "timestamp": "00:26"
    },
    {
      "start": 28.8,
      "duration": 3.278,
      "text": "Gen AI and the adoption of Gen AI um",
      "timestamp": "00:28"
    },
    {
      "start": 32.079,
      "duration": 1.521,
      "text": "this is actually data that's collected",
      "timestamp": "00:32"
    },
    {
      "start": 33.6,
      "duration": 3.68,
      "text": "by the OECD and uh in this example this",
      "timestamp": "00:33"
    },
    {
      "start": 37.28,
      "duration": 3.279,
      "text": "is an example of London's mayor um as as",
      "timestamp": "00:37"
    },
    {
      "start": 40.559,
      "duration": 2.0,
      "text": "a deep fake um that caused quite a bit",
      "timestamp": "00:40"
    },
    {
      "start": 42.559,
      "duration": 2.241,
      "text": "of uh challenge and concern but beyond",
      "timestamp": "00:42"
    },
    {
      "start": 44.8,
      "duration": 2.239,
      "text": "that um inference challenges and bias",
      "timestamp": "00:44"
    },
    {
      "start": 47.039,
      "duration": 1.921,
      "text": "are becoming a much more common",
      "timestamp": "00:47"
    },
    {
      "start": 48.96,
      "duration": 2.8,
      "text": "situation and uh these are actually",
      "timestamp": "00:48"
    },
    {
      "start": 51.76,
      "duration": 3.439,
      "text": "recent uh data points but nearly 70% of",
      "timestamp": "00:51"
    },
    {
      "start": 55.199,
      "duration": 2.961,
      "text": "all data and actually uh 60% uh this",
      "timestamp": "00:55"
    },
    {
      "start": 58.16,
      "duration": 2.719,
      "text": "just came out in 2025 of AI deployments",
      "timestamp": "00:58"
    },
    {
      "start": 60.879,
      "duration": 2.881,
      "text": "still fail due to risk in building these",
      "timestamp": "01:00"
    },
    {
      "start": 63.76,
      "duration": 3.679,
      "text": "systems misinformation is a large risk",
      "timestamp": "01:03"
    },
    {
      "start": 67.439,
      "duration": 1.841,
      "text": "reputational risk which is incredibly",
      "timestamp": "01:07"
    },
    {
      "start": 69.28,
      "duration": 3.6,
      "text": "costly bias and inaccuracies and last",
      "timestamp": "01:09"
    },
    {
      "start": 72.88,
      "duration": 4.08,
      "text": "but not least security and data privacy",
      "timestamp": "01:12"
    },
    {
      "start": 76.96,
      "duration": 1.68,
      "text": "what's also really interesting is that",
      "timestamp": "01:16"
    },
    {
      "start": 78.64,
      "duration": 2.479,
      "text": "Gartner does a top 10 predictions every",
      "timestamp": "01:18"
    },
    {
      "start": 81.119,
      "duration": 1.841,
      "text": "year and one of their predictions is",
      "timestamp": "01:21"
    },
    {
      "start": 82.96,
      "duration": 3.199,
      "text": "that up to 10% of company AI spend in",
      "timestamp": "01:22"
    },
    {
      "start": 86.159,
      "duration": 1.681,
      "text": "the course of the next decade is going",
      "timestamp": "01:26"
    },
    {
      "start": 87.84,
      "duration": 3.04,
      "text": "to be spent on addressing misinformation",
      "timestamp": "01:27"
    },
    {
      "start": 90.88,
      "duration": 1.44,
      "text": "that's actually a really staggering",
      "timestamp": "01:30"
    },
    {
      "start": 92.32,
      "duration": 3.0,
      "text": "number",
      "timestamp": "01:32"
    },
    {
      "start": 95.52,
      "duration": 2.48,
      "text": "so what's been happening governments",
      "timestamp": "01:35"
    },
    {
      "start": 98.0,
      "duration": 2.4,
      "text": "standard bodies and corporations have",
      "timestamp": "01:38"
    },
    {
      "start": 100.4,
      "duration": 2.0,
      "text": "come to develop responsible AI",
      "timestamp": "01:40"
    },
    {
      "start": 102.4,
      "duration": 2.16,
      "text": "frameworks you may have seen some of",
      "timestamp": "01:42"
    },
    {
      "start": 104.56,
      "duration": 2.16,
      "text": "them nist the National Institute for",
      "timestamp": "01:44"
    },
    {
      "start": 106.72,
      "duration": 2.56,
      "text": "Standards and Technology uh is one the",
      "timestamp": "01:46"
    },
    {
      "start": 109.28,
      "duration": 3.28,
      "text": "global OECD has created um their own and",
      "timestamp": "01:49"
    },
    {
      "start": 112.56,
      "duration": 1.199,
      "text": "then there have also been um",
      "timestamp": "01:52"
    },
    {
      "start": 113.759,
      "duration": 1.441,
      "text": "corporations that have developed their",
      "timestamp": "01:53"
    },
    {
      "start": 115.2,
      "duration": 2.64,
      "text": "own uh responsible AI frameworks like",
      "timestamp": "01:55"
    },
    {
      "start": 117.84,
      "duration": 2.319,
      "text": "Microsoft Google Meta and even the",
      "timestamp": "01:57"
    },
    {
      "start": 120.159,
      "duration": 2.161,
      "text": "Partnership on AI which is a nonprofit",
      "timestamp": "02:00"
    },
    {
      "start": 122.32,
      "duration": 2.24,
      "text": "that brings together experts as well as",
      "timestamp": "02:02"
    },
    {
      "start": 124.56,
      "duration": 4.16,
      "text": "corporations to develop these standards",
      "timestamp": "02:04"
    },
    {
      "start": 128.72,
      "duration": 1.28,
      "text": "uh but there isn't really a",
      "timestamp": "02:08"
    },
    {
      "start": 130.0,
      "duration": 2.319,
      "text": "one-sizefits-all approach uh I think we",
      "timestamp": "02:10"
    },
    {
      "start": 132.319,
      "duration": 2.0,
      "text": "can all agree that nobody wants to",
      "timestamp": "02:12"
    },
    {
      "start": 134.319,
      "duration": 2.241,
      "text": "stifle innovation yet at the same time",
      "timestamp": "02:14"
    },
    {
      "start": 136.56,
      "duration": 2.0,
      "text": "we recognize that as more a the more",
      "timestamp": "02:16"
    },
    {
      "start": 138.56,
      "duration": 2.16,
      "text": "that AI gets adopted the more important",
      "timestamp": "02:18"
    },
    {
      "start": 140.72,
      "duration": 2.0,
      "text": "it is to have some level of guard rails",
      "timestamp": "02:20"
    },
    {
      "start": 142.72,
      "duration": 1.76,
      "text": "or frameworks in which developers can",
      "timestamp": "02:22"
    },
    {
      "start": 144.48,
      "duration": 4.0,
      "text": "build so the EU um who uh they actually",
      "timestamp": "02:24"
    },
    {
      "start": 148.48,
      "duration": 2.32,
      "text": "passed the uh EU artificial intelligence",
      "timestamp": "02:28"
    },
    {
      "start": 150.8,
      "duration": 2.159,
      "text": "act u by show of hands is anybody",
      "timestamp": "02:30"
    },
    {
      "start": 152.959,
      "duration": 3.28,
      "text": "familiar with that okay yeah exactly and",
      "timestamp": "02:32"
    },
    {
      "start": 156.239,
      "duration": 2.64,
      "text": "that's really based off of uh four core",
      "timestamp": "02:36"
    },
    {
      "start": 158.879,
      "duration": 2.401,
      "text": "pillars now this compliance actually",
      "timestamp": "02:38"
    },
    {
      "start": 161.28,
      "duration": 3.44,
      "text": "passed unanimously like twothirds to one",
      "timestamp": "02:41"
    },
    {
      "start": 164.72,
      "duration": 2.56,
      "text": "so the first is on data governance how",
      "timestamp": "02:44"
    },
    {
      "start": 167.28,
      "duration": 2.0,
      "text": "do you know that the data sets that",
      "timestamp": "02:47"
    },
    {
      "start": 169.28,
      "duration": 2.319,
      "text": "you're using are fit for the purpose of",
      "timestamp": "02:49"
    },
    {
      "start": 171.599,
      "duration": 1.601,
      "text": "your application how do how do you do",
      "timestamp": "02:51"
    },
    {
      "start": 173.2,
      "duration": 2.16,
      "text": "that how do you build that to the best",
      "timestamp": "02:53"
    },
    {
      "start": 175.36,
      "duration": 2.0,
      "text": "uh extent possible that you've got the",
      "timestamp": "02:55"
    },
    {
      "start": 177.36,
      "duration": 2.159,
      "text": "right data that is free of errors and",
      "timestamp": "02:57"
    },
    {
      "start": 179.519,
      "duration": 2.481,
      "text": "that is complete the second component of",
      "timestamp": "02:59"
    },
    {
      "start": 182.0,
      "duration": 2.319,
      "text": "this is human oversight right how do you",
      "timestamp": "03:02"
    },
    {
      "start": 184.319,
      "duration": 2.0,
      "text": "ensure that higher risk systems whether",
      "timestamp": "03:04"
    },
    {
      "start": 186.319,
      "duration": 3.2,
      "text": "it be financial services insurance uh",
      "timestamp": "03:06"
    },
    {
      "start": 189.519,
      "duration": 2.0,
      "text": "things of that nature um have some level",
      "timestamp": "03:09"
    },
    {
      "start": 191.519,
      "duration": 2.161,
      "text": "of human validation",
      "timestamp": "03:11"
    },
    {
      "start": 193.68,
      "duration": 2.4,
      "text": "the third is model evaluation right how",
      "timestamp": "03:13"
    },
    {
      "start": 196.08,
      "duration": 1.76,
      "text": "do you establish a quality measurement",
      "timestamp": "03:16"
    },
    {
      "start": 197.84,
      "duration": 3.2,
      "text": "system so new models are being developed",
      "timestamp": "03:17"
    },
    {
      "start": 201.04,
      "duration": 2.64,
      "text": "all the time right uh Llama 4.0 was",
      "timestamp": "03:21"
    },
    {
      "start": 203.68,
      "duration": 1.12,
      "text": "dropped there's going to be a new",
      "timestamp": "03:23"
    },
    {
      "start": 204.8,
      "duration": 2.159,
      "text": "version and another version we speak to",
      "timestamp": "03:24"
    },
    {
      "start": 206.959,
      "duration": 1.761,
      "text": "clients often lots of different",
      "timestamp": "03:26"
    },
    {
      "start": 208.72,
      "duration": 1.84,
      "text": "companies who know that their technology",
      "timestamp": "03:28"
    },
    {
      "start": 210.56,
      "duration": 1.84,
      "text": "may change but that doesn't mean the",
      "timestamp": "03:30"
    },
    {
      "start": 212.4,
      "duration": 2.0,
      "text": "purpose of your model changes so you",
      "timestamp": "03:32"
    },
    {
      "start": 214.4,
      "duration": 1.52,
      "text": "need to have a common framework in which",
      "timestamp": "03:34"
    },
    {
      "start": 215.92,
      "duration": 2.0,
      "text": "you're managing and measuring this and",
      "timestamp": "03:35"
    },
    {
      "start": 217.92,
      "duration": 4.44,
      "text": "then last but not least is data privacy",
      "timestamp": "03:37"
    },
    {
      "start": 222.879,
      "duration": 2.64,
      "text": "uh but compliance uh as I mentioned",
      "timestamp": "03:42"
    },
    {
      "start": 225.519,
      "duration": 2.561,
      "text": "before has been identified as a way to",
      "timestamp": "03:45"
    },
    {
      "start": 228.08,
      "duration": 2.159,
      "text": "uh you know to stifle innovation so",
      "timestamp": "03:48"
    },
    {
      "start": 230.239,
      "duration": 1.521,
      "text": "there have been plenty of bills that are",
      "timestamp": "03:50"
    },
    {
      "start": 231.76,
      "duration": 1.52,
      "text": "overreaching that are extremely",
      "timestamp": "03:51"
    },
    {
      "start": 233.28,
      "duration": 1.92,
      "text": "challenging to ensure that you get to",
      "timestamp": "03:53"
    },
    {
      "start": 235.2,
      "duration": 2.239,
      "text": "these frameworks so one that's been very",
      "timestamp": "03:55"
    },
    {
      "start": 237.439,
      "duration": 1.921,
      "text": "interesting um that just came about is",
      "timestamp": "03:57"
    },
    {
      "start": 239.36,
      "duration": 2.32,
      "text": "California bill SB813",
      "timestamp": "03:59"
    },
    {
      "start": 241.68,
      "duration": 1.759,
      "text": "and this is actually a bill that is a",
      "timestamp": "04:01"
    },
    {
      "start": 243.439,
      "duration": 2.561,
      "text": "public public private partnership where",
      "timestamp": "04:03"
    },
    {
      "start": 246.0,
      "duration": 1.84,
      "text": "instead of trying to drive this through",
      "timestamp": "04:06"
    },
    {
      "start": 247.84,
      "duration": 1.679,
      "text": "compliance it's actually about",
      "timestamp": "04:07"
    },
    {
      "start": 249.519,
      "duration": 2.561,
      "text": "incentivizing developers to adopt",
      "timestamp": "04:09"
    },
    {
      "start": 252.08,
      "duration": 2.48,
      "text": "responsible AI best practices so these",
      "timestamp": "04:12"
    },
    {
      "start": 254.56,
      "duration": 1.6,
      "text": "developers who can get certified by",
      "timestamp": "04:14"
    },
    {
      "start": 256.16,
      "duration": 1.52,
      "text": "third parties actually get",
      "timestamp": "04:16"
    },
    {
      "start": 257.68,
      "duration": 2.799,
      "text": "indemnification from legal action if",
      "timestamp": "04:17"
    },
    {
      "start": 260.479,
      "duration": 1.761,
      "text": "they have been certified in these",
      "timestamp": "04:20"
    },
    {
      "start": 262.24,
      "duration": 2.399,
      "text": "practices so it's an interesting uh very",
      "timestamp": "04:22"
    },
    {
      "start": 264.639,
      "duration": 2.401,
      "text": "interesting uh concept about how to",
      "timestamp": "04:24"
    },
    {
      "start": 267.04,
      "duration": 1.84,
      "text": "bring together the best of both worlds",
      "timestamp": "04:27"
    },
    {
      "start": 268.88,
      "duration": 2.08,
      "text": "because at the end of the day engineers",
      "timestamp": "04:28"
    },
    {
      "start": 270.96,
      "duration": 2.799,
      "text": "who are building these models right and",
      "timestamp": "04:30"
    },
    {
      "start": 273.759,
      "duration": 1.761,
      "text": "creating an incentivization program is",
      "timestamp": "04:33"
    },
    {
      "start": 275.52,
      "duration": 3.119,
      "text": "really quite interesting",
      "timestamp": "04:35"
    },
    {
      "start": 278.639,
      "duration": 2.801,
      "text": "all of these frameworks they really have",
      "timestamp": "04:38"
    },
    {
      "start": 281.44,
      "duration": 1.92,
      "text": "these things in common one it's about",
      "timestamp": "04:41"
    },
    {
      "start": 283.36,
      "duration": 3.119,
      "text": "fairness transparency ultimately",
      "timestamp": "04:43"
    },
    {
      "start": 286.479,
      "duration": 2.401,
      "text": "accountability right so how do you how",
      "timestamp": "04:46"
    },
    {
      "start": 288.88,
      "duration": 3.599,
      "text": "do you define those AI outcomes privacy",
      "timestamp": "04:48"
    },
    {
      "start": 292.479,
      "duration": 2.241,
      "text": "how effective is this model is it robust",
      "timestamp": "04:52"
    },
    {
      "start": 294.72,
      "duration": 1.68,
      "text": "and then ultimately human human",
      "timestamp": "04:54"
    },
    {
      "start": 296.4,
      "duration": 3.0,
      "text": "oversight",
      "timestamp": "04:56"
    },
    {
      "start": 299.84,
      "duration": 3.04,
      "text": "so why do this why does it matter right",
      "timestamp": "04:59"
    },
    {
      "start": 302.88,
      "duration": 2.48,
      "text": "responsible AI is better performing AI",
      "timestamp": "05:02"
    },
    {
      "start": 305.36,
      "duration": 3.2,
      "text": "at the end of the day right to do this",
      "timestamp": "05:05"
    },
    {
      "start": 308.56,
      "duration": 1.76,
      "text": "uh we need to recognize that this",
      "timestamp": "05:08"
    },
    {
      "start": 310.32,
      "duration": 3.2,
      "text": "technology is ubiquitous right I started",
      "timestamp": "05:10"
    },
    {
      "start": 313.52,
      "duration": 1.84,
      "text": "this conversation by sharing that AI is",
      "timestamp": "05:13"
    },
    {
      "start": 315.36,
      "duration": 2.48,
      "text": "a global phenomenon global adoption well",
      "timestamp": "05:15"
    },
    {
      "start": 317.84,
      "duration": 1.76,
      "text": "so are products and you want your",
      "timestamp": "05:17"
    },
    {
      "start": 319.6,
      "duration": 2.56,
      "text": "products to work across borders um and",
      "timestamp": "05:19"
    },
    {
      "start": 322.16,
      "duration": 3.84,
      "text": "across geographies across people",
      "timestamp": "05:22"
    },
    {
      "start": 326.0,
      "duration": 2.0,
      "text": "uh but at the end of the day it's all",
      "timestamp": "05:26"
    },
    {
      "start": 328.0,
      "duration": 2.72,
      "text": "about whether these u these models",
      "timestamp": "05:28"
    },
    {
      "start": 330.72,
      "duration": 1.68,
      "text": "understand both the cost of failure and",
      "timestamp": "05:30"
    },
    {
      "start": 332.4,
      "duration": 2.16,
      "text": "whether they're adopted so one example",
      "timestamp": "05:32"
    },
    {
      "start": 334.56,
      "duration": 1.52,
      "text": "I'd like to raise that has done an",
      "timestamp": "05:34"
    },
    {
      "start": 336.08,
      "duration": 2.8,
      "text": "incredible job of This is is Whimo so",
      "timestamp": "05:36"
    },
    {
      "start": 338.88,
      "duration": 1.92,
      "text": "they publish a report they publish a",
      "timestamp": "05:38"
    },
    {
      "start": 340.8,
      "duration": 2.959,
      "text": "report that shows that um after 10",
      "timestamp": "05:40"
    },
    {
      "start": 343.759,
      "duration": 1.521,
      "text": "million rides and something like 50",
      "timestamp": "05:43"
    },
    {
      "start": 345.28,
      "duration": 3.44,
      "text": "million miles their cars beat human",
      "timestamp": "05:45"
    },
    {
      "start": 348.72,
      "duration": 1.919,
      "text": "drivers if you will or have better uh",
      "timestamp": "05:48"
    },
    {
      "start": 350.639,
      "duration": 2.0,
      "text": "safety records from an accident",
      "timestamp": "05:50"
    },
    {
      "start": 352.639,
      "duration": 3.921,
      "text": "standpoint by literally 10 to one okay",
      "timestamp": "05:52"
    },
    {
      "start": 356.56,
      "duration": 3.28,
      "text": "10 to one these these uh these uh autos",
      "timestamp": "05:56"
    },
    {
      "start": 359.84,
      "duration": 1.52,
      "text": "are driving more miles with less",
      "timestamp": "05:59"
    },
    {
      "start": 361.36,
      "duration": 3.2,
      "text": "accidents than humans and this is the",
      "timestamp": "06:01"
    },
    {
      "start": 364.56,
      "duration": 1.52,
      "text": "kind of trust that is really necessary",
      "timestamp": "06:04"
    },
    {
      "start": 366.08,
      "duration": 3.119,
      "text": "for companies to adopt AI the standard",
      "timestamp": "06:06"
    },
    {
      "start": 369.199,
      "duration": 2.72,
      "text": "for AI performance is a lot higher than",
      "timestamp": "06:09"
    },
    {
      "start": 371.919,
      "duration": 4.56,
      "text": "human performance okay",
      "timestamp": "06:11"
    },
    {
      "start": 376.479,
      "duration": 4.241,
      "text": "so why does this matter um let's see",
      "timestamp": "06:16"
    },
    {
      "start": 380.72,
      "duration": 2.8,
      "text": "actually let me go back one is that uh",
      "timestamp": "06:20"
    },
    {
      "start": 383.52,
      "duration": 1.6,
      "text": "at the end of the day many of these",
      "timestamp": "06:23"
    },
    {
      "start": 385.12,
      "duration": 1.6,
      "text": "models and most models are now being",
      "timestamp": "06:25"
    },
    {
      "start": 386.72,
      "duration": 1.84,
      "text": "developed off of foundation models and",
      "timestamp": "06:26"
    },
    {
      "start": 388.56,
      "duration": 1.6,
      "text": "those foundation models are trained on",
      "timestamp": "06:28"
    },
    {
      "start": 390.16,
      "duration": 3.12,
      "text": "the internet the internet isn't exactly",
      "timestamp": "06:30"
    },
    {
      "start": 393.28,
      "duration": 2.08,
      "text": "completely representative of everybody",
      "timestamp": "06:33"
    },
    {
      "start": 395.36,
      "duration": 3.2,
      "text": "and everything right so that's why you",
      "timestamp": "06:35"
    },
    {
      "start": 398.56,
      "duration": 2.16,
      "text": "have to include specialized annotated",
      "timestamp": "06:38"
    },
    {
      "start": 400.72,
      "duration": 2.16,
      "text": "data and at the end of the day that data",
      "timestamp": "06:40"
    },
    {
      "start": 402.88,
      "duration": 2.159,
      "text": "needs to be mo uh needs to be validated",
      "timestamp": "06:42"
    },
    {
      "start": 405.039,
      "duration": 3.0,
      "text": "right",
      "timestamp": "06:45"
    },
    {
      "start": 408.72,
      "duration": 1.28,
      "text": "and I just like this picture because",
      "timestamp": "06:48"
    },
    {
      "start": 410.0,
      "duration": 2.72,
      "text": "it's kind of funny but the at the end of",
      "timestamp": "06:50"
    },
    {
      "start": 412.72,
      "duration": 1.599,
      "text": "the day right without the right data and",
      "timestamp": "06:52"
    },
    {
      "start": 414.319,
      "duration": 2.16,
      "text": "model valuation um practices in place",
      "timestamp": "06:54"
    },
    {
      "start": 416.479,
      "duration": 2.241,
      "text": "models can hallucinate right and this is",
      "timestamp": "06:56"
    },
    {
      "start": 418.72,
      "duration": 4.52,
      "text": "a a good example of of hallucination",
      "timestamp": "06:58"
    },
    {
      "start": 424.72,
      "duration": 2.879,
      "text": "so why why does human context matter",
      "timestamp": "07:04"
    },
    {
      "start": 427.599,
      "duration": 1.761,
      "text": "okay why can't we just synthetically",
      "timestamp": "07:07"
    },
    {
      "start": 429.36,
      "duration": 2.559,
      "text": "generate everything um well the",
      "timestamp": "07:09"
    },
    {
      "start": 431.919,
      "duration": 2.241,
      "text": "challenge is is that models that are",
      "timestamp": "07:11"
    },
    {
      "start": 434.16,
      "duration": 2.08,
      "text": "trained on models can carry forward",
      "timestamp": "07:14"
    },
    {
      "start": 436.24,
      "duration": 2.079,
      "text": "their bias and at the end of the day you",
      "timestamp": "07:16"
    },
    {
      "start": 438.319,
      "duration": 1.841,
      "text": "need to understand what you are training",
      "timestamp": "07:18"
    },
    {
      "start": 440.16,
      "duration": 2.159,
      "text": "for so I know there's a lot on this",
      "timestamp": "07:20"
    },
    {
      "start": 442.319,
      "duration": 2.0,
      "text": "slide but it's a pretty simple example",
      "timestamp": "07:22"
    },
    {
      "start": 444.319,
      "duration": 1.681,
      "text": "if you're building a model right and",
      "timestamp": "07:24"
    },
    {
      "start": 446.0,
      "duration": 2.8,
      "text": "here's this model with parents pushing",
      "timestamp": "07:26"
    },
    {
      "start": 448.8,
      "duration": 2.799,
      "text": "strollers well if you synthetically",
      "timestamp": "07:28"
    },
    {
      "start": 451.599,
      "duration": 2.16,
      "text": "generate this you may end up in",
      "timestamp": "07:31"
    },
    {
      "start": 453.759,
      "duration": 2.401,
      "text": "situations where for example does this",
      "timestamp": "07:33"
    },
    {
      "start": 456.16,
      "duration": 1.28,
      "text": "look right would you see a stroller",
      "timestamp": "07:36"
    },
    {
      "start": 457.44,
      "duration": 1.44,
      "text": "without a baby in the middle of the",
      "timestamp": "07:37"
    },
    {
      "start": 458.88,
      "duration": 2.96,
      "text": "street right would you see a a stroller",
      "timestamp": "07:38"
    },
    {
      "start": 461.84,
      "duration": 1.84,
      "text": "without a baby and these are some of the",
      "timestamp": "07:41"
    },
    {
      "start": 463.68,
      "duration": 1.76,
      "text": "challenges is that as humans we",
      "timestamp": "07:43"
    },
    {
      "start": 465.44,
      "duration": 2.479,
      "text": "recognize that context so oftent times",
      "timestamp": "07:45"
    },
    {
      "start": 467.919,
      "duration": 2.081,
      "text": "even if synthetic data is being used to",
      "timestamp": "07:47"
    },
    {
      "start": 470.0,
      "duration": 2.16,
      "text": "train models there's actually a human",
      "timestamp": "07:50"
    },
    {
      "start": 472.16,
      "duration": 2.08,
      "text": "validation component that's necessary to",
      "timestamp": "07:52"
    },
    {
      "start": 474.24,
      "duration": 1.92,
      "text": "ensure that the models that the",
      "timestamp": "07:54"
    },
    {
      "start": 476.16,
      "duration": 1.599,
      "text": "synthetic data that's being created",
      "timestamp": "07:56"
    },
    {
      "start": 477.759,
      "duration": 4.44,
      "text": "actually has the appropriate context",
      "timestamp": "07:57"
    },
    {
      "start": 482.56,
      "duration": 3.6,
      "text": "and um ultimately central to developing",
      "timestamp": "08:02"
    },
    {
      "start": 486.16,
      "duration": 2.479,
      "text": "responsible AI beyond human insight is",
      "timestamp": "08:06"
    },
    {
      "start": 488.639,
      "duration": 2.24,
      "text": "really data diversity so that data",
      "timestamp": "08:08"
    },
    {
      "start": 490.879,
      "duration": 1.76,
      "text": "diversity typically goes around",
      "timestamp": "08:10"
    },
    {
      "start": 492.639,
      "duration": 2.481,
      "text": "demographics environment language edge",
      "timestamp": "08:12"
    },
    {
      "start": 495.12,
      "duration": 3.6,
      "text": "cases and modalities so in terms of uh",
      "timestamp": "08:15"
    },
    {
      "start": 498.72,
      "duration": 1.599,
      "text": "uh different environments as an example",
      "timestamp": "08:18"
    },
    {
      "start": 500.319,
      "duration": 2.0,
      "text": "if you're building a self-driving car",
      "timestamp": "08:20"
    },
    {
      "start": 502.319,
      "duration": 1.521,
      "text": "you need to know if you're in Singapore",
      "timestamp": "08:22"
    },
    {
      "start": 503.84,
      "duration": 2.24,
      "text": "if there you know drivers sit on the uh",
      "timestamp": "08:23"
    },
    {
      "start": 506.08,
      "duration": 2.48,
      "text": "right side versus for example um you",
      "timestamp": "08:26"
    },
    {
      "start": 508.56,
      "duration": 1.84,
      "text": "know in China on the left side so",
      "timestamp": "08:28"
    },
    {
      "start": 510.4,
      "duration": 1.44,
      "text": "understanding that is really important",
      "timestamp": "08:30"
    },
    {
      "start": 511.84,
      "duration": 2.72,
      "text": "because um you need to reduce you want",
      "timestamp": "08:31"
    },
    {
      "start": 514.56,
      "duration": 1.599,
      "text": "to reduce the overfitting of",
      "timestamp": "08:34"
    },
    {
      "start": 516.159,
      "duration": 2.161,
      "text": "applications and to improve",
      "timestamp": "08:36"
    },
    {
      "start": 518.32,
      "duration": 1.68,
      "text": "generalization that works across",
      "timestamp": "08:38"
    },
    {
      "start": 520.0,
      "duration": 3.399,
      "text": "multiple regions",
      "timestamp": "08:40"
    },
    {
      "start": 523.68,
      "duration": 2.0,
      "text": "so one of the things that uh we've done",
      "timestamp": "08:43"
    },
    {
      "start": 525.68,
      "duration": 1.839,
      "text": "as a company is we're actually building",
      "timestamp": "08:45"
    },
    {
      "start": 527.519,
      "duration": 3.041,
      "text": "an uh an Africacentric data set and the",
      "timestamp": "08:47"
    },
    {
      "start": 530.56,
      "duration": 1.36,
      "text": "reason we're doing this for research",
      "timestamp": "08:50"
    },
    {
      "start": 531.92,
      "duration": 1.84,
      "text": "purposes is because it's a region that",
      "timestamp": "08:51"
    },
    {
      "start": 533.76,
      "duration": 2.079,
      "text": "has been consistently under reppresented",
      "timestamp": "08:53"
    },
    {
      "start": 535.839,
      "duration": 2.081,
      "text": "in the world so by building this",
      "timestamp": "08:55"
    },
    {
      "start": 537.92,
      "duration": 2.08,
      "text": "additional data set we hope that people",
      "timestamp": "08:57"
    },
    {
      "start": 540.0,
      "duration": 1.92,
      "text": "for example can train their models to",
      "timestamp": "09:00"
    },
    {
      "start": 541.92,
      "duration": 3.359,
      "text": "recognize um and um ultimately build",
      "timestamp": "09:01"
    },
    {
      "start": 545.279,
      "duration": 1.281,
      "text": "applications that are more",
      "timestamp": "09:05"
    },
    {
      "start": 546.56,
      "duration": 1.92,
      "text": "representative of of where we live and",
      "timestamp": "09:06"
    },
    {
      "start": 548.48,
      "duration": 3.479,
      "text": "where we live globally",
      "timestamp": "09:08"
    },
    {
      "start": 552.0,
      "duration": 1.6,
      "text": "the last thing I want to note I know",
      "timestamp": "09:12"
    },
    {
      "start": 553.6,
      "duration": 1.44,
      "text": "this is a little bit of a goofy slide is",
      "timestamp": "09:13"
    },
    {
      "start": 555.04,
      "duration": 2.16,
      "text": "that diversity is important not only in",
      "timestamp": "09:15"
    },
    {
      "start": 557.2,
      "duration": 1.6,
      "text": "terms of data diversity but it's also",
      "timestamp": "09:17"
    },
    {
      "start": 558.8,
      "duration": 2.32,
      "text": "about who builds your AI one of the",
      "timestamp": "09:18"
    },
    {
      "start": 561.12,
      "duration": 1.6,
      "text": "practices that we have as a company is",
      "timestamp": "09:21"
    },
    {
      "start": 562.72,
      "duration": 3.44,
      "text": "we not only hire at least 50% uh women",
      "timestamp": "09:22"
    },
    {
      "start": 566.16,
      "duration": 1.28,
      "text": "we actually carry that all the way",
      "timestamp": "09:26"
    },
    {
      "start": 567.44,
      "duration": 2.32,
      "text": "through up to the top and the reason",
      "timestamp": "09:27"
    },
    {
      "start": 569.76,
      "duration": 2.0,
      "text": "being is that different people and",
      "timestamp": "09:29"
    },
    {
      "start": 571.76,
      "duration": 2.88,
      "text": "different ideas breed you know better",
      "timestamp": "09:31"
    },
    {
      "start": 574.64,
      "duration": 3.52,
      "text": "and more rich results uh if anybody here",
      "timestamp": "09:34"
    },
    {
      "start": 578.16,
      "duration": 2.32,
      "text": "is an engineer when did you ever get it",
      "timestamp": "09:38"
    },
    {
      "start": 580.48,
      "duration": 1.76,
      "text": "right and all the requirements the first",
      "timestamp": "09:40"
    },
    {
      "start": 582.24,
      "duration": 3.76,
      "text": "time probably never right requirements",
      "timestamp": "09:42"
    },
    {
      "start": 586.0,
      "duration": 1.92,
      "text": "are iterative they continue to build and",
      "timestamp": "09:46"
    },
    {
      "start": 587.92,
      "duration": 2.4,
      "text": "so having people with a diversity of",
      "timestamp": "09:47"
    },
    {
      "start": 590.32,
      "duration": 2.639,
      "text": "thinking diversity of backgrounds is an",
      "timestamp": "09:50"
    },
    {
      "start": 592.959,
      "duration": 1.761,
      "text": "incredible help because we are",
      "timestamp": "09:52"
    },
    {
      "start": 594.72,
      "duration": 1.6,
      "text": "ultimately training AI that needs to",
      "timestamp": "09:54"
    },
    {
      "start": 596.32,
      "duration": 1.76,
      "text": "learn from experience and that's going",
      "timestamp": "09:56"
    },
    {
      "start": 598.08,
      "duration": 3.04,
      "text": "to change over time",
      "timestamp": "09:58"
    },
    {
      "start": 601.12,
      "duration": 2.08,
      "text": "uh last but not least I just wanted to",
      "timestamp": "10:01"
    },
    {
      "start": 603.2,
      "duration": 1.84,
      "text": "put a couple of points in here around",
      "timestamp": "10:03"
    },
    {
      "start": 605.04,
      "duration": 2.32,
      "text": "responsible AI learnings there are many",
      "timestamp": "10:05"
    },
    {
      "start": 607.36,
      "duration": 1.44,
      "text": "different checklists that are out there",
      "timestamp": "10:07"
    },
    {
      "start": 608.8,
      "duration": 2.08,
      "text": "and I'd certainly be happy to share a",
      "timestamp": "10:08"
    },
    {
      "start": 610.88,
      "duration": 1.6,
      "text": "bunch of different um frameworks and",
      "timestamp": "10:10"
    },
    {
      "start": 612.48,
      "duration": 2.0,
      "text": "references for how to develop",
      "timestamp": "10:12"
    },
    {
      "start": 614.48,
      "duration": 1.919,
      "text": "responsible AI but I think the point",
      "timestamp": "10:14"
    },
    {
      "start": 616.399,
      "duration": 1.361,
      "text": "that I really wanted to share is",
      "timestamp": "10:16"
    },
    {
      "start": 617.76,
      "duration": 2.24,
      "text": "ultimately this",
      "timestamp": "10:17"
    },
    {
      "start": 620.0,
      "duration": 2.64,
      "text": "responsible AI is it's a development",
      "timestamp": "10:20"
    },
    {
      "start": 622.64,
      "duration": 2.72,
      "text": "framework okay it's not about",
      "timestamp": "10:22"
    },
    {
      "start": 625.36,
      "duration": 2.64,
      "text": "necessarily just ethics or morality it's",
      "timestamp": "10:25"
    },
    {
      "start": 628.0,
      "duration": 1.92,
      "text": "really about how do you build a model",
      "timestamp": "10:28"
    },
    {
      "start": 629.92,
      "duration": 2.159,
      "text": "that is performant to your application",
      "timestamp": "10:29"
    },
    {
      "start": 632.079,
      "duration": 2.32,
      "text": "and that really starts with practices",
      "timestamp": "10:32"
    },
    {
      "start": 634.399,
      "duration": 2.081,
      "text": "that you think about when you begin to",
      "timestamp": "10:34"
    },
    {
      "start": 636.48,
      "duration": 2.479,
      "text": "build a model okay so if anybody has",
      "timestamp": "10:36"
    },
    {
      "start": 638.959,
      "duration": 2.721,
      "text": "again built models built models or built",
      "timestamp": "10:38"
    },
    {
      "start": 641.68,
      "duration": 2.08,
      "text": "software if you think about security as",
      "timestamp": "10:41"
    },
    {
      "start": 643.76,
      "duration": 1.92,
      "text": "an afterthought it's really challenging",
      "timestamp": "10:43"
    },
    {
      "start": 645.68,
      "duration": 2.0,
      "text": "right so the idea here is think with",
      "timestamp": "10:45"
    },
    {
      "start": 647.68,
      "duration": 2.32,
      "text": "these parameters in mind first the first",
      "timestamp": "10:47"
    },
    {
      "start": 650.0,
      "duration": 3.6,
      "text": "is what you know what uh what AI do you",
      "timestamp": "10:50"
    },
    {
      "start": 653.6,
      "duration": 2.0,
      "text": "want to develop how do you measure the",
      "timestamp": "10:53"
    },
    {
      "start": 655.6,
      "duration": 2.239,
      "text": "effectiveness of that AI do you actually",
      "timestamp": "10:55"
    },
    {
      "start": 657.839,
      "duration": 1.921,
      "text": "have a way in which you can measure it",
      "timestamp": "10:57"
    },
    {
      "start": 659.76,
      "duration": 1.68,
      "text": "because the world changes it changes",
      "timestamp": "10:59"
    },
    {
      "start": 661.44,
      "duration": 1.76,
      "text": "constantly how do you know if your model",
      "timestamp": "11:01"
    },
    {
      "start": 663.2,
      "duration": 1.92,
      "text": "is right if you change your model um you",
      "timestamp": "11:03"
    },
    {
      "start": 665.12,
      "duration": 1.68,
      "text": "know the model that you're using how do",
      "timestamp": "11:05"
    },
    {
      "start": 666.8,
      "duration": 2.08,
      "text": "you ensure that again the model the uh",
      "timestamp": "11:06"
    },
    {
      "start": 668.88,
      "duration": 1.36,
      "text": "the application you're building is",
      "timestamp": "11:08"
    },
    {
      "start": 670.24,
      "duration": 1.92,
      "text": "performant so when you build this and",
      "timestamp": "11:10"
    },
    {
      "start": 672.16,
      "duration": 1.44,
      "text": "you come up with these frameworks and",
      "timestamp": "11:12"
    },
    {
      "start": 673.6,
      "duration": 1.52,
      "text": "you educate your developers on this",
      "timestamp": "11:13"
    },
    {
      "start": 675.12,
      "duration": 3.2,
      "text": "upfront the ability to build models that",
      "timestamp": "11:15"
    },
    {
      "start": 678.32,
      "duration": 3.199,
      "text": "are more performant more accurate um is",
      "timestamp": "11:18"
    },
    {
      "start": 681.519,
      "duration": 2.0,
      "text": "is much much higher but it's really",
      "timestamp": "11:21"
    },
    {
      "start": 683.519,
      "duration": 1.921,
      "text": "difficult to do if you do this midway",
      "timestamp": "11:23"
    },
    {
      "start": 685.44,
      "duration": 2.24,
      "text": "through your process so my",
      "timestamp": "11:25"
    },
    {
      "start": 687.68,
      "duration": 2.88,
      "text": "recommendation is measure twice cut once",
      "timestamp": "11:27"
    },
    {
      "start": 690.56,
      "duration": 2.16,
      "text": "have a plan the tech is going to change",
      "timestamp": "11:30"
    },
    {
      "start": 692.72,
      "duration": 2.799,
      "text": "but the evaluation is not thank you",
      "timestamp": "11:32"
    },
    {
      "start": 695.519,
      "duration": 2.88,
      "text": "everybody",
      "timestamp": "11:35"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.391445",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}