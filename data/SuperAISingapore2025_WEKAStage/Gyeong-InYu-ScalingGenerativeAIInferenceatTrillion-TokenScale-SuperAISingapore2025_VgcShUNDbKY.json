{
  "video_id": "VgcShUNDbKY",
  "video_title": "Gyeong-In Yu - Scaling Generative AI Inference at Trillion-Token Scale - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=VgcShUNDbKY",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T14:05:38+00:00",
  "duration_seconds": null,
  "view_count": 15,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: Scaling Generative AI Inference at Trillion-Token Scale\n\nSpeaker:\nGyeong-In Yu, CTO @ FriendliAI\n\nStage: WEKA Stage\n#superai #friendliai #genai #inference #aiinfrastructure \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 278,
    "aggregated_text": "hi i am kyouong from friendly ai and thank you for joining us today um in this session i am going to talk about scaling generative ai inference at training on tokens scale uh so um before getting started a little bit about me i am cto at friendly ai and um i build technologies for um genai inference acceleration so um in 2025 more and more people talk about ai inference than before and um we are seeing a growing demand and importance of inference so um this is uh primarily due to the large scale adoption of ai in our daily life including um officers ai models so um in these days um companies are um training fine-tuned model starting from an officer's foundation model instead of training the models from scratch so because of that um we are seeing an an increasing number of gpus are now being used for inference rather than training um secondly uh we are seeing uh increasing computation use in at inference time to unlock greater intelligence um starting from openai's o1 model we have a bunch of reasoning models at the market and um companies are starting to adopt these reasoning models into their ai services lastly um uh we should mention agentic ai or ai agent and this is one of the uh biggest reason why we have a um radical increase in inference demand um nowadays that humans but also not only humans but also um software components are um calling model apis and we believe um this uh agentic ai increases the amount of api calls by up to 10x so um the problem right the problem right now is that despite this rapid ai adoption um developers in the company are wasting months setting up infrastructure and um running the gpu infrastructure instead of building the building their products and focusing on their core value first of all of course the most important part is gpu cost how can we reduce them um because of the rising gpu cost they are cutting into your profits and um this is especially important when you want to scale your ai service uh the next one is we need specialized optimization for different types of services uh for example let's say openai is deep research um people can wait up to tens of minutes for getting response from deep research but for other type of application let's say um we have typical chatbot in that case um people the people expect the response will the service will give response in a few seconds so um depending on the um ai service that you want to run we have to apply different we have to apply the optimization differently lastly um we should mention the infra operation how can we manage the infrastructure efficiently without um the minimal human effort so um uh because of this um optim uh optimizing genai inference is becoming more crucial for the success of your um ai business and we uh roughly have two types of goals first we want to keep our users happy with our application and second we want to manage our cost so uh to keep our users happy we want to keep the generation quality as high as possible with low response time and for the for to manage the cost uh reasonable we need high surfer from uh our model api and so we can reduce the gpu cost the challenge is um these goals often have trade of relationship between each other and um this is what i i am this is going to be the main topic of this session so um friendly ai is a gpu platform for acceler accelerated ai and our approach for um handling the above handling the challenge is to consider trade-offs in the context of application scenarios so um based on this principle um we've built our software and infrastructure from scratch and now we are growing fast and now we are serving hundreds of billions of tokens every day um we are trying to make our platform as simple as possible and easy to use and now so you can run hundreds of thousands of geni models in the in the hog hoging face hub within just a few clicks so as demand for fast and efficient ai increases researchers have devised more advanced optimization technologies for accelerating this kind of uh very expensive workload so we can't name all of them but i have brought a few important um technologies here so starting from batching we need to uh develop a efficient kernel to uh fully harness the full computational capacity of gpus we need to choose the right quantization method for our application we need to employ caching and so on so um these different optimization techniques um have different implications on model quality and response time and surplus so um we have to uh carefully choose what to use but with friendly ai you don't need to um worry about these um complex optimization spaces because we um we provide a good combination of these optimization spaces based on our um long experience um this is possible because most of the uh optimization technology share the same underlying principle um that is the optimizations are mainly built for mitigating the same problem memory band is bound so um this graph is um uh showing the classic roofline model for um analyzing uh hardware efficiency uh so at the x-axis we have uh arithmetic intensity and y-axis we have um attainable flops and the green dotted line shows a boundary between memory vendor bound and compute bound so um the situation when you run a uh ji models is it's naturally very bend bound and we need to apply optimizations to make it competit bound so it means that we want to um switch from this area to that area this is because the nature of llm inference llm uses auto reggressive decoding and it uh generates one tok at a time um so this is very memory vendor bound and we need to solve this problem by applying various optimizations so the most important one is batching um computations on gpus incorporate two types of input data the first one is model weight which is shared across multiple requests and the second one is activation so um if we can group multiple requests from different users together and run it once run it at once we can reduce the amount of data that we that we need to read and therefore we can increase arithmetic intensity so this enables transition from memory bound to compute bound reason so um uh let me introduce a important technique in this area so it is named continuous fetching and um it is in invented by me at friendly ai and it's mainly devised for handling early finished and late arrive request efficiently um but we cannot conclude the story for optimization because it often have trade-offs between other uh with other optimizations one prominent example is batching and caching um in the context of jai inference caching means reusing tensors stored in kb cache and this is mainly used for skipping prefer computation for the over overlapped part of input tokens um we often have overlap in the input tokens between requests because um we may have a long system prompt or um we have shared context between multiple requests for example um the shared documentation for um different requests we also need to handle multimodel data as well as text efficiently in the caching system and we need off gpu variant support and of course in large at large scale deployment we need cash away routing between replicas um the point is caching has a thread of relationship between batching just like ordinary caching um kach in um recent ai systems have two roles the first one is scratch pad memory for handling the current batch of requests and the second one is storing kway tensors for future reuse if the working set does not fit into the scratch pace the the system will slow down significantly so um this is why we have a um creative relationship between batching and caching so when we need to increase the batch size we need to carefully think about it because it may hinder um cachet rate of your application uh we also have quantization also have a trade-off relationship between the goals so quantization may degrade your generation quality so when you want to use it you have to be careful when whether it's good enough for um satisfying your end users so um i want to highlight at this point so um our uh we have our own fp8 quantization method and it pre preserves the model quality while speeding up the execution so um based on the techniques that we've built and researched we have a product named friendly suite uh friendly suite is composed of mainly composed of two components so first we have container microser at the bottom level and on top of that we've built fully managed cloud service um friendly container is uh is a you can think it as a simple taker image that you can pull from our image registry and deploy it on your kubernetes cluster we provide um we we provide a good combination of optimization technologies and so we so we can deliver gpu cost reduction and fast response um for the fully managed service part i want to talk about friendly dedicated endpoints it's a fully managed service running on our um cloud infrastructure and you can easily um pick up a model from hoging face hub and deploy the model within a few clicks of course we provide other functionalities like uh high availability autoscaling depending on your um incoming traffic and intelligent routing between replicas um i will show you a demo video now you can easily deploy models directly from hugging face hub with friendly endpoints just go to deploy options on hugging face model page and select friendly endpoints it's that simple you can deploy a model with just one click [Music] once the deployment is complete head over to the friendly suite to access even more powerful features now you can enjoy the fastest and cost-effective inference with friendly ai's gpu optimized inference engine [Music] yeah um before ending my session um i want to uh share a news um we in uh this week we've launched a promotion program uh so we uh give a free 10k dollar credits so um uh so if you're interested please um check out the qr code here and apply in two minutes and we are also running a booth over there so um please feel free to come to our booth and uh have let's so let's have let's talk thank you",
    "text_length": 10044,
    "word_count": 1827
  },
  "segments": [
    {
      "start": 7.359,
      "duration": 2.32,
      "text": "hi i am kyouong from friendly ai and",
      "timestamp": "00:07"
    },
    {
      "start": 9.679,
      "duration": 2.241,
      "text": "thank you for joining us today um in",
      "timestamp": "00:09"
    },
    {
      "start": 11.92,
      "duration": 1.52,
      "text": "this session i am going to talk about",
      "timestamp": "00:11"
    },
    {
      "start": 13.44,
      "duration": 2.0,
      "text": "scaling generative ai inference at",
      "timestamp": "00:13"
    },
    {
      "start": 15.44,
      "duration": 2.8,
      "text": "training on tokens scale uh so um before",
      "timestamp": "00:15"
    },
    {
      "start": 18.24,
      "duration": 1.76,
      "text": "getting started a little bit about me i",
      "timestamp": "00:18"
    },
    {
      "start": 20.0,
      "duration": 3.84,
      "text": "am cto at friendly ai and um i build",
      "timestamp": "00:20"
    },
    {
      "start": 23.84,
      "duration": 2.8,
      "text": "technologies for um genai inference",
      "timestamp": "00:23"
    },
    {
      "start": 26.64,
      "duration": 2.479,
      "text": "acceleration",
      "timestamp": "00:26"
    },
    {
      "start": 29.119,
      "duration": 3.119,
      "text": "so um in 2025 more and more people talk",
      "timestamp": "00:29"
    },
    {
      "start": 32.239,
      "duration": 2.641,
      "text": "about ai inference than before and um we",
      "timestamp": "00:32"
    },
    {
      "start": 34.88,
      "duration": 1.839,
      "text": "are seeing a growing demand and",
      "timestamp": "00:34"
    },
    {
      "start": 36.719,
      "duration": 3.281,
      "text": "importance of inference so um this is uh",
      "timestamp": "00:36"
    },
    {
      "start": 40.0,
      "duration": 1.84,
      "text": "primarily due to the large scale",
      "timestamp": "00:40"
    },
    {
      "start": 41.84,
      "duration": 2.32,
      "text": "adoption of ai in our daily life",
      "timestamp": "00:41"
    },
    {
      "start": 44.16,
      "duration": 4.239,
      "text": "including um officers ai models so um in",
      "timestamp": "00:44"
    },
    {
      "start": 48.399,
      "duration": 3.201,
      "text": "these days um companies are um training",
      "timestamp": "00:48"
    },
    {
      "start": 51.6,
      "duration": 2.16,
      "text": "fine-tuned model starting from an",
      "timestamp": "00:51"
    },
    {
      "start": 53.76,
      "duration": 2.08,
      "text": "officer's foundation model instead of",
      "timestamp": "00:53"
    },
    {
      "start": 55.84,
      "duration": 2.48,
      "text": "training the models from scratch so",
      "timestamp": "00:55"
    },
    {
      "start": 58.32,
      "duration": 2.719,
      "text": "because of that um we are seeing an an",
      "timestamp": "00:58"
    },
    {
      "start": 61.039,
      "duration": 2.721,
      "text": "increasing number of gpus are now being",
      "timestamp": "01:01"
    },
    {
      "start": 63.76,
      "duration": 3.039,
      "text": "used for inference rather than training",
      "timestamp": "01:03"
    },
    {
      "start": 66.799,
      "duration": 3.441,
      "text": "um secondly uh we are seeing uh",
      "timestamp": "01:06"
    },
    {
      "start": 70.24,
      "duration": 2.72,
      "text": "increasing computation use in at",
      "timestamp": "01:10"
    },
    {
      "start": 72.96,
      "duration": 2.56,
      "text": "inference time to unlock greater",
      "timestamp": "01:12"
    },
    {
      "start": 75.52,
      "duration": 3.04,
      "text": "intelligence um starting from openai's",
      "timestamp": "01:15"
    },
    {
      "start": 78.56,
      "duration": 2.559,
      "text": "o1 model we have a bunch of reasoning",
      "timestamp": "01:18"
    },
    {
      "start": 81.119,
      "duration": 2.721,
      "text": "models at the market and um companies",
      "timestamp": "01:21"
    },
    {
      "start": 83.84,
      "duration": 2.48,
      "text": "are starting to adopt these reasoning",
      "timestamp": "01:23"
    },
    {
      "start": 86.32,
      "duration": 4.56,
      "text": "models into their ai services lastly um",
      "timestamp": "01:26"
    },
    {
      "start": 90.88,
      "duration": 3.44,
      "text": "uh we should mention agentic ai or ai",
      "timestamp": "01:30"
    },
    {
      "start": 94.32,
      "duration": 3.52,
      "text": "agent and this is one of the uh biggest",
      "timestamp": "01:34"
    },
    {
      "start": 97.84,
      "duration": 4.319,
      "text": "reason why we have a um radical increase",
      "timestamp": "01:37"
    },
    {
      "start": 102.159,
      "duration": 4.561,
      "text": "in inference demand um nowadays that",
      "timestamp": "01:42"
    },
    {
      "start": 106.72,
      "duration": 2.64,
      "text": "humans but also not only humans but also",
      "timestamp": "01:46"
    },
    {
      "start": 109.36,
      "duration": 3.52,
      "text": "um software components are um calling",
      "timestamp": "01:49"
    },
    {
      "start": 112.88,
      "duration": 5.76,
      "text": "model apis and we believe um this uh",
      "timestamp": "01:52"
    },
    {
      "start": 118.64,
      "duration": 3.519,
      "text": "agentic ai increases the amount of api",
      "timestamp": "01:58"
    },
    {
      "start": 122.159,
      "duration": 4.32,
      "text": "calls by up to 10x",
      "timestamp": "02:02"
    },
    {
      "start": 126.479,
      "duration": 2.081,
      "text": "so um the problem right the problem",
      "timestamp": "02:06"
    },
    {
      "start": 128.56,
      "duration": 3.2,
      "text": "right now is that despite this rapid ai",
      "timestamp": "02:08"
    },
    {
      "start": 131.76,
      "duration": 2.88,
      "text": "adoption um developers in the company",
      "timestamp": "02:11"
    },
    {
      "start": 134.64,
      "duration": 2.0,
      "text": "are wasting months setting up",
      "timestamp": "02:14"
    },
    {
      "start": 136.64,
      "duration": 3.76,
      "text": "infrastructure and um running the gpu",
      "timestamp": "02:16"
    },
    {
      "start": 140.4,
      "duration": 2.4,
      "text": "infrastructure instead of building the",
      "timestamp": "02:20"
    },
    {
      "start": 142.8,
      "duration": 1.84,
      "text": "building their products and focusing on",
      "timestamp": "02:22"
    },
    {
      "start": 144.64,
      "duration": 2.56,
      "text": "their core value",
      "timestamp": "02:24"
    },
    {
      "start": 147.2,
      "duration": 1.44,
      "text": "first of all of course the most",
      "timestamp": "02:27"
    },
    {
      "start": 148.64,
      "duration": 2.48,
      "text": "important part is gpu cost how can we",
      "timestamp": "02:28"
    },
    {
      "start": 151.12,
      "duration": 2.8,
      "text": "reduce them um because of the rising gpu",
      "timestamp": "02:31"
    },
    {
      "start": 153.92,
      "duration": 2.959,
      "text": "cost they are cutting into your profits",
      "timestamp": "02:33"
    },
    {
      "start": 156.879,
      "duration": 2.801,
      "text": "and um this is especially important when",
      "timestamp": "02:36"
    },
    {
      "start": 159.68,
      "duration": 3.279,
      "text": "you want to scale your ai service",
      "timestamp": "02:39"
    },
    {
      "start": 162.959,
      "duration": 3.521,
      "text": "uh the next one is we need specialized",
      "timestamp": "02:42"
    },
    {
      "start": 166.48,
      "duration": 2.24,
      "text": "optimization for different types of",
      "timestamp": "02:46"
    },
    {
      "start": 168.72,
      "duration": 3.519,
      "text": "services uh for example let's say openai",
      "timestamp": "02:48"
    },
    {
      "start": 172.239,
      "duration": 3.521,
      "text": "is deep research um people can wait up",
      "timestamp": "02:52"
    },
    {
      "start": 175.76,
      "duration": 2.64,
      "text": "to tens of minutes for getting response",
      "timestamp": "02:55"
    },
    {
      "start": 178.4,
      "duration": 3.28,
      "text": "from deep research but for other type of",
      "timestamp": "02:58"
    },
    {
      "start": 181.68,
      "duration": 2.16,
      "text": "application let's say um we have typical",
      "timestamp": "03:01"
    },
    {
      "start": 183.84,
      "duration": 2.64,
      "text": "chatbot in that case um people the",
      "timestamp": "03:03"
    },
    {
      "start": 186.48,
      "duration": 2.72,
      "text": "people expect the response will the",
      "timestamp": "03:06"
    },
    {
      "start": 189.2,
      "duration": 1.679,
      "text": "service will give response in a few",
      "timestamp": "03:09"
    },
    {
      "start": 190.879,
      "duration": 4.321,
      "text": "seconds so um depending on the um ai",
      "timestamp": "03:10"
    },
    {
      "start": 195.2,
      "duration": 1.92,
      "text": "service that you want to run we have to",
      "timestamp": "03:15"
    },
    {
      "start": 197.12,
      "duration": 2.0,
      "text": "apply different we have to apply the",
      "timestamp": "03:17"
    },
    {
      "start": 199.12,
      "duration": 3.52,
      "text": "optimization differently lastly um we",
      "timestamp": "03:19"
    },
    {
      "start": 202.64,
      "duration": 2.72,
      "text": "should mention the infra operation how",
      "timestamp": "03:22"
    },
    {
      "start": 205.36,
      "duration": 1.84,
      "text": "can we manage the infrastructure",
      "timestamp": "03:25"
    },
    {
      "start": 207.2,
      "duration": 3.679,
      "text": "efficiently without um the minimal human",
      "timestamp": "03:27"
    },
    {
      "start": 210.879,
      "duration": 1.521,
      "text": "effort",
      "timestamp": "03:30"
    },
    {
      "start": 212.4,
      "duration": 2.479,
      "text": "so um",
      "timestamp": "03:32"
    },
    {
      "start": 214.879,
      "duration": 2.481,
      "text": "uh because of this um optim uh",
      "timestamp": "03:34"
    },
    {
      "start": 217.36,
      "duration": 2.239,
      "text": "optimizing genai inference is becoming",
      "timestamp": "03:37"
    },
    {
      "start": 219.599,
      "duration": 2.401,
      "text": "more crucial for the success of your um",
      "timestamp": "03:39"
    },
    {
      "start": 222.0,
      "duration": 3.68,
      "text": "ai business and we uh roughly have two",
      "timestamp": "03:42"
    },
    {
      "start": 225.68,
      "duration": 2.559,
      "text": "types of goals first we want to keep our",
      "timestamp": "03:45"
    },
    {
      "start": 228.239,
      "duration": 2.401,
      "text": "users happy with our application and",
      "timestamp": "03:48"
    },
    {
      "start": 230.64,
      "duration": 3.2,
      "text": "second we want to manage our cost",
      "timestamp": "03:50"
    },
    {
      "start": 233.84,
      "duration": 2.8,
      "text": "so uh to keep our users happy we want to",
      "timestamp": "03:53"
    },
    {
      "start": 236.64,
      "duration": 2.08,
      "text": "keep the generation quality as high as",
      "timestamp": "03:56"
    },
    {
      "start": 238.72,
      "duration": 3.28,
      "text": "possible with low response time",
      "timestamp": "03:58"
    },
    {
      "start": 242.0,
      "duration": 3.84,
      "text": "and for the for to manage the cost uh",
      "timestamp": "04:02"
    },
    {
      "start": 245.84,
      "duration": 3.44,
      "text": "reasonable we need high surfer from uh",
      "timestamp": "04:05"
    },
    {
      "start": 249.28,
      "duration": 3.92,
      "text": "our model api and so we can reduce the",
      "timestamp": "04:09"
    },
    {
      "start": 253.2,
      "duration": 3.92,
      "text": "gpu cost the challenge is um these goals",
      "timestamp": "04:13"
    },
    {
      "start": 257.12,
      "duration": 1.92,
      "text": "often have trade of relationship between",
      "timestamp": "04:17"
    },
    {
      "start": 259.04,
      "duration": 3.92,
      "text": "each other and um this is what i i am",
      "timestamp": "04:19"
    },
    {
      "start": 262.96,
      "duration": 1.76,
      "text": "this is going to be the main topic of",
      "timestamp": "04:22"
    },
    {
      "start": 264.72,
      "duration": 3.36,
      "text": "this session so um friendly ai is a gpu",
      "timestamp": "04:24"
    },
    {
      "start": 268.08,
      "duration": 2.72,
      "text": "platform for acceler accelerated ai and",
      "timestamp": "04:28"
    },
    {
      "start": 270.8,
      "duration": 3.76,
      "text": "our approach for um handling the above",
      "timestamp": "04:30"
    },
    {
      "start": 274.56,
      "duration": 2.32,
      "text": "handling the challenge is to consider",
      "timestamp": "04:34"
    },
    {
      "start": 276.88,
      "duration": 2.4,
      "text": "trade-offs in the context of application",
      "timestamp": "04:36"
    },
    {
      "start": 279.28,
      "duration": 3.919,
      "text": "scenarios so um based on this principle",
      "timestamp": "04:39"
    },
    {
      "start": 283.199,
      "duration": 2.0,
      "text": "um we've built our software and",
      "timestamp": "04:43"
    },
    {
      "start": 285.199,
      "duration": 2.961,
      "text": "infrastructure from scratch and now we",
      "timestamp": "04:45"
    },
    {
      "start": 288.16,
      "duration": 2.24,
      "text": "are growing fast and now we are serving",
      "timestamp": "04:48"
    },
    {
      "start": 290.4,
      "duration": 3.28,
      "text": "hundreds of billions of tokens every day",
      "timestamp": "04:50"
    },
    {
      "start": 293.68,
      "duration": 3.28,
      "text": "um we are trying to make our platform as",
      "timestamp": "04:53"
    },
    {
      "start": 296.96,
      "duration": 2.88,
      "text": "simple as possible and easy to use and",
      "timestamp": "04:56"
    },
    {
      "start": 299.84,
      "duration": 3.12,
      "text": "now so you can run hundreds of thousands",
      "timestamp": "04:59"
    },
    {
      "start": 302.96,
      "duration": 3.04,
      "text": "of geni models in the in the hog hoging",
      "timestamp": "05:02"
    },
    {
      "start": 306.0,
      "duration": 3.6,
      "text": "face hub within just a few clicks so as",
      "timestamp": "05:06"
    },
    {
      "start": 309.6,
      "duration": 2.56,
      "text": "demand for fast and efficient ai",
      "timestamp": "05:09"
    },
    {
      "start": 312.16,
      "duration": 2.879,
      "text": "increases researchers have devised more",
      "timestamp": "05:12"
    },
    {
      "start": 315.039,
      "duration": 2.16,
      "text": "advanced optimization technologies for",
      "timestamp": "05:15"
    },
    {
      "start": 317.199,
      "duration": 3.28,
      "text": "accelerating this kind of uh very",
      "timestamp": "05:17"
    },
    {
      "start": 320.479,
      "duration": 2.081,
      "text": "expensive workload",
      "timestamp": "05:20"
    },
    {
      "start": 322.56,
      "duration": 3.6,
      "text": "so we can't name all of them but i have",
      "timestamp": "05:22"
    },
    {
      "start": 326.16,
      "duration": 2.72,
      "text": "brought a few important um technologies",
      "timestamp": "05:26"
    },
    {
      "start": 328.88,
      "duration": 2.96,
      "text": "here so starting from batching we need",
      "timestamp": "05:28"
    },
    {
      "start": 331.84,
      "duration": 3.6,
      "text": "to uh develop a efficient kernel to uh",
      "timestamp": "05:31"
    },
    {
      "start": 335.44,
      "duration": 1.92,
      "text": "fully harness the full computational",
      "timestamp": "05:35"
    },
    {
      "start": 337.36,
      "duration": 2.88,
      "text": "capacity of gpus we need to choose the",
      "timestamp": "05:37"
    },
    {
      "start": 340.24,
      "duration": 1.679,
      "text": "right quantization method for our",
      "timestamp": "05:40"
    },
    {
      "start": 341.919,
      "duration": 3.201,
      "text": "application we need to employ caching",
      "timestamp": "05:41"
    },
    {
      "start": 345.12,
      "duration": 3.2,
      "text": "and so on so um these different",
      "timestamp": "05:45"
    },
    {
      "start": 348.32,
      "duration": 2.319,
      "text": "optimization techniques um have",
      "timestamp": "05:48"
    },
    {
      "start": 350.639,
      "duration": 2.641,
      "text": "different implications on model quality",
      "timestamp": "05:50"
    },
    {
      "start": 353.28,
      "duration": 3.28,
      "text": "and response time and surplus so um we",
      "timestamp": "05:53"
    },
    {
      "start": 356.56,
      "duration": 3.6,
      "text": "have to uh carefully choose what to use",
      "timestamp": "05:56"
    },
    {
      "start": 360.16,
      "duration": 2.479,
      "text": "but with friendly ai you don't need to",
      "timestamp": "06:00"
    },
    {
      "start": 362.639,
      "duration": 1.921,
      "text": "um worry about these um complex",
      "timestamp": "06:02"
    },
    {
      "start": 364.56,
      "duration": 3.359,
      "text": "optimization spaces because we um we",
      "timestamp": "06:04"
    },
    {
      "start": 367.919,
      "duration": 3.441,
      "text": "provide a good combination of these",
      "timestamp": "06:07"
    },
    {
      "start": 371.36,
      "duration": 3.2,
      "text": "optimization spaces based on our um long",
      "timestamp": "06:11"
    },
    {
      "start": 374.56,
      "duration": 2.56,
      "text": "experience",
      "timestamp": "06:14"
    },
    {
      "start": 377.12,
      "duration": 3.44,
      "text": "um this is possible because most of the",
      "timestamp": "06:17"
    },
    {
      "start": 380.56,
      "duration": 2.4,
      "text": "uh optimization technology share the",
      "timestamp": "06:20"
    },
    {
      "start": 382.96,
      "duration": 2.32,
      "text": "same underlying principle",
      "timestamp": "06:22"
    },
    {
      "start": 385.28,
      "duration": 3.28,
      "text": "um that is the optimizations are mainly",
      "timestamp": "06:25"
    },
    {
      "start": 388.56,
      "duration": 2.88,
      "text": "built for mitigating the same problem",
      "timestamp": "06:28"
    },
    {
      "start": 391.44,
      "duration": 5.12,
      "text": "memory band is bound so um this graph is",
      "timestamp": "06:31"
    },
    {
      "start": 396.56,
      "duration": 2.8,
      "text": "um uh showing the classic roofline model",
      "timestamp": "06:36"
    },
    {
      "start": 399.36,
      "duration": 3.6,
      "text": "for um analyzing uh hardware efficiency",
      "timestamp": "06:39"
    },
    {
      "start": 402.96,
      "duration": 2.64,
      "text": "uh so at the x-axis we have uh",
      "timestamp": "06:42"
    },
    {
      "start": 405.6,
      "duration": 2.719,
      "text": "arithmetic intensity and y-axis we have",
      "timestamp": "06:45"
    },
    {
      "start": 408.319,
      "duration": 3.201,
      "text": "um attainable flops and the green dotted",
      "timestamp": "06:48"
    },
    {
      "start": 411.52,
      "duration": 2.56,
      "text": "line shows a boundary between memory",
      "timestamp": "06:51"
    },
    {
      "start": 414.08,
      "duration": 2.88,
      "text": "vendor bound and compute bound so um the",
      "timestamp": "06:54"
    },
    {
      "start": 416.96,
      "duration": 3.92,
      "text": "situation when you run a uh ji models is",
      "timestamp": "06:56"
    },
    {
      "start": 420.88,
      "duration": 3.439,
      "text": "it's naturally very bend bound and we",
      "timestamp": "07:00"
    },
    {
      "start": 424.319,
      "duration": 2.401,
      "text": "need to apply optimizations to make it",
      "timestamp": "07:04"
    },
    {
      "start": 426.72,
      "duration": 2.4,
      "text": "competit bound so it means that we want",
      "timestamp": "07:06"
    },
    {
      "start": 429.12,
      "duration": 5.519,
      "text": "to um switch from this area to that area",
      "timestamp": "07:09"
    },
    {
      "start": 434.639,
      "duration": 1.921,
      "text": "this is because the nature of llm",
      "timestamp": "07:14"
    },
    {
      "start": 436.56,
      "duration": 2.88,
      "text": "inference llm uses auto reggressive",
      "timestamp": "07:16"
    },
    {
      "start": 439.44,
      "duration": 3.039,
      "text": "decoding and it uh generates one tok at",
      "timestamp": "07:19"
    },
    {
      "start": 442.479,
      "duration": 2.801,
      "text": "a time um so this is very memory vendor",
      "timestamp": "07:22"
    },
    {
      "start": 445.28,
      "duration": 2.0,
      "text": "bound and we need to solve this problem",
      "timestamp": "07:25"
    },
    {
      "start": 447.28,
      "duration": 3.039,
      "text": "by applying various optimizations",
      "timestamp": "07:27"
    },
    {
      "start": 450.319,
      "duration": 3.121,
      "text": "so the most important one is batching",
      "timestamp": "07:30"
    },
    {
      "start": 453.44,
      "duration": 2.72,
      "text": "um computations on gpus incorporate two",
      "timestamp": "07:33"
    },
    {
      "start": 456.16,
      "duration": 2.319,
      "text": "types of input data the first one is",
      "timestamp": "07:36"
    },
    {
      "start": 458.479,
      "duration": 2.241,
      "text": "model weight which is shared across",
      "timestamp": "07:38"
    },
    {
      "start": 460.72,
      "duration": 2.319,
      "text": "multiple requests and the second one is",
      "timestamp": "07:40"
    },
    {
      "start": 463.039,
      "duration": 1.6,
      "text": "activation",
      "timestamp": "07:43"
    },
    {
      "start": 464.639,
      "duration": 3.84,
      "text": "so um if we can group multiple requests",
      "timestamp": "07:44"
    },
    {
      "start": 468.479,
      "duration": 2.72,
      "text": "from different users together and run it",
      "timestamp": "07:48"
    },
    {
      "start": 471.199,
      "duration": 3.12,
      "text": "once run it at once we can reduce the",
      "timestamp": "07:51"
    },
    {
      "start": 474.319,
      "duration": 2.081,
      "text": "amount of data that we that we need to",
      "timestamp": "07:54"
    },
    {
      "start": 476.4,
      "duration": 2.4,
      "text": "read and therefore we can increase",
      "timestamp": "07:56"
    },
    {
      "start": 478.8,
      "duration": 2.799,
      "text": "arithmetic intensity so this enables",
      "timestamp": "07:58"
    },
    {
      "start": 481.599,
      "duration": 1.681,
      "text": "transition from memory bound to compute",
      "timestamp": "08:01"
    },
    {
      "start": 483.28,
      "duration": 2.88,
      "text": "bound reason",
      "timestamp": "08:03"
    },
    {
      "start": 486.16,
      "duration": 3.52,
      "text": "so um uh let me introduce a important",
      "timestamp": "08:06"
    },
    {
      "start": 489.68,
      "duration": 2.079,
      "text": "technique in this area so it is named",
      "timestamp": "08:09"
    },
    {
      "start": 491.759,
      "duration": 2.16,
      "text": "continuous fetching and um it is in",
      "timestamp": "08:11"
    },
    {
      "start": 493.919,
      "duration": 3.12,
      "text": "invented by me at friendly ai and it's",
      "timestamp": "08:13"
    },
    {
      "start": 497.039,
      "duration": 2.241,
      "text": "mainly devised for handling early",
      "timestamp": "08:17"
    },
    {
      "start": 499.28,
      "duration": 1.599,
      "text": "finished and late arrive request",
      "timestamp": "08:19"
    },
    {
      "start": 500.879,
      "duration": 4.401,
      "text": "efficiently um but we cannot conclude",
      "timestamp": "08:20"
    },
    {
      "start": 505.28,
      "duration": 2.8,
      "text": "the story for optimization because it",
      "timestamp": "08:25"
    },
    {
      "start": 508.08,
      "duration": 3.2,
      "text": "often have trade-offs between other uh",
      "timestamp": "08:28"
    },
    {
      "start": 511.28,
      "duration": 2.96,
      "text": "with other optimizations",
      "timestamp": "08:31"
    },
    {
      "start": 514.24,
      "duration": 2.64,
      "text": "one prominent example is batching and",
      "timestamp": "08:34"
    },
    {
      "start": 516.88,
      "duration": 3.12,
      "text": "caching um in the context of jai",
      "timestamp": "08:36"
    },
    {
      "start": 520.0,
      "duration": 2.159,
      "text": "inference caching means reusing tensors",
      "timestamp": "08:40"
    },
    {
      "start": 522.159,
      "duration": 2.721,
      "text": "stored in kb cache and this is mainly",
      "timestamp": "08:42"
    },
    {
      "start": 524.88,
      "duration": 2.639,
      "text": "used for skipping prefer computation for",
      "timestamp": "08:44"
    },
    {
      "start": 527.519,
      "duration": 2.641,
      "text": "the over overlapped part of input tokens",
      "timestamp": "08:47"
    },
    {
      "start": 530.16,
      "duration": 1.76,
      "text": "um we often have overlap in the input",
      "timestamp": "08:50"
    },
    {
      "start": 531.92,
      "duration": 2.16,
      "text": "tokens between requests because um we",
      "timestamp": "08:51"
    },
    {
      "start": 534.08,
      "duration": 2.72,
      "text": "may have a long system prompt or um we",
      "timestamp": "08:54"
    },
    {
      "start": 536.8,
      "duration": 1.68,
      "text": "have shared context between multiple",
      "timestamp": "08:56"
    },
    {
      "start": 538.48,
      "duration": 2.4,
      "text": "requests for example um the shared",
      "timestamp": "08:58"
    },
    {
      "start": 540.88,
      "duration": 3.2,
      "text": "documentation for um different requests",
      "timestamp": "09:00"
    },
    {
      "start": 544.08,
      "duration": 2.319,
      "text": "we also need to handle multimodel data",
      "timestamp": "09:04"
    },
    {
      "start": 546.399,
      "duration": 2.081,
      "text": "as well as text efficiently in the",
      "timestamp": "09:06"
    },
    {
      "start": 548.48,
      "duration": 2.799,
      "text": "caching system and we need off gpu",
      "timestamp": "09:08"
    },
    {
      "start": 551.279,
      "duration": 2.24,
      "text": "variant support and of course in large",
      "timestamp": "09:11"
    },
    {
      "start": 553.519,
      "duration": 2.161,
      "text": "at large scale deployment we need cash",
      "timestamp": "09:13"
    },
    {
      "start": 555.68,
      "duration": 4.04,
      "text": "away routing between replicas",
      "timestamp": "09:15"
    },
    {
      "start": 559.839,
      "duration": 3.281,
      "text": "um the point is caching has a thread of",
      "timestamp": "09:19"
    },
    {
      "start": 563.12,
      "duration": 2.0,
      "text": "relationship between batching just like",
      "timestamp": "09:23"
    },
    {
      "start": 565.12,
      "duration": 4.56,
      "text": "ordinary caching um kach in um recent ai",
      "timestamp": "09:25"
    },
    {
      "start": 569.68,
      "duration": 2.88,
      "text": "systems have two roles the first one is",
      "timestamp": "09:29"
    },
    {
      "start": 572.56,
      "duration": 1.92,
      "text": "scratch pad memory for handling the",
      "timestamp": "09:32"
    },
    {
      "start": 574.48,
      "duration": 2.4,
      "text": "current batch of requests and the second",
      "timestamp": "09:34"
    },
    {
      "start": 576.88,
      "duration": 2.24,
      "text": "one is storing kway tensors for future",
      "timestamp": "09:36"
    },
    {
      "start": 579.12,
      "duration": 2.48,
      "text": "reuse if the working set does not fit",
      "timestamp": "09:39"
    },
    {
      "start": 581.6,
      "duration": 2.64,
      "text": "into the scratch pace the the system",
      "timestamp": "09:41"
    },
    {
      "start": 584.24,
      "duration": 2.719,
      "text": "will slow down significantly so um this",
      "timestamp": "09:44"
    },
    {
      "start": 586.959,
      "duration": 3.121,
      "text": "is why we have a um creative",
      "timestamp": "09:46"
    },
    {
      "start": 590.08,
      "duration": 1.28,
      "text": "relationship between batching and",
      "timestamp": "09:50"
    },
    {
      "start": 591.36,
      "duration": 2.88,
      "text": "caching so when we need to increase the",
      "timestamp": "09:51"
    },
    {
      "start": 594.24,
      "duration": 2.32,
      "text": "batch size we need to carefully think",
      "timestamp": "09:54"
    },
    {
      "start": 596.56,
      "duration": 3.52,
      "text": "about it because it may hinder um cachet",
      "timestamp": "09:56"
    },
    {
      "start": 600.08,
      "duration": 3.439,
      "text": "rate of your application",
      "timestamp": "10:00"
    },
    {
      "start": 603.519,
      "duration": 3.361,
      "text": "uh we also have quantization also have a",
      "timestamp": "10:03"
    },
    {
      "start": 606.88,
      "duration": 2.24,
      "text": "trade-off relationship between the goals",
      "timestamp": "10:06"
    },
    {
      "start": 609.12,
      "duration": 2.56,
      "text": "so quantization may degrade your",
      "timestamp": "10:09"
    },
    {
      "start": 611.68,
      "duration": 2.88,
      "text": "generation quality so when you want to",
      "timestamp": "10:11"
    },
    {
      "start": 614.56,
      "duration": 2.08,
      "text": "use it you have to be careful when",
      "timestamp": "10:14"
    },
    {
      "start": 616.64,
      "duration": 2.48,
      "text": "whether it's good enough for um",
      "timestamp": "10:16"
    },
    {
      "start": 619.12,
      "duration": 3.12,
      "text": "satisfying your end users so um i want",
      "timestamp": "10:19"
    },
    {
      "start": 622.24,
      "duration": 3.279,
      "text": "to highlight at this point so um our uh",
      "timestamp": "10:22"
    },
    {
      "start": 625.519,
      "duration": 2.961,
      "text": "we have our own fp8 quantization method",
      "timestamp": "10:25"
    },
    {
      "start": 628.48,
      "duration": 2.479,
      "text": "and it pre preserves the model quality",
      "timestamp": "10:28"
    },
    {
      "start": 630.959,
      "duration": 3.521,
      "text": "while speeding up the execution",
      "timestamp": "10:30"
    },
    {
      "start": 634.48,
      "duration": 3.039,
      "text": "so um based on the techniques that we've",
      "timestamp": "10:34"
    },
    {
      "start": 637.519,
      "duration": 3.681,
      "text": "built and researched we have a product",
      "timestamp": "10:37"
    },
    {
      "start": 641.2,
      "duration": 3.72,
      "text": "named friendly suite",
      "timestamp": "10:41"
    },
    {
      "start": 645.36,
      "duration": 3.44,
      "text": "uh friendly suite is composed of mainly",
      "timestamp": "10:45"
    },
    {
      "start": 648.8,
      "duration": 2.08,
      "text": "composed of two components so first we",
      "timestamp": "10:48"
    },
    {
      "start": 650.88,
      "duration": 1.92,
      "text": "have container microser at the bottom",
      "timestamp": "10:50"
    },
    {
      "start": 652.8,
      "duration": 2.159,
      "text": "level and on top of that we've built",
      "timestamp": "10:52"
    },
    {
      "start": 654.959,
      "duration": 3.281,
      "text": "fully managed cloud service um friendly",
      "timestamp": "10:54"
    },
    {
      "start": 658.24,
      "duration": 3.52,
      "text": "container is uh is a you can think it as",
      "timestamp": "10:58"
    },
    {
      "start": 661.76,
      "duration": 2.079,
      "text": "a simple taker image that you can pull",
      "timestamp": "11:01"
    },
    {
      "start": 663.839,
      "duration": 3.201,
      "text": "from our image registry and deploy it on",
      "timestamp": "11:03"
    },
    {
      "start": 667.04,
      "duration": 4.32,
      "text": "your kubernetes cluster we provide um we",
      "timestamp": "11:07"
    },
    {
      "start": 671.36,
      "duration": 1.919,
      "text": "we provide a good combination of",
      "timestamp": "11:11"
    },
    {
      "start": 673.279,
      "duration": 4.081,
      "text": "optimization technologies and so we so",
      "timestamp": "11:13"
    },
    {
      "start": 677.36,
      "duration": 2.719,
      "text": "we can deliver gpu cost reduction and",
      "timestamp": "11:17"
    },
    {
      "start": 680.079,
      "duration": 2.32,
      "text": "fast response",
      "timestamp": "11:20"
    },
    {
      "start": 682.399,
      "duration": 2.641,
      "text": "um for the fully managed service part i",
      "timestamp": "11:22"
    },
    {
      "start": 685.04,
      "duration": 1.84,
      "text": "want to talk about friendly dedicated",
      "timestamp": "11:25"
    },
    {
      "start": 686.88,
      "duration": 1.519,
      "text": "endpoints",
      "timestamp": "11:26"
    },
    {
      "start": 688.399,
      "duration": 2.481,
      "text": "it's a fully managed service running on",
      "timestamp": "11:28"
    },
    {
      "start": 690.88,
      "duration": 3.44,
      "text": "our um cloud infrastructure and you can",
      "timestamp": "11:30"
    },
    {
      "start": 694.32,
      "duration": 2.4,
      "text": "easily um pick up a model from hoging",
      "timestamp": "11:34"
    },
    {
      "start": 696.72,
      "duration": 2.799,
      "text": "face hub and deploy the model within a",
      "timestamp": "11:36"
    },
    {
      "start": 699.519,
      "duration": 2.961,
      "text": "few clicks of course we provide other",
      "timestamp": "11:39"
    },
    {
      "start": 702.48,
      "duration": 2.08,
      "text": "functionalities like uh high",
      "timestamp": "11:42"
    },
    {
      "start": 704.56,
      "duration": 2.88,
      "text": "availability autoscaling depending on",
      "timestamp": "11:44"
    },
    {
      "start": 707.44,
      "duration": 3.36,
      "text": "your um incoming traffic and intelligent",
      "timestamp": "11:47"
    },
    {
      "start": 710.8,
      "duration": 3.88,
      "text": "routing between replicas",
      "timestamp": "11:50"
    },
    {
      "start": 714.959,
      "duration": 4.241,
      "text": "um i will show you a demo video",
      "timestamp": "11:54"
    },
    {
      "start": 719.2,
      "duration": 2.0,
      "text": "now you can easily deploy models",
      "timestamp": "11:59"
    },
    {
      "start": 721.2,
      "duration": 2.0,
      "text": "directly from hugging face hub with",
      "timestamp": "12:01"
    },
    {
      "start": 723.2,
      "duration": 3.48,
      "text": "friendly endpoints",
      "timestamp": "12:03"
    },
    {
      "start": 726.72,
      "duration": 1.84,
      "text": "just go to deploy options on hugging",
      "timestamp": "12:06"
    },
    {
      "start": 728.56,
      "duration": 1.68,
      "text": "face model page and select friendly",
      "timestamp": "12:08"
    },
    {
      "start": 730.24,
      "duration": 4.24,
      "text": "endpoints it's that simple",
      "timestamp": "12:10"
    },
    {
      "start": 734.48,
      "duration": 1.599,
      "text": "you can deploy a model with just one",
      "timestamp": "12:14"
    },
    {
      "start": 736.079,
      "duration": 3.0,
      "text": "click",
      "timestamp": "12:16"
    },
    {
      "start": 739.68,
      "duration": 3.76,
      "text": "[Music]",
      "timestamp": "12:19"
    },
    {
      "start": 743.44,
      "duration": 2.16,
      "text": "once the deployment is complete head",
      "timestamp": "12:23"
    },
    {
      "start": 745.6,
      "duration": 1.6,
      "text": "over to the friendly suite to access",
      "timestamp": "12:25"
    },
    {
      "start": 747.2,
      "duration": 3.84,
      "text": "even more powerful features",
      "timestamp": "12:27"
    },
    {
      "start": 751.04,
      "duration": 1.599,
      "text": "now you can enjoy the fastest and",
      "timestamp": "12:31"
    },
    {
      "start": 752.639,
      "duration": 1.681,
      "text": "cost-effective inference with friendly",
      "timestamp": "12:32"
    },
    {
      "start": 754.32,
      "duration": 3.85,
      "text": "ai's gpu optimized inference engine",
      "timestamp": "12:34"
    },
    {
      "start": 758.17,
      "duration": 4.27,
      "text": "[Music]",
      "timestamp": "12:38"
    },
    {
      "start": 762.56,
      "duration": 3.6,
      "text": "yeah um before ending my session um i",
      "timestamp": "12:42"
    },
    {
      "start": 766.16,
      "duration": 4.16,
      "text": "want to uh share a news um we in uh this",
      "timestamp": "12:46"
    },
    {
      "start": 770.32,
      "duration": 3.28,
      "text": "week we've launched a promotion program",
      "timestamp": "12:50"
    },
    {
      "start": 773.6,
      "duration": 4.16,
      "text": "uh so we uh give a free 10k dollar",
      "timestamp": "12:53"
    },
    {
      "start": 777.76,
      "duration": 4.72,
      "text": "credits so um uh so if you're interested",
      "timestamp": "12:57"
    },
    {
      "start": 782.48,
      "duration": 2.32,
      "text": "please um check out the qr code here and",
      "timestamp": "13:02"
    },
    {
      "start": 784.8,
      "duration": 2.08,
      "text": "apply in two minutes and we are also",
      "timestamp": "13:04"
    },
    {
      "start": 786.88,
      "duration": 2.639,
      "text": "running a booth over there so um please",
      "timestamp": "13:06"
    },
    {
      "start": 789.519,
      "duration": 3.76,
      "text": "feel free to come to our booth and uh",
      "timestamp": "13:09"
    },
    {
      "start": 793.279,
      "duration": 2.721,
      "text": "have let's so let's have let's talk",
      "timestamp": "13:13"
    },
    {
      "start": 796.0,
      "duration": 3.24,
      "text": "thank you",
      "timestamp": "13:16"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.371689",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}