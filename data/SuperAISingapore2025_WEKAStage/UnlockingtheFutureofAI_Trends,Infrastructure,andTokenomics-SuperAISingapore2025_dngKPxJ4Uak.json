{
  "video_id": "dngKPxJ4Uak",
  "video_title": "Unlocking the Future of AI: Trends, Infrastructure, and Tokenomics - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=dngKPxJ4Uak",
  "channel_title": "SuperAI",
  "published_at": "2025-06-25T15:39:23+00:00",
  "duration_seconds": null,
  "view_count": 27,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nFireside Chat: Unlocking the Future of AI: Trends, Infrastructure, and Tokenomics\n\nSpeakers:\nValentin Bercovici, Chief AI Officer @ WEKA\nDan Nishball, Director of Research @ SemiAnalysis\n\nStage: WEKA Stage\n#superai #byteplus #ai #business #enterprise \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 771,
    "aggregated_text": "dan and I here are here to answer the questions that are already you know top of our minds that we're obsessed about every day which is where does your $20 a month go if you're a Chat GPT user where does your $200 or $2,000 a month go if you're an API developer an LLMbased developer the answer is not obvious dan and I are here to actually dive into the details of that and help you understand how to overcome some of the the limits that perhaps might be imposed on your day-to-day work so for context 2025 is already an amazing year in the AI space and the LLM and LRM large reasoning model space because in only six months now we've gone from a world where we had mostly language models that were foundation models and didn't do thinking or reasoning and today anytime a new model is introduced it's like you have to explain that it's not a reasoning model so the the pendulum if you will is really shifted and we'll see whether it even shifts back or not and maybe the most important thing to open this conversation with is the trend line going from top to bottom of the declining token costs declining cost of inference and I've seen numbers all over the map here dan you're the expert generally what have we seen over the past year or two well we've actually seen an explosion inference demand we've seen uh rental prices at Neocloud stay quite supported um when we actually saw deepseek release many inference providers saw a doubling of their tokens quarteron quarter and another 50% growth after that so it's been a lot of angst you know especially among the investor community well gee when price of tokens goes down does that mean I need less GPUs and the answer is absolutely not it means more more GPUs the demand for intelligence is insatiable so reading the semi- analysis reports is always fun because when the deepseeek moment happened and this is obviously deepseek i1 was not their first release They've been releasing amazing models months before that a year before that but when that NASDAQ Monday happened and $1 trillion dollars disappeared from the stock market everyone was thinking \"Oh my god it's the end of GPUs it's the end of Nvidia and the bubble has burst.\" I think I saw a lot of those headlines and if you read the semi- analysis tweets and the report like within the next day or the week what they showed you is that Jeban's paradox happens right so we'll see if the the thing works it does and when you lower the cost of a valuable commodity demand surges and the two best recent examples of that but go ahead if you had a comment actually well no I was going to say that you know the magnitude is stunning it's a 100 times drop in price right in just two years right so if you were selling GPUs you know you might freak out a little bit but that's not what we've seen right demand just kept going up best example is cursor right so follow the money kursa went from zero to $200 million in annual recurring revenue in less than two years it is by all opinions now the first true killer app of AI the first paid killer app of AI and it's horizontal it's across every industry vertical and just like 15 years ago when you had wanted to measure where cloud was in the enterprise it was stealth IT developers with their credit cards guess what's happening all over again if you want to measure the true adoption at your company at your government department of AI find the cursor bills because it's developers today right now whether you know it or not that is signing up and using cursor to write code in your organization i think the more interesting example though is the ARC prize and the ARC AGI benchmark because what they're showing is that you can now manufacture intelligence depending on the number of tokens you generate if you generate more tokens more quality tokens you get a higher quality output your manis agent your other agents can do more intelligent work more productive work but it comes at a cost so the cost in the ARC AGI1 benchmark was about 16 hours one six hours of processing time at over a million dollars of token costs that is proving you can do a lot but certainly not cost- effectively and ARGI 2 now is a benchmark that introduces efficiency token efficiency which we agree is just a much better way to measure the productivity of models because I don't know about you but I don't have a million dollars to throw at my next AI problem so the situation we find ourselves in right now is whether you're a small or large AI company your GPUs are melting you don't have enough capacity despite the plunging cost of inference you still never have enough capacity to serve all the demand for your users so this is a problem that you know the industry is solving in many many ways one of the things they're doing is saying hey you're just going to have to pay more so there's already three classes of pricing for every AI model and particularly in the developer the API version this is very visible to developers it's very visible to cursor and and ISVS that develop on LLMs you've got your input token cost and you've got your output token cost and the most interesting one of course is in the middle the cached input token cost now that now there's a pretty universal consensus between open AI that's joined the price war with Google that had to join the price war with Deepseek do you want to explain what is a cached input token actually so a great question Dan a cached input token essentially says that if you're reusing the same prompt over and over again we're not going to charge you to process that same prompt over again because we have it cached we have a a tokenized or vectorized version of that prompt in memory something called a key value cache or KV cache we already have that in memory so we're not going to charge you we're going to give you a 75% discount actually for processing that same token again because we reserve about 25% for overhead still but it's a much more cost-effective way to really create an AI app and every single AI app Cursor Windsurf client Ader you name it manis AI they all use cacheed input pricing and cacheed input processing first however you notice there's a catch at the bottom you can only get anywhere and it's not guaranteed but between five and 15 minutes of cash so if you go for a coffee break or if your agent is so busy that they're spinning up many many subtasks and it takes longer than 15 minutes like even a deep research project a long reasoning prompt takes longer than that to process boom you go back to the full price no discount and that radically impacts the economics and and usability utility we're going to talk about that dan's going to dive deep into that and one of the ways to really understand a construct a framework for how we think about the costs are the trade-offs we have to make in this triad between accuracy and cost you don't get the best quality answer if you don't pay for it and latency really really matters you know if you get an answer an hour from now when you're doing real-time voice translation that's not useful at all in many use cases you either need an immediate answer or if you have an agentic use case a lot of parallel tasks you need rapid response low latency how do we interpret this graph right because we got latency squeezing what does that mean for cost and what does it mean for accuracy when you when you're at that squeezing point of the the diagram yeah this is a great question because this is a fun thing to look at the animation but actually what's happening here means a lot what you want always is lower latency right when you're doing real-time voice translation there's just no pausing no awkwardness you just want instant low latency as you do that as you pinch this triad this triangle when you do it right when you configure your models correctly when you use good infrastructure in your data center you're driving up the accuracy so you see sort of the slope of of pretty intense accuracy increase which everybody wants and and and um you're driving down at the slope an aggressive reduction in overall costs because quite simply you can process more tokens per second everyone has a budget everyone has a fixed GPU budget a fixed power budget and so the more cycles you know the more processing per cycle that you have the more tokens you get and Dan's actually going to do a great job right now of really explaining how that directly translates to profitability and how it translates to whether you get a rate limit or not and whether you get all the tokens you want so one last of the many many lessons DeepSeek taught us and the reason we love DeepSseek is they're incredibly transparent they're not just an open- source model with open weights um the way a lot of open models are but they actually disclose on GitHub a lot of how they operate their inference infrastructure no other lab to my knowledge does that the closed labs obviously don't that's a proprietary trade secret but even the open labs even meta you know even Quen etc they don't disclose actually how they operate their inference and we learn so much all the other labs the big labs as well as the open labs learned so much from this so one of the key lessons is elasticity a best practice up until DeepSeek has been two clusters you separate your training cluster so be on you know this side if you will from your inference cluster and you have a big firewall in between uh organizational firewall and network firewall and you never have researchers mess with inference and you never have inference mess with research deepseek said we have export controls restrictive export controls on us we are not allowed to use the latest generation of processors that you know very well documented by semi analysis we have memory bandwidth constraints so we don't have the luxury of having separate clusters for training and inference we're going to write white papers combine our research and inference departments and operate our entire business on the same cluster which is revolutionary but it leads to enormous efficiencies and it leads to the price war that we're all enjoying from it it's a big reason why the cost of inference has plummeted and so what DeepS seek does is they infer a lot during daylight hours and then they elastically train overnight and then they elastically ramp up and do inference again when when China in this case wakes up and that is much more complicated to execute than to actually say because we're dealing with giant model weight files here we're dealing with giant clusters here so it's not easy at all to turn these things on and off so one of the last things uh that we'll I'll I'll uh wrap up with before we hand this over to Dan for a bit is this really important announcement just last week from Nvidia they had an acquisition of Leptin was it Leptin AI they acquired yeah Leptton that was Lepon leptton and so uh with Leptton they're creating a consistent developer experience for all users across any cloud but also this is where I think the tokconomics are going to really factor in a open marketplace for GPU hours an open marketplace for inference effectively and the fact that AWS NBUS Cororeweave Lambda Labs and other hypers scale clouds and Neoclouds GPU clouds have already signed up to this means it's not just a wish or a desire on Nvidia's part this is real it's going to create a hyperco competitive marketplace and put a cute focus on the tokconomics it's a little like you know Yuber right there used to be so many different taxi companies and if you want a consistent experience go to Uber exactly uh so it's really homogenizing the whole thing so I'll I'll talk through five metrics that matter and then we'll go into the full stack so token throughput which is when you start asking Chap a question and you get the response you get token by token and how long does it take for all those tokens to come out like what's the time between tokens typically 20 was the the lower limit and 50 to 80 is is is about normal now time the first token is when you put in your question and when do you start getting the response and we add the two together we've get end to end latency which just adds a time the first token and then all the tokens together you know 20 to 30 seconds could be typical um cost per token um obvious one and then context window which is the maximum amount of tokens you can put into a model or a typical it could be 100,000 in the case of cursor it could be 4,000 in the case of summarization and what we'll see is there's really no typical case like you know no oneizefits-all solutions plama 4 is up to a million now so oh so a typical I think uh for the investors out there bond perspectus would be about like a 100,000 tokens you know 200 100 pages so I'll take you through this right and I think the upshot is you all are familiar with $20 a month for chat GBTN those in the uh I don't know you can so the 33,000 up top right that's the capital cost per one H100 GPU and how do we get from the capital cost of a GPU all the way to $20 which is right down here which is what we're all paying and that's what we do here at semi analysis right we have nearly 40 folks uh around the world and we have a few practice areas and to really understand the full stack you have to understand each layer and I'll talk you through that the first layer is neoclouds right and neoclouds purchase GPUs they turn capital expenditure into dollars per hour what are the inputs well capital cost of capital collocation energy um and you know there's also a margin right so how do we analyze this we have to understand IC design we have to understand AI data centers and we have to understand AI neocloud so those are three practice groups now let's look at the next layer of the stack right so it's inference providers this could be your cloud your open AI what they do is they rent capacity off the neoclouds at $2 an hour and they sell tokens but how do we get from dollars per hour on a on a GPU to dollars per token right well I think It's obvious right you know how much tokens can we get off of these right and so this is where our next practice area comes in you see that 723 right how do we get that 723 and the way we do is AI engineering we have a team of three AI engineers we typically have five to eight AI servers at any given time and we're just running a lot of benchmarks you know we work a lot with Val and his team to think about what are some use cases and and how would they perform and how if we you know kind of do creative things you know what can we get out of this so that's how we get in 723 if you do the math the cost is going to be 150 right and what goes on top think of cursor cursor will purchase inference off of Claude Gemini or OpenAI and they'll sell it as an Arboo product right so a typical user might consume six million tokens a month at 150 per million token um their profit margin is about $11 and this is how we look at the world you can't understand the whole factory without understanding each part of the stack this is the capital cost just to show you the assumptions 13% cost of capital for your useful life the chip will last um the chip will last six years maybe what we like to say you want to make your economic useful life for this is all available in selling analysis.com we got tons of tables so okay and then this is just to show the upshot is a 76% capital cost so very capital intensive um so Neoclouds let's talk about Neoclouds this is the the trend and one of the big worries about Neoclouds is well how quickly um you know will the rental price go down right and with the furious space in which Nvidia is advancing a lot of a lot of them ask like why should I ever buy GPUs because they'll be superseded pretty soon um but you know if you're doing the right thing doing term contracts you'll be okay why are NeoCloud so important you see OpenAI as a perfect example they're out of Microsoft capacity they went to Oracle they're out of Oracle capacity they go to Corewave and NeoCloud for more capacity now they're even going to Google for extra GPU capacity there just quite frankly is not enough capacity in the world for the demand for all these you know apps like cursor and others and that's why NeoCloud economics are vital and supply you can analyze demand is hard right but we have price right so if you have two you can figure out demand right and we'll show you some examples but first let me take you through an illustrative example right so it's a bit like the analogy I like to use is it's a bus and a private jet right we've got this inference system if we have two users it's going to be fast right look at this 141 tokens per second per user that's uh fast it's cheetah fast as you'll see in the next slide um and it's like riding a private jet now if we put 128 users right um then we get it slows down right a bus has to make many stops right and um as a result 18 tokens per second it's slow right but if you look at this column total tokens per second we see the throughput is high right so we're sweating our capacity really effectively um so what does this mean right this means that in order at this fixed selling price in order to make the economics work you have to be operating as if you're a bus right and if you want to run a private jet well you need to change the price of tokens right so I think the big animal takeaway here is some people prefer the the bus some people prefer the jet if you want cheetah inference well well if you want tortoise inference that's the bus and if you want cheetah inference that's the private jet um and let's look at how we analyze this okay so in this graph up here this graphs on the y-axis throughput per GPU how many tokens we're getting per per GPU right and on the uh x-axis this is latency remember how long it takes from when you first ask a question to when it completes giving you the answer and let's overlay an analogy here right just so you all don't get lost um the bus is up here so the bus is up here long latency it takes a long time to get everything uh there but we're running a lot of users right and that is why your throughput is high private jet few users very fast but we don't get a lot right now what does this mean for cost per token right and the way we get this right is we actually you know have run I guess it's hundreds of benchmarks you know we took about a few months to do this um I mean we ran on all these servers what's interesting is by the way this is both a benchmark of the hardware and the software because what you're seeing at the top there are different you know labels for the different color bars the TRT means it's not the open-source VLM Python based inference server it's actually Nvidia's faster often faster TRTLM C-based inference server so that's why you're seeing some differences in the performance the latency and so forth yep so everything else is VLM and that's tensor RT which which runs really well in terms of throughput um so again if we use our analogy right remember the private jet right um we actually had very fast you know response very low latency but we had very low throughput what does that mean right you're paying a fix remember you're paying a fixed $2 an hour right if you generate less tokens the cost per token goes up obviously flying a private jet's more expensive than flying coach um but if you ride coach then you're out here um but here's the point right you know as Val said it's it's a it's a triad there are trade-offs and there's no one-sizefits-all some people like to fly in the private jet some people like to take the bus right concretely you know some people like to do 1,000 tokens some people want to do like 100,000 tokens there's all these different you know use cases and what NVIDIA showed here in their GTC is all the different trade-off points you can have uh and here's our private jet here's our bus private jet this here is tokens per second so this side means more interactive tokens are coming faster and then on this side is slower right but we get more throughput um so let's walk through a few examples right let's say we want to have the private jet right what does that mean that means obviously fewer tokens what does that do to economics well inference compute cost same $2 but now we've got 7.48 for a million tokens because we're generating fewer inference provider gross margin minus 66% uh oh yeah uh and then if the uh if the upstream the the provider um the application providers adjust prices it's still a bit tight right um here's your bus private jet um let's talk about another angle we talked a lot about the trade-off between interactivity right having faster tokens what about more tokens what if you want instead of putting you know a summary of a website you want to put in a summary of like five perspectuses you want to put in your entire codebase what does that mean right as your context length increases you put in more that means your memory requirements sore and what happens well cheetah inference with only a little bit you're putting in you put a perspectus in you put your codebase and it's a tortoise inference unfortunately right but you know we want to be putting code bases we want to ingest multiple practices multiple earning calls you know all this health data like electronic health records right yeah just you know in the next stage over if you saw the manis keynote towo and cursor that is this workload it's all long context it's all multi-turn multiple questions hundreds of thousands of tokens right we used to think about you know maybe summarize you know it used to be um you know for information the the input output ratio is typically three input to one so people are sending a lot of information for summarization analysis right and we're going you know even higher thousand 10,000 to one and hundreds of thousands of tokens right so As it stands today we have our bus our private jet but if we want long context it's um going to be expensive so they're going to be a launch vehicle practically um and you can see the economics right so you know what do we do right well Deepseek figured this out right and they're able to create a lot of optimizations to get us uh on this curve and this is the curve I was mentioning to Valund times lower in two years and um you know this will continue right we'll keep marching down the various optimizations this will hold true and as I mentioned to Val earlier and you know um Jevon's paradoxes at work when price goes down empirically demand goes up so what this chart shows here is this is when deepseek v3 came out and what happened in the next month prices shot up as everyone rushed to implement deepseeek it's one of the most popular inference providers i already mentioned double quarter and quarter and another 50% um in the in the the second quarter and this is what happened insatiable demand for inference and price have stabilized on the NeoCloud but they're still going to you know gradually decline as the new systems come out um and um you know I'll turn over to Val to talk a little bit about how we keep marching lower in cost per token there's always a set of trade-offs to make but you can certainly make better trade-offs right you you can improve a situation the technology behind what I showed earlier on that 5 to 15 minutes of cache is actually well illustrated by Nvidia in a recent blog post when they launched their project Dynamo but it shows that we have GPUs in the old days over on the left trying to do too much trying to have different kinds of performance some that stresses the processing power of the GPU and then quickly after you do this thing called prefill you have to decode where you actually generate the reasoning tokens and especially the output tokens that's very memory bandwidth oriented it's not as processing heavy and having the same GPUs do that work simultaneously it's very inefficient deepse seek did away with that and one of the innovations the open innovations that everyone is adopting right now is something called disagregated a lot of lot of syllables there so we shorten it to disag prefill and that's separating the workload so you can have a cluster of GPUs that focus just on the comput intensive part so Nvidia for example would like that to be blackwell and then you have a different class of GPUs on the same network that are focused on the decode part that's more memory bandwidth intensive and that can be older prior generations of GPUs and as you do your depreciation and you figure out your overall cost of capital uh and you figure out how you want to optimize at scale you now can combine these two within the same inference workload and really get each one to optimize what they're good for but you still have that really troublesome five to 15 minute window bottleneck and what that means when you run out is you have to go back and burn 10,000 watts per user every 15 minutes or more just to go back to prefill so this is incredibly inefficient and the best way to think of it is that inference today hasn't had its model T moment yet inference today still doesn't use assembly lines in these multi10 billion dollar AI factories which is crazy so a year ago DeepSeek released pricing based on this technology of theirs that's open now and about three months ago they finally finally published the white paper behind it but you see at the bottom between that compute intensive prefill on the left and that memory intensive decode on the right you see this assembly line this key value cache that for performance reasons DeepS was able to migrate down to storage so now we revisit the triad how can we improve this with faster processors with more memory bandwidth with more processing power with more demands GPUs are melting agentic applications how do we improve upon this well we use a formula so DeepSc I think was the first to really document this formula XPYD and now we see the open-source community adopting it really really widely right now so the VLM community and Nvidia software developers are all adopting it and XBYD is as simple as count the number of prefills that's your X count the number of decodes that's your Y and in a perfect world you want to get as close to just one prefill as possible because once you've got the key values for your tokens in theory you shouldn't have to recomputee them and if you break through this memory barrier then you're able to actually get to that ideal world of one prefill for an infinite amount of decodes and we're we're down to a hierarchy here that uh is an interesting hierarchy because I like to say it's a little bit deceiving in order to get the performance that you want particularly if you're not using older processors or or D-tuned processors like Deep Seekers is forced to use but modern ones you need to be at that memory class of performance you need to be in those top two tiers of this pyramid so to run your your private jet or your bus right you need to have it the infrastructure right your airport your bus station you know everything you need up there right yeah and we should have drawn it but you actually want you know certainly the private jet at the top but the bus doesn't go to the bottom here the bus with modern processors goes actually at the host memory layer and if you go to your Amazon cart and you want to buy memory dim and you want to buy a enterprise class NVME drive you'll see that there's about a 20 times price difference the markets already value these things differently so with getting this right getting assembly lines implemented in inference is all about is improving and making better tradeoffs you get new performance frontiers and the cool thing is you go from you know a traditional sweet spot on the Hopper processors a traditional or a new sweet spot on the Blackwell processors you're able now to get your old processors to perform like the modern ones just through software inference software i think it's important with this chart Val right because remember I I I said it was a trade-off right but this is a shift in the curve right so it's still the bus but we sort of just put rocket engines on the bus right and shift the entire curve out right you can do more with less and that's what you know a lot of the work that a lot of these folks are doing with Dynamo and uh AB cache offload exactly semi analysis is actually going to be benchmarking this you know at an upcoming report in the future so it's not just a vendor claiming it it's objective benchmarking that's going to prove new new graphs for you soon hopefully exactly and so what this really means for the industry it's profound we go from three classes of pricing collapse an entire major class of pricing in the industry and down to just two no more need for separate input token costs all input processing is now cash processing depending on whether you want it for weeks or months we don't think people or apps or developers will want more than that but it's just input and output now no need for separate cash input pricing so if you want to find out more feel free to come and approach us and ask questions and I'll get out of your way so you can take a picture of this QR code and we can get you more detailed information but thank you very much for your time",
    "text_length": 29360,
    "word_count": 5508
  },
  "segments": [
    {
      "start": 7.44,
      "duration": 2.32,
      "text": "dan and I here are here to answer the",
      "timestamp": "00:07"
    },
    {
      "start": 9.76,
      "duration": 1.44,
      "text": "questions that are already you know top",
      "timestamp": "00:09"
    },
    {
      "start": 11.2,
      "duration": 2.0,
      "text": "of our minds that we're obsessed about",
      "timestamp": "00:11"
    },
    {
      "start": 13.2,
      "duration": 3.12,
      "text": "every day which is where does your $20 a",
      "timestamp": "00:13"
    },
    {
      "start": 16.32,
      "duration": 3.36,
      "text": "month go if you're a Chat GPT user where",
      "timestamp": "00:16"
    },
    {
      "start": 19.68,
      "duration": 3.2,
      "text": "does your $200 or $2,000 a month go if",
      "timestamp": "00:19"
    },
    {
      "start": 22.88,
      "duration": 2.319,
      "text": "you're an API developer an LLMbased",
      "timestamp": "00:22"
    },
    {
      "start": 25.199,
      "duration": 2.481,
      "text": "developer the answer is not obvious dan",
      "timestamp": "00:25"
    },
    {
      "start": 27.68,
      "duration": 1.759,
      "text": "and I are here to actually dive into the",
      "timestamp": "00:27"
    },
    {
      "start": 29.439,
      "duration": 1.92,
      "text": "details of that and help you understand",
      "timestamp": "00:29"
    },
    {
      "start": 31.359,
      "duration": 2.321,
      "text": "how to overcome some of the the limits",
      "timestamp": "00:31"
    },
    {
      "start": 33.68,
      "duration": 1.44,
      "text": "that perhaps might be imposed on your",
      "timestamp": "00:33"
    },
    {
      "start": 35.12,
      "duration": 4.48,
      "text": "day-to-day work so for context 2025 is",
      "timestamp": "00:35"
    },
    {
      "start": 39.6,
      "duration": 2.08,
      "text": "already an amazing year in the AI space",
      "timestamp": "00:39"
    },
    {
      "start": 41.68,
      "duration": 3.6,
      "text": "and the LLM and LRM large reasoning",
      "timestamp": "00:41"
    },
    {
      "start": 45.28,
      "duration": 3.04,
      "text": "model space because in only six months",
      "timestamp": "00:45"
    },
    {
      "start": 48.32,
      "duration": 2.079,
      "text": "now we've gone from a world where we had",
      "timestamp": "00:48"
    },
    {
      "start": 50.399,
      "duration": 2.32,
      "text": "mostly language models that were",
      "timestamp": "00:50"
    },
    {
      "start": 52.719,
      "duration": 1.761,
      "text": "foundation models and didn't do thinking",
      "timestamp": "00:52"
    },
    {
      "start": 54.48,
      "duration": 2.64,
      "text": "or reasoning and today anytime a new",
      "timestamp": "00:54"
    },
    {
      "start": 57.12,
      "duration": 1.84,
      "text": "model is introduced it's like you have",
      "timestamp": "00:57"
    },
    {
      "start": 58.96,
      "duration": 1.439,
      "text": "to explain that it's not a reasoning",
      "timestamp": "00:58"
    },
    {
      "start": 60.399,
      "duration": 2.8,
      "text": "model so the the pendulum if you will is",
      "timestamp": "01:00"
    },
    {
      "start": 63.199,
      "duration": 1.36,
      "text": "really shifted and we'll see whether it",
      "timestamp": "01:03"
    },
    {
      "start": 64.559,
      "duration": 2.481,
      "text": "even shifts back or not and maybe the",
      "timestamp": "01:04"
    },
    {
      "start": 67.04,
      "duration": 2.16,
      "text": "most important thing to open this",
      "timestamp": "01:07"
    },
    {
      "start": 69.2,
      "duration": 3.44,
      "text": "conversation with is the trend line",
      "timestamp": "01:09"
    },
    {
      "start": 72.64,
      "duration": 1.76,
      "text": "going from top to bottom of the",
      "timestamp": "01:12"
    },
    {
      "start": 74.4,
      "duration": 4.0,
      "text": "declining token costs declining cost of",
      "timestamp": "01:14"
    },
    {
      "start": 78.4,
      "duration": 1.759,
      "text": "inference and I've seen numbers all over",
      "timestamp": "01:18"
    },
    {
      "start": 80.159,
      "duration": 1.921,
      "text": "the map here dan you're the expert",
      "timestamp": "01:20"
    },
    {
      "start": 82.08,
      "duration": 1.28,
      "text": "generally what have we seen over the",
      "timestamp": "01:22"
    },
    {
      "start": 83.36,
      "duration": 1.52,
      "text": "past year or two\n well we've actually",
      "timestamp": "01:23"
    },
    {
      "start": 84.88,
      "duration": 2.08,
      "text": "seen an explosion inference demand we've",
      "timestamp": "01:24"
    },
    {
      "start": 86.96,
      "duration": 2.479,
      "text": "seen uh rental prices at Neocloud stay",
      "timestamp": "01:26"
    },
    {
      "start": 89.439,
      "duration": 2.32,
      "text": "quite supported um when we actually saw",
      "timestamp": "01:29"
    },
    {
      "start": 91.759,
      "duration": 1.36,
      "text": "deepseek release many inference",
      "timestamp": "01:31"
    },
    {
      "start": 93.119,
      "duration": 2.32,
      "text": "providers saw a doubling of their tokens",
      "timestamp": "01:33"
    },
    {
      "start": 95.439,
      "duration": 2.561,
      "text": "quarteron quarter and another 50% growth",
      "timestamp": "01:35"
    },
    {
      "start": 98.0,
      "duration": 2.24,
      "text": "after that so it's been a lot of angst",
      "timestamp": "01:38"
    },
    {
      "start": 100.24,
      "duration": 1.36,
      "text": "you know especially among the investor",
      "timestamp": "01:40"
    },
    {
      "start": 101.6,
      "duration": 2.24,
      "text": "community well gee when price of tokens",
      "timestamp": "01:41"
    },
    {
      "start": 103.84,
      "duration": 1.279,
      "text": "goes down does that mean I need less",
      "timestamp": "01:43"
    },
    {
      "start": 105.119,
      "duration": 2.32,
      "text": "GPUs and the answer is absolutely not it",
      "timestamp": "01:45"
    },
    {
      "start": 107.439,
      "duration": 2.161,
      "text": "means more more GPUs the demand for",
      "timestamp": "01:47"
    },
    {
      "start": 109.6,
      "duration": 2.64,
      "text": "intelligence is insatiable so reading",
      "timestamp": "01:49"
    },
    {
      "start": 112.24,
      "duration": 2.0,
      "text": "the semi- analysis reports is always fun",
      "timestamp": "01:52"
    },
    {
      "start": 114.24,
      "duration": 2.159,
      "text": "because when the deepseeek moment",
      "timestamp": "01:54"
    },
    {
      "start": 116.399,
      "duration": 1.76,
      "text": "happened and this is obviously deepseek",
      "timestamp": "01:56"
    },
    {
      "start": 118.159,
      "duration": 2.081,
      "text": "i1 was not their first release They've",
      "timestamp": "01:58"
    },
    {
      "start": 120.24,
      "duration": 1.6,
      "text": "been releasing amazing models months",
      "timestamp": "02:00"
    },
    {
      "start": 121.84,
      "duration": 1.919,
      "text": "before that a year before that but when",
      "timestamp": "02:01"
    },
    {
      "start": 123.759,
      "duration": 2.081,
      "text": "that NASDAQ Monday happened and $1",
      "timestamp": "02:03"
    },
    {
      "start": 125.84,
      "duration": 2.639,
      "text": "trillion dollars disappeared from the",
      "timestamp": "02:05"
    },
    {
      "start": 128.479,
      "duration": 2.48,
      "text": "stock market everyone was thinking \"Oh",
      "timestamp": "02:08"
    },
    {
      "start": 130.959,
      "duration": 1.92,
      "text": "my god it's the end of GPUs it's the end",
      "timestamp": "02:10"
    },
    {
      "start": 132.879,
      "duration": 2.241,
      "text": "of Nvidia and the bubble has burst.\" I",
      "timestamp": "02:12"
    },
    {
      "start": 135.12,
      "duration": 1.68,
      "text": "think I saw a lot of those headlines and",
      "timestamp": "02:15"
    },
    {
      "start": 136.8,
      "duration": 1.76,
      "text": "if you read the semi- analysis tweets",
      "timestamp": "02:16"
    },
    {
      "start": 138.56,
      "duration": 1.44,
      "text": "and the report like within the next day",
      "timestamp": "02:18"
    },
    {
      "start": 140.0,
      "duration": 2.72,
      "text": "or the week what they showed you is that",
      "timestamp": "02:20"
    },
    {
      "start": 142.72,
      "duration": 2.159,
      "text": "Jeban's paradox happens right so we'll",
      "timestamp": "02:22"
    },
    {
      "start": 144.879,
      "duration": 2.801,
      "text": "see if the the thing works it does and",
      "timestamp": "02:24"
    },
    {
      "start": 147.68,
      "duration": 2.88,
      "text": "when you lower the cost of a valuable",
      "timestamp": "02:27"
    },
    {
      "start": 150.56,
      "duration": 4.64,
      "text": "commodity demand surges and the two best",
      "timestamp": "02:30"
    },
    {
      "start": 155.2,
      "duration": 1.92,
      "text": "recent examples of that but go ahead if",
      "timestamp": "02:35"
    },
    {
      "start": 157.12,
      "duration": 1.28,
      "text": "you had a comment actually\n well no I was",
      "timestamp": "02:37"
    },
    {
      "start": 158.4,
      "duration": 1.44,
      "text": "going to say that you know the magnitude",
      "timestamp": "02:38"
    },
    {
      "start": 159.84,
      "duration": 2.08,
      "text": "is stunning it's a 100 times drop in",
      "timestamp": "02:39"
    },
    {
      "start": 161.92,
      "duration": 2.959,
      "text": "price right in just two years right so",
      "timestamp": "02:41"
    },
    {
      "start": 164.879,
      "duration": 2.241,
      "text": "if you were selling GPUs you know you",
      "timestamp": "02:44"
    },
    {
      "start": 167.12,
      "duration": 1.68,
      "text": "might freak out a little bit but that's",
      "timestamp": "02:47"
    },
    {
      "start": 168.8,
      "duration": 2.32,
      "text": "not what we've seen right\n demand just",
      "timestamp": "02:48"
    },
    {
      "start": 171.12,
      "duration": 2.96,
      "text": "kept going up\n best example is cursor",
      "timestamp": "02:51"
    },
    {
      "start": 174.08,
      "duration": 2.08,
      "text": "right so follow the money kursa went",
      "timestamp": "02:54"
    },
    {
      "start": 176.16,
      "duration": 2.56,
      "text": "from zero to $200 million in annual",
      "timestamp": "02:56"
    },
    {
      "start": 178.72,
      "duration": 2.64,
      "text": "recurring revenue in less than two years",
      "timestamp": "02:58"
    },
    {
      "start": 181.36,
      "duration": 3.2,
      "text": "it is by all opinions now the first true",
      "timestamp": "03:01"
    },
    {
      "start": 184.56,
      "duration": 3.52,
      "text": "killer app of AI the first paid killer",
      "timestamp": "03:04"
    },
    {
      "start": 188.08,
      "duration": 2.879,
      "text": "app of AI and it's horizontal it's",
      "timestamp": "03:08"
    },
    {
      "start": 190.959,
      "duration": 2.401,
      "text": "across every industry vertical and just",
      "timestamp": "03:10"
    },
    {
      "start": 193.36,
      "duration": 2.08,
      "text": "like 15 years ago when you had wanted to",
      "timestamp": "03:13"
    },
    {
      "start": 195.44,
      "duration": 1.6,
      "text": "measure where cloud was in the",
      "timestamp": "03:15"
    },
    {
      "start": 197.04,
      "duration": 2.64,
      "text": "enterprise it was stealth IT developers",
      "timestamp": "03:17"
    },
    {
      "start": 199.68,
      "duration": 1.839,
      "text": "with their credit cards guess what's",
      "timestamp": "03:19"
    },
    {
      "start": 201.519,
      "duration": 1.521,
      "text": "happening all over again if you want to",
      "timestamp": "03:21"
    },
    {
      "start": 203.04,
      "duration": 2.0,
      "text": "measure the true adoption at your",
      "timestamp": "03:23"
    },
    {
      "start": 205.04,
      "duration": 2.24,
      "text": "company at your government department of",
      "timestamp": "03:25"
    },
    {
      "start": 207.28,
      "duration": 3.519,
      "text": "AI find the cursor bills because it's",
      "timestamp": "03:27"
    },
    {
      "start": 210.799,
      "duration": 1.921,
      "text": "developers today right now whether you",
      "timestamp": "03:30"
    },
    {
      "start": 212.72,
      "duration": 1.84,
      "text": "know it or not that is signing up and",
      "timestamp": "03:32"
    },
    {
      "start": 214.56,
      "duration": 1.92,
      "text": "using cursor to write code in your",
      "timestamp": "03:34"
    },
    {
      "start": 216.48,
      "duration": 1.92,
      "text": "organization i think the more",
      "timestamp": "03:36"
    },
    {
      "start": 218.4,
      "duration": 1.839,
      "text": "interesting example though is the ARC",
      "timestamp": "03:38"
    },
    {
      "start": 220.239,
      "duration": 3.041,
      "text": "prize and the ARC AGI benchmark because",
      "timestamp": "03:40"
    },
    {
      "start": 223.28,
      "duration": 3.28,
      "text": "what they're showing is that you can now",
      "timestamp": "03:43"
    },
    {
      "start": 226.56,
      "duration": 2.56,
      "text": "manufacture intelligence",
      "timestamp": "03:46"
    },
    {
      "start": 229.12,
      "duration": 1.679,
      "text": "depending on the number of tokens you",
      "timestamp": "03:49"
    },
    {
      "start": 230.799,
      "duration": 1.841,
      "text": "generate if you generate more tokens",
      "timestamp": "03:50"
    },
    {
      "start": 232.64,
      "duration": 2.159,
      "text": "more quality tokens you get a higher",
      "timestamp": "03:52"
    },
    {
      "start": 234.799,
      "duration": 2.321,
      "text": "quality output your manis agent your",
      "timestamp": "03:54"
    },
    {
      "start": 237.12,
      "duration": 1.6,
      "text": "other agents can do more intelligent",
      "timestamp": "03:57"
    },
    {
      "start": 238.72,
      "duration": 3.12,
      "text": "work more productive work but it comes",
      "timestamp": "03:58"
    },
    {
      "start": 241.84,
      "duration": 5.039,
      "text": "at a cost so the cost in the ARC AGI1",
      "timestamp": "04:01"
    },
    {
      "start": 246.879,
      "duration": 4.72,
      "text": "benchmark was about 16 hours one six",
      "timestamp": "04:06"
    },
    {
      "start": 251.599,
      "duration": 2.56,
      "text": "hours of processing time at over a",
      "timestamp": "04:11"
    },
    {
      "start": 254.159,
      "duration": 3.76,
      "text": "million dollars of token costs that is",
      "timestamp": "04:14"
    },
    {
      "start": 257.919,
      "duration": 2.481,
      "text": "proving you can do a lot but certainly",
      "timestamp": "04:17"
    },
    {
      "start": 260.4,
      "duration": 4.0,
      "text": "not cost- effectively and ARGI 2 now is",
      "timestamp": "04:20"
    },
    {
      "start": 264.4,
      "duration": 2.079,
      "text": "a benchmark that introduces efficiency",
      "timestamp": "04:24"
    },
    {
      "start": 266.479,
      "duration": 2.401,
      "text": "token efficiency which we agree is just",
      "timestamp": "04:26"
    },
    {
      "start": 268.88,
      "duration": 1.52,
      "text": "a much better way to measure the",
      "timestamp": "04:28"
    },
    {
      "start": 270.4,
      "duration": 2.239,
      "text": "productivity of models because I don't",
      "timestamp": "04:30"
    },
    {
      "start": 272.639,
      "duration": 1.12,
      "text": "know about you but I don't have a",
      "timestamp": "04:32"
    },
    {
      "start": 273.759,
      "duration": 2.88,
      "text": "million dollars to throw at my next AI",
      "timestamp": "04:33"
    },
    {
      "start": 276.639,
      "duration": 2.241,
      "text": "problem",
      "timestamp": "04:36"
    },
    {
      "start": 278.88,
      "duration": 2.879,
      "text": "so the situation we find ourselves in",
      "timestamp": "04:38"
    },
    {
      "start": 281.759,
      "duration": 2.961,
      "text": "right now is whether you're a small or",
      "timestamp": "04:41"
    },
    {
      "start": 284.72,
      "duration": 4.24,
      "text": "large AI company your GPUs are melting",
      "timestamp": "04:44"
    },
    {
      "start": 288.96,
      "duration": 3.12,
      "text": "you don't have enough capacity despite",
      "timestamp": "04:48"
    },
    {
      "start": 292.08,
      "duration": 2.88,
      "text": "the plunging cost of inference you still",
      "timestamp": "04:52"
    },
    {
      "start": 294.96,
      "duration": 2.32,
      "text": "never have enough capacity to serve all",
      "timestamp": "04:54"
    },
    {
      "start": 297.28,
      "duration": 2.4,
      "text": "the demand for your users so this is a",
      "timestamp": "04:57"
    },
    {
      "start": 299.68,
      "duration": 1.84,
      "text": "problem that you know the industry is",
      "timestamp": "04:59"
    },
    {
      "start": 301.52,
      "duration": 2.32,
      "text": "solving in many many ways one of the",
      "timestamp": "05:01"
    },
    {
      "start": 303.84,
      "duration": 1.44,
      "text": "things they're doing is saying hey",
      "timestamp": "05:03"
    },
    {
      "start": 305.28,
      "duration": 2.24,
      "text": "you're just going to have to pay more so",
      "timestamp": "05:05"
    },
    {
      "start": 307.52,
      "duration": 2.08,
      "text": "there's already three classes of pricing",
      "timestamp": "05:07"
    },
    {
      "start": 309.6,
      "duration": 2.159,
      "text": "for every AI model and particularly in",
      "timestamp": "05:09"
    },
    {
      "start": 311.759,
      "duration": 2.321,
      "text": "the developer the API version this is",
      "timestamp": "05:11"
    },
    {
      "start": 314.08,
      "duration": 2.0,
      "text": "very visible to developers it's very",
      "timestamp": "05:14"
    },
    {
      "start": 316.08,
      "duration": 2.559,
      "text": "visible to cursor and and ISVS that",
      "timestamp": "05:16"
    },
    {
      "start": 318.639,
      "duration": 2.881,
      "text": "develop on LLMs you've got your input",
      "timestamp": "05:18"
    },
    {
      "start": 321.52,
      "duration": 2.08,
      "text": "token cost and you've got your output",
      "timestamp": "05:21"
    },
    {
      "start": 323.6,
      "duration": 2.159,
      "text": "token cost and the most interesting one",
      "timestamp": "05:23"
    },
    {
      "start": 325.759,
      "duration": 2.481,
      "text": "of course is in the middle the cached",
      "timestamp": "05:25"
    },
    {
      "start": 328.24,
      "duration": 2.88,
      "text": "input token cost now that now there's a",
      "timestamp": "05:28"
    },
    {
      "start": 331.12,
      "duration": 2.0,
      "text": "pretty universal consensus between open",
      "timestamp": "05:31"
    },
    {
      "start": 333.12,
      "duration": 2.16,
      "text": "AI that's joined the price war with",
      "timestamp": "05:33"
    },
    {
      "start": 335.28,
      "duration": 1.919,
      "text": "Google that had to join the price war",
      "timestamp": "05:35"
    },
    {
      "start": 337.199,
      "duration": 1.521,
      "text": "with Deepseek\n do you want to explain",
      "timestamp": "05:37"
    },
    {
      "start": 338.72,
      "duration": 2.479,
      "text": "what is a cached input token actually\n so",
      "timestamp": "05:38"
    },
    {
      "start": 341.199,
      "duration": 2.241,
      "text": "a great question Dan a cached input",
      "timestamp": "05:41"
    },
    {
      "start": 343.44,
      "duration": 2.88,
      "text": "token essentially says that if you're",
      "timestamp": "05:43"
    },
    {
      "start": 346.32,
      "duration": 2.879,
      "text": "reusing the same prompt over and over",
      "timestamp": "05:46"
    },
    {
      "start": 349.199,
      "duration": 2.56,
      "text": "again we're not going to charge you to",
      "timestamp": "05:49"
    },
    {
      "start": 351.759,
      "duration": 2.321,
      "text": "process that same prompt over again",
      "timestamp": "05:51"
    },
    {
      "start": 354.08,
      "duration": 2.32,
      "text": "because we have it cached we have a a",
      "timestamp": "05:54"
    },
    {
      "start": 356.4,
      "duration": 2.88,
      "text": "tokenized or vectorized version of that",
      "timestamp": "05:56"
    },
    {
      "start": 359.28,
      "duration": 2.479,
      "text": "prompt in memory something called a key",
      "timestamp": "05:59"
    },
    {
      "start": 361.759,
      "duration": 2.481,
      "text": "value cache or KV cache we already have",
      "timestamp": "06:01"
    },
    {
      "start": 364.24,
      "duration": 1.679,
      "text": "that in memory so we're not going to",
      "timestamp": "06:04"
    },
    {
      "start": 365.919,
      "duration": 2.0,
      "text": "charge you we're going to give you a 75%",
      "timestamp": "06:05"
    },
    {
      "start": 367.919,
      "duration": 2.481,
      "text": "discount actually for processing that",
      "timestamp": "06:07"
    },
    {
      "start": 370.4,
      "duration": 2.48,
      "text": "same token again because we reserve",
      "timestamp": "06:10"
    },
    {
      "start": 372.88,
      "duration": 2.96,
      "text": "about 25% for overhead still but it's a",
      "timestamp": "06:12"
    },
    {
      "start": 375.84,
      "duration": 2.639,
      "text": "much more cost-effective way to really",
      "timestamp": "06:15"
    },
    {
      "start": 378.479,
      "duration": 3.44,
      "text": "create an AI app and every single AI app",
      "timestamp": "06:18"
    },
    {
      "start": 381.919,
      "duration": 2.881,
      "text": "Cursor Windsurf client Ader you name it",
      "timestamp": "06:21"
    },
    {
      "start": 384.8,
      "duration": 3.679,
      "text": "manis AI they all use cacheed input",
      "timestamp": "06:24"
    },
    {
      "start": 388.479,
      "duration": 1.921,
      "text": "pricing and cacheed input processing",
      "timestamp": "06:28"
    },
    {
      "start": 390.4,
      "duration": 2.56,
      "text": "first however you notice there's a catch",
      "timestamp": "06:30"
    },
    {
      "start": 392.96,
      "duration": 3.12,
      "text": "at the bottom you can only get anywhere",
      "timestamp": "06:32"
    },
    {
      "start": 396.08,
      "duration": 2.88,
      "text": "and it's not guaranteed but between five",
      "timestamp": "06:36"
    },
    {
      "start": 398.96,
      "duration": 3.44,
      "text": "and 15 minutes of cash",
      "timestamp": "06:38"
    },
    {
      "start": 402.4,
      "duration": 2.32,
      "text": "so if you go for a coffee break or if",
      "timestamp": "06:42"
    },
    {
      "start": 404.72,
      "duration": 1.599,
      "text": "your agent is so busy that they're",
      "timestamp": "06:44"
    },
    {
      "start": 406.319,
      "duration": 2.641,
      "text": "spinning up many many subtasks and it",
      "timestamp": "06:46"
    },
    {
      "start": 408.96,
      "duration": 2.0,
      "text": "takes longer than 15 minutes like even a",
      "timestamp": "06:48"
    },
    {
      "start": 410.96,
      "duration": 2.079,
      "text": "deep research project a long reasoning",
      "timestamp": "06:50"
    },
    {
      "start": 413.039,
      "duration": 3.521,
      "text": "prompt takes longer than that to process",
      "timestamp": "06:53"
    },
    {
      "start": 416.56,
      "duration": 3.039,
      "text": "boom you go back to the full price no",
      "timestamp": "06:56"
    },
    {
      "start": 419.599,
      "duration": 3.121,
      "text": "discount and that radically impacts the",
      "timestamp": "06:59"
    },
    {
      "start": 422.72,
      "duration": 2.72,
      "text": "economics and and usability utility",
      "timestamp": "07:02"
    },
    {
      "start": 425.44,
      "duration": 1.44,
      "text": "we're going to talk about that dan's",
      "timestamp": "07:05"
    },
    {
      "start": 426.88,
      "duration": 2.48,
      "text": "going to dive deep into that and one of",
      "timestamp": "07:06"
    },
    {
      "start": 429.36,
      "duration": 2.64,
      "text": "the ways to really understand a",
      "timestamp": "07:09"
    },
    {
      "start": 432.0,
      "duration": 2.56,
      "text": "construct a framework for how we think",
      "timestamp": "07:12"
    },
    {
      "start": 434.56,
      "duration": 2.8,
      "text": "about the costs are the trade-offs we",
      "timestamp": "07:14"
    },
    {
      "start": 437.36,
      "duration": 1.52,
      "text": "have to make in this triad between",
      "timestamp": "07:17"
    },
    {
      "start": 438.88,
      "duration": 3.759,
      "text": "accuracy and cost you don't get the best",
      "timestamp": "07:18"
    },
    {
      "start": 442.639,
      "duration": 2.641,
      "text": "quality answer if you don't pay for it",
      "timestamp": "07:22"
    },
    {
      "start": 445.28,
      "duration": 2.96,
      "text": "and latency really really matters you",
      "timestamp": "07:25"
    },
    {
      "start": 448.24,
      "duration": 1.92,
      "text": "know if you get an answer an hour from",
      "timestamp": "07:28"
    },
    {
      "start": 450.16,
      "duration": 2.24,
      "text": "now when you're doing real-time voice",
      "timestamp": "07:30"
    },
    {
      "start": 452.4,
      "duration": 2.4,
      "text": "translation that's not useful at all in",
      "timestamp": "07:32"
    },
    {
      "start": 454.8,
      "duration": 1.519,
      "text": "many use cases you either need an",
      "timestamp": "07:34"
    },
    {
      "start": 456.319,
      "duration": 1.361,
      "text": "immediate answer or if you have an",
      "timestamp": "07:36"
    },
    {
      "start": 457.68,
      "duration": 2.72,
      "text": "agentic use case a lot of parallel tasks",
      "timestamp": "07:37"
    },
    {
      "start": 460.4,
      "duration": 2.56,
      "text": "you need rapid response low latency\n how",
      "timestamp": "07:40"
    },
    {
      "start": 462.96,
      "duration": 1.519,
      "text": "do we interpret this graph right because",
      "timestamp": "07:42"
    },
    {
      "start": 464.479,
      "duration": 1.921,
      "text": "we got latency squeezing what does that",
      "timestamp": "07:44"
    },
    {
      "start": 466.4,
      "duration": 1.44,
      "text": "mean for cost and what does it mean for",
      "timestamp": "07:46"
    },
    {
      "start": 467.84,
      "duration": 1.68,
      "text": "accuracy when you when you're at that",
      "timestamp": "07:47"
    },
    {
      "start": 469.52,
      "duration": 1.76,
      "text": "squeezing point of the the diagram\n yeah",
      "timestamp": "07:49"
    },
    {
      "start": 471.28,
      "duration": 1.199,
      "text": "this is a great question because this is",
      "timestamp": "07:51"
    },
    {
      "start": 472.479,
      "duration": 2.0,
      "text": "a fun thing to look at the animation but",
      "timestamp": "07:52"
    },
    {
      "start": 474.479,
      "duration": 1.84,
      "text": "actually what's happening here means a",
      "timestamp": "07:54"
    },
    {
      "start": 476.319,
      "duration": 2.56,
      "text": "lot what you want always is lower",
      "timestamp": "07:56"
    },
    {
      "start": 478.879,
      "duration": 1.281,
      "text": "latency right when you're doing",
      "timestamp": "07:58"
    },
    {
      "start": 480.16,
      "duration": 1.759,
      "text": "real-time voice translation there's just",
      "timestamp": "08:00"
    },
    {
      "start": 481.919,
      "duration": 2.641,
      "text": "no pausing no awkwardness you just want",
      "timestamp": "08:01"
    },
    {
      "start": 484.56,
      "duration": 2.88,
      "text": "instant low latency as you do that as",
      "timestamp": "08:04"
    },
    {
      "start": 487.44,
      "duration": 2.64,
      "text": "you pinch this triad this triangle when",
      "timestamp": "08:07"
    },
    {
      "start": 490.08,
      "duration": 2.08,
      "text": "you do it right when you configure your",
      "timestamp": "08:10"
    },
    {
      "start": 492.16,
      "duration": 1.52,
      "text": "models correctly when you use good",
      "timestamp": "08:12"
    },
    {
      "start": 493.68,
      "duration": 2.4,
      "text": "infrastructure in your data center",
      "timestamp": "08:13"
    },
    {
      "start": 496.08,
      "duration": 2.559,
      "text": "you're driving up the accuracy so you",
      "timestamp": "08:16"
    },
    {
      "start": 498.639,
      "duration": 2.081,
      "text": "see sort of the slope of of pretty",
      "timestamp": "08:18"
    },
    {
      "start": 500.72,
      "duration": 2.159,
      "text": "intense accuracy increase which",
      "timestamp": "08:20"
    },
    {
      "start": 502.879,
      "duration": 2.481,
      "text": "everybody wants and and and um you're",
      "timestamp": "08:22"
    },
    {
      "start": 505.36,
      "duration": 3.2,
      "text": "driving down at the slope an aggressive",
      "timestamp": "08:25"
    },
    {
      "start": 508.56,
      "duration": 2.479,
      "text": "reduction in overall costs because quite",
      "timestamp": "08:28"
    },
    {
      "start": 511.039,
      "duration": 2.24,
      "text": "simply you can process more tokens per",
      "timestamp": "08:31"
    },
    {
      "start": 513.279,
      "duration": 2.081,
      "text": "second everyone has a budget everyone",
      "timestamp": "08:33"
    },
    {
      "start": 515.36,
      "duration": 3.119,
      "text": "has a fixed GPU budget a fixed power",
      "timestamp": "08:35"
    },
    {
      "start": 518.479,
      "duration": 2.961,
      "text": "budget and so the more cycles you know",
      "timestamp": "08:38"
    },
    {
      "start": 521.44,
      "duration": 2.32,
      "text": "the more processing per cycle that you",
      "timestamp": "08:41"
    },
    {
      "start": 523.76,
      "duration": 2.56,
      "text": "have the more tokens you get and Dan's",
      "timestamp": "08:43"
    },
    {
      "start": 526.32,
      "duration": 1.199,
      "text": "actually going to do a great job right",
      "timestamp": "08:46"
    },
    {
      "start": 527.519,
      "duration": 2.641,
      "text": "now of really explaining how that",
      "timestamp": "08:47"
    },
    {
      "start": 530.16,
      "duration": 2.799,
      "text": "directly translates to profitability and",
      "timestamp": "08:50"
    },
    {
      "start": 532.959,
      "duration": 1.921,
      "text": "how it translates to whether you get a",
      "timestamp": "08:52"
    },
    {
      "start": 534.88,
      "duration": 2.16,
      "text": "rate limit or not and whether you get",
      "timestamp": "08:54"
    },
    {
      "start": 537.04,
      "duration": 3.28,
      "text": "all the tokens you want so one last of",
      "timestamp": "08:57"
    },
    {
      "start": 540.32,
      "duration": 2.079,
      "text": "the many many lessons DeepSeek taught us",
      "timestamp": "09:00"
    },
    {
      "start": 542.399,
      "duration": 2.0,
      "text": "and the reason we love DeepSseek is",
      "timestamp": "09:02"
    },
    {
      "start": 544.399,
      "duration": 2.161,
      "text": "they're incredibly transparent they're",
      "timestamp": "09:04"
    },
    {
      "start": 546.56,
      "duration": 1.92,
      "text": "not just an open- source model with open",
      "timestamp": "09:06"
    },
    {
      "start": 548.48,
      "duration": 2.72,
      "text": "weights um the way a lot of open models",
      "timestamp": "09:08"
    },
    {
      "start": 551.2,
      "duration": 2.88,
      "text": "are but they actually disclose on GitHub",
      "timestamp": "09:11"
    },
    {
      "start": 554.08,
      "duration": 1.92,
      "text": "a lot of how they operate their",
      "timestamp": "09:14"
    },
    {
      "start": 556.0,
      "duration": 2.64,
      "text": "inference infrastructure no other lab to",
      "timestamp": "09:16"
    },
    {
      "start": 558.64,
      "duration": 2.24,
      "text": "my knowledge does that the closed labs",
      "timestamp": "09:18"
    },
    {
      "start": 560.88,
      "duration": 1.6,
      "text": "obviously don't that's a proprietary",
      "timestamp": "09:20"
    },
    {
      "start": 562.48,
      "duration": 2.4,
      "text": "trade secret but even the open labs even",
      "timestamp": "09:22"
    },
    {
      "start": 564.88,
      "duration": 3.68,
      "text": "meta you know even Quen etc they don't",
      "timestamp": "09:24"
    },
    {
      "start": 568.56,
      "duration": 2.08,
      "text": "disclose actually how they operate their",
      "timestamp": "09:28"
    },
    {
      "start": 570.64,
      "duration": 2.0,
      "text": "inference and we learn so much all the",
      "timestamp": "09:30"
    },
    {
      "start": 572.64,
      "duration": 2.16,
      "text": "other labs the big labs as well as the",
      "timestamp": "09:32"
    },
    {
      "start": 574.8,
      "duration": 2.479,
      "text": "open labs learned so much from this so",
      "timestamp": "09:34"
    },
    {
      "start": 577.279,
      "duration": 3.12,
      "text": "one of the key lessons is elasticity",
      "timestamp": "09:37"
    },
    {
      "start": 580.399,
      "duration": 2.641,
      "text": "a best practice up until DeepSeek has",
      "timestamp": "09:40"
    },
    {
      "start": 583.04,
      "duration": 3.28,
      "text": "been two clusters you separate your",
      "timestamp": "09:43"
    },
    {
      "start": 586.32,
      "duration": 1.84,
      "text": "training cluster so be on you know this",
      "timestamp": "09:46"
    },
    {
      "start": 588.16,
      "duration": 2.32,
      "text": "side if you will from your inference",
      "timestamp": "09:48"
    },
    {
      "start": 590.48,
      "duration": 2.24,
      "text": "cluster and you have a big firewall in",
      "timestamp": "09:50"
    },
    {
      "start": 592.72,
      "duration": 2.96,
      "text": "between uh organizational firewall and",
      "timestamp": "09:52"
    },
    {
      "start": 595.68,
      "duration": 1.92,
      "text": "network firewall and you never have",
      "timestamp": "09:55"
    },
    {
      "start": 597.6,
      "duration": 2.08,
      "text": "researchers mess with inference and you",
      "timestamp": "09:57"
    },
    {
      "start": 599.68,
      "duration": 2.4,
      "text": "never have inference mess with research",
      "timestamp": "09:59"
    },
    {
      "start": 602.08,
      "duration": 2.08,
      "text": "deepseek said we have export controls",
      "timestamp": "10:02"
    },
    {
      "start": 604.16,
      "duration": 2.32,
      "text": "restrictive export controls on us we are",
      "timestamp": "10:04"
    },
    {
      "start": 606.48,
      "duration": 1.52,
      "text": "not allowed to use the latest generation",
      "timestamp": "10:06"
    },
    {
      "start": 608.0,
      "duration": 2.08,
      "text": "of processors that you know very well",
      "timestamp": "10:08"
    },
    {
      "start": 610.08,
      "duration": 1.84,
      "text": "documented by semi analysis we have",
      "timestamp": "10:10"
    },
    {
      "start": 611.92,
      "duration": 2.24,
      "text": "memory bandwidth constraints so we don't",
      "timestamp": "10:11"
    },
    {
      "start": 614.16,
      "duration": 2.08,
      "text": "have the luxury of having separate",
      "timestamp": "10:14"
    },
    {
      "start": 616.24,
      "duration": 1.92,
      "text": "clusters for training and inference",
      "timestamp": "10:16"
    },
    {
      "start": 618.16,
      "duration": 2.88,
      "text": "we're going to write white papers",
      "timestamp": "10:18"
    },
    {
      "start": 621.04,
      "duration": 1.68,
      "text": "combine our research and inference",
      "timestamp": "10:21"
    },
    {
      "start": 622.72,
      "duration": 2.32,
      "text": "departments and operate our entire",
      "timestamp": "10:22"
    },
    {
      "start": 625.04,
      "duration": 3.28,
      "text": "business on the same cluster which is",
      "timestamp": "10:25"
    },
    {
      "start": 628.32,
      "duration": 2.32,
      "text": "revolutionary but it leads to enormous",
      "timestamp": "10:28"
    },
    {
      "start": 630.64,
      "duration": 1.6,
      "text": "efficiencies and it leads to the price",
      "timestamp": "10:30"
    },
    {
      "start": 632.24,
      "duration": 1.92,
      "text": "war that we're all enjoying from it it's",
      "timestamp": "10:32"
    },
    {
      "start": 634.16,
      "duration": 2.32,
      "text": "a big reason why the cost of inference",
      "timestamp": "10:34"
    },
    {
      "start": 636.48,
      "duration": 1.52,
      "text": "has plummeted and so what DeepS seek",
      "timestamp": "10:36"
    },
    {
      "start": 638.0,
      "duration": 3.76,
      "text": "does is they infer a lot during daylight",
      "timestamp": "10:38"
    },
    {
      "start": 641.76,
      "duration": 2.48,
      "text": "hours and then they elastically train",
      "timestamp": "10:41"
    },
    {
      "start": 644.24,
      "duration": 2.56,
      "text": "overnight and then they elastically ramp",
      "timestamp": "10:44"
    },
    {
      "start": 646.8,
      "duration": 1.68,
      "text": "up and do inference again when when",
      "timestamp": "10:46"
    },
    {
      "start": 648.48,
      "duration": 2.32,
      "text": "China in this case wakes up and that is",
      "timestamp": "10:48"
    },
    {
      "start": 650.8,
      "duration": 2.24,
      "text": "much more complicated to execute than to",
      "timestamp": "10:50"
    },
    {
      "start": 653.04,
      "duration": 1.84,
      "text": "actually say because we're dealing with",
      "timestamp": "10:53"
    },
    {
      "start": 654.88,
      "duration": 2.079,
      "text": "giant model weight files here we're",
      "timestamp": "10:54"
    },
    {
      "start": 656.959,
      "duration": 3.12,
      "text": "dealing with giant clusters here so it's",
      "timestamp": "10:56"
    },
    {
      "start": 660.079,
      "duration": 1.841,
      "text": "not easy at all to turn these things on",
      "timestamp": "11:00"
    },
    {
      "start": 661.92,
      "duration": 1.76,
      "text": "and off",
      "timestamp": "11:01"
    },
    {
      "start": 663.68,
      "duration": 2.719,
      "text": "so one of the last things uh that we'll",
      "timestamp": "11:03"
    },
    {
      "start": 666.399,
      "duration": 2.241,
      "text": "I'll I'll uh wrap up with before we hand",
      "timestamp": "11:06"
    },
    {
      "start": 668.64,
      "duration": 2.16,
      "text": "this over to Dan for a bit is this",
      "timestamp": "11:08"
    },
    {
      "start": 670.8,
      "duration": 1.84,
      "text": "really important announcement just last",
      "timestamp": "11:10"
    },
    {
      "start": 672.64,
      "duration": 2.8,
      "text": "week from Nvidia they had an acquisition",
      "timestamp": "11:12"
    },
    {
      "start": 675.44,
      "duration": 2.32,
      "text": "of Leptin was it Leptin AI they acquired",
      "timestamp": "11:15"
    },
    {
      "start": 677.76,
      "duration": 3.36,
      "text": "yeah Leptton that was\n Lepon leptton\n and",
      "timestamp": "11:17"
    },
    {
      "start": 681.12,
      "duration": 3.279,
      "text": "so uh with Leptton they're creating a",
      "timestamp": "11:21"
    },
    {
      "start": 684.399,
      "duration": 3.841,
      "text": "consistent developer experience for all",
      "timestamp": "11:24"
    },
    {
      "start": 688.24,
      "duration": 2.719,
      "text": "users across any cloud but also this is",
      "timestamp": "11:28"
    },
    {
      "start": 690.959,
      "duration": 1.361,
      "text": "where I think the tokconomics are going",
      "timestamp": "11:30"
    },
    {
      "start": 692.32,
      "duration": 3.12,
      "text": "to really factor in a open marketplace",
      "timestamp": "11:32"
    },
    {
      "start": 695.44,
      "duration": 3.04,
      "text": "for GPU hours an open marketplace for",
      "timestamp": "11:35"
    },
    {
      "start": 698.48,
      "duration": 2.479,
      "text": "inference effectively and the fact that",
      "timestamp": "11:38"
    },
    {
      "start": 700.959,
      "duration": 4.32,
      "text": "AWS NBUS Cororeweave Lambda Labs and",
      "timestamp": "11:40"
    },
    {
      "start": 705.279,
      "duration": 2.961,
      "text": "other hypers scale clouds and Neoclouds",
      "timestamp": "11:45"
    },
    {
      "start": 708.24,
      "duration": 1.68,
      "text": "GPU clouds have already signed up to",
      "timestamp": "11:48"
    },
    {
      "start": 709.92,
      "duration": 2.159,
      "text": "this means it's not just a wish or a",
      "timestamp": "11:49"
    },
    {
      "start": 712.079,
      "duration": 2.641,
      "text": "desire on Nvidia's part this is real",
      "timestamp": "11:52"
    },
    {
      "start": 714.72,
      "duration": 1.2,
      "text": "it's going to create a hyperco",
      "timestamp": "11:54"
    },
    {
      "start": 715.92,
      "duration": 2.479,
      "text": "competitive marketplace and put a cute",
      "timestamp": "11:55"
    },
    {
      "start": 718.399,
      "duration": 2.401,
      "text": "focus on the tokconomics\n it's a little",
      "timestamp": "11:58"
    },
    {
      "start": 720.8,
      "duration": 1.599,
      "text": "like you know Yuber right there used to",
      "timestamp": "12:00"
    },
    {
      "start": 722.399,
      "duration": 1.841,
      "text": "be so many different taxi companies and",
      "timestamp": "12:02"
    },
    {
      "start": 724.24,
      "duration": 1.839,
      "text": "if you want a consistent experience go",
      "timestamp": "12:04"
    },
    {
      "start": 726.079,
      "duration": 2.641,
      "text": "to Uber\n exactly uh so it's really",
      "timestamp": "12:06"
    },
    {
      "start": 728.72,
      "duration": 1.84,
      "text": "homogenizing the whole thing\n so I'll",
      "timestamp": "12:08"
    },
    {
      "start": 730.56,
      "duration": 1.44,
      "text": "I'll talk through five metrics that",
      "timestamp": "12:10"
    },
    {
      "start": 732.0,
      "duration": 1.36,
      "text": "matter and then we'll go into the full",
      "timestamp": "12:12"
    },
    {
      "start": 733.36,
      "duration": 2.56,
      "text": "stack so token throughput which is when",
      "timestamp": "12:13"
    },
    {
      "start": 735.92,
      "duration": 2.32,
      "text": "you start asking Chap a question and you",
      "timestamp": "12:15"
    },
    {
      "start": 738.24,
      "duration": 1.76,
      "text": "get the response you get token by token",
      "timestamp": "12:18"
    },
    {
      "start": 740.0,
      "duration": 1.6,
      "text": "and how long does it take for all those",
      "timestamp": "12:20"
    },
    {
      "start": 741.6,
      "duration": 1.28,
      "text": "tokens to come out like what's the time",
      "timestamp": "12:21"
    },
    {
      "start": 742.88,
      "duration": 3.04,
      "text": "between tokens typically 20 was the the",
      "timestamp": "12:22"
    },
    {
      "start": 745.92,
      "duration": 2.8,
      "text": "lower limit and 50 to 80 is is is about",
      "timestamp": "12:25"
    },
    {
      "start": 748.72,
      "duration": 2.48,
      "text": "normal now time the first token is when",
      "timestamp": "12:28"
    },
    {
      "start": 751.2,
      "duration": 1.68,
      "text": "you put in your question and when do you",
      "timestamp": "12:31"
    },
    {
      "start": 752.88,
      "duration": 1.759,
      "text": "start getting the response and we add",
      "timestamp": "12:32"
    },
    {
      "start": 754.639,
      "duration": 1.521,
      "text": "the two together we've get end to end",
      "timestamp": "12:34"
    },
    {
      "start": 756.16,
      "duration": 2.08,
      "text": "latency which just adds a time the first",
      "timestamp": "12:36"
    },
    {
      "start": 758.24,
      "duration": 2.159,
      "text": "token and then all the tokens together",
      "timestamp": "12:38"
    },
    {
      "start": 760.399,
      "duration": 1.761,
      "text": "you know 20 to 30 seconds could be",
      "timestamp": "12:40"
    },
    {
      "start": 762.16,
      "duration": 3.76,
      "text": "typical um cost per token um obvious one",
      "timestamp": "12:42"
    },
    {
      "start": 765.92,
      "duration": 2.4,
      "text": "and then context window which is the",
      "timestamp": "12:45"
    },
    {
      "start": 768.32,
      "duration": 2.16,
      "text": "maximum amount of tokens you can put",
      "timestamp": "12:48"
    },
    {
      "start": 770.48,
      "duration": 1.68,
      "text": "into a model or a typical it could be",
      "timestamp": "12:50"
    },
    {
      "start": 772.16,
      "duration": 2.0,
      "text": "100,000 in the case of cursor it could",
      "timestamp": "12:52"
    },
    {
      "start": 774.16,
      "duration": 2.0,
      "text": "be 4,000 in the case of summarization",
      "timestamp": "12:54"
    },
    {
      "start": 776.16,
      "duration": 1.679,
      "text": "and what we'll see is there's really no",
      "timestamp": "12:56"
    },
    {
      "start": 777.839,
      "duration": 1.761,
      "text": "typical case like you know no",
      "timestamp": "12:57"
    },
    {
      "start": 779.6,
      "duration": 2.4,
      "text": "oneizefits-all solutions\n plama 4 is up",
      "timestamp": "12:59"
    },
    {
      "start": 782.0,
      "duration": 3.36,
      "text": "to a million now so\n oh so a typical I",
      "timestamp": "13:02"
    },
    {
      "start": 785.36,
      "duration": 1.44,
      "text": "think uh for the investors out there",
      "timestamp": "13:05"
    },
    {
      "start": 786.8,
      "duration": 1.52,
      "text": "bond perspectus would be about like a",
      "timestamp": "13:06"
    },
    {
      "start": 788.32,
      "duration": 4.16,
      "text": "100,000 tokens you know 200 100 pages so",
      "timestamp": "13:08"
    },
    {
      "start": 792.48,
      "duration": 1.44,
      "text": "I'll take you through this right and I",
      "timestamp": "13:12"
    },
    {
      "start": 793.92,
      "duration": 2.24,
      "text": "think the upshot is you all are familiar",
      "timestamp": "13:13"
    },
    {
      "start": 796.16,
      "duration": 2.799,
      "text": "with $20 a month for chat GBTN those in",
      "timestamp": "13:16"
    },
    {
      "start": 798.959,
      "duration": 2.241,
      "text": "the uh I don't know you can so the",
      "timestamp": "13:18"
    },
    {
      "start": 801.2,
      "duration": 3.04,
      "text": "33,000 up top right that's the capital",
      "timestamp": "13:21"
    },
    {
      "start": 804.24,
      "duration": 3.92,
      "text": "cost per one H100 GPU and how do we get",
      "timestamp": "13:24"
    },
    {
      "start": 808.16,
      "duration": 3.2,
      "text": "from the capital cost of a GPU all the",
      "timestamp": "13:28"
    },
    {
      "start": 811.36,
      "duration": 1.919,
      "text": "way to $20 which is right down here",
      "timestamp": "13:31"
    },
    {
      "start": 813.279,
      "duration": 1.441,
      "text": "which is what we're all paying and",
      "timestamp": "13:33"
    },
    {
      "start": 814.72,
      "duration": 1.76,
      "text": "that's what we do here at semi analysis",
      "timestamp": "13:34"
    },
    {
      "start": 816.48,
      "duration": 2.4,
      "text": "right we have nearly 40 folks uh around",
      "timestamp": "13:36"
    },
    {
      "start": 818.88,
      "duration": 1.759,
      "text": "the world and we have a few practice",
      "timestamp": "13:38"
    },
    {
      "start": 820.639,
      "duration": 1.921,
      "text": "areas and to really understand the full",
      "timestamp": "13:40"
    },
    {
      "start": 822.56,
      "duration": 1.68,
      "text": "stack you have to understand each layer",
      "timestamp": "13:42"
    },
    {
      "start": 824.24,
      "duration": 1.76,
      "text": "and I'll talk you through that the first",
      "timestamp": "13:44"
    },
    {
      "start": 826.0,
      "duration": 2.399,
      "text": "layer is neoclouds right and neoclouds",
      "timestamp": "13:46"
    },
    {
      "start": 828.399,
      "duration": 1.601,
      "text": "purchase GPUs they turn capital",
      "timestamp": "13:48"
    },
    {
      "start": 830.0,
      "duration": 2.88,
      "text": "expenditure into dollars per hour what",
      "timestamp": "13:50"
    },
    {
      "start": 832.88,
      "duration": 1.68,
      "text": "are the inputs well capital cost of",
      "timestamp": "13:52"
    },
    {
      "start": 834.56,
      "duration": 3.2,
      "text": "capital collocation energy",
      "timestamp": "13:54"
    },
    {
      "start": 837.76,
      "duration": 2.24,
      "text": "um and you know there's also a margin",
      "timestamp": "13:57"
    },
    {
      "start": 840.0,
      "duration": 2.16,
      "text": "right so how do we analyze this we have",
      "timestamp": "14:00"
    },
    {
      "start": 842.16,
      "duration": 1.6,
      "text": "to understand IC design we have to",
      "timestamp": "14:02"
    },
    {
      "start": 843.76,
      "duration": 1.92,
      "text": "understand AI data centers and we have",
      "timestamp": "14:03"
    },
    {
      "start": 845.68,
      "duration": 1.68,
      "text": "to understand AI neocloud so those are",
      "timestamp": "14:05"
    },
    {
      "start": 847.36,
      "duration": 1.76,
      "text": "three practice groups now let's look at",
      "timestamp": "14:07"
    },
    {
      "start": 849.12,
      "duration": 1.519,
      "text": "the next layer of the stack right so",
      "timestamp": "14:09"
    },
    {
      "start": 850.639,
      "duration": 1.361,
      "text": "it's inference providers this could be",
      "timestamp": "14:10"
    },
    {
      "start": 852.0,
      "duration": 2.24,
      "text": "your cloud your open AI what they do is",
      "timestamp": "14:12"
    },
    {
      "start": 854.24,
      "duration": 2.08,
      "text": "they rent capacity off the neoclouds at",
      "timestamp": "14:14"
    },
    {
      "start": 856.32,
      "duration": 3.04,
      "text": "$2 an hour and they sell tokens but how",
      "timestamp": "14:16"
    },
    {
      "start": 859.36,
      "duration": 3.2,
      "text": "do we get from dollars per hour on a on",
      "timestamp": "14:19"
    },
    {
      "start": 862.56,
      "duration": 3.6,
      "text": "a GPU to dollars per token right well I",
      "timestamp": "14:22"
    },
    {
      "start": 866.16,
      "duration": 1.76,
      "text": "think It's obvious right you know how",
      "timestamp": "14:26"
    },
    {
      "start": 867.92,
      "duration": 1.84,
      "text": "much tokens can we get off of these",
      "timestamp": "14:27"
    },
    {
      "start": 869.76,
      "duration": 2.319,
      "text": "right and so this is where our next",
      "timestamp": "14:29"
    },
    {
      "start": 872.079,
      "duration": 3.041,
      "text": "practice area comes in you see that 723",
      "timestamp": "14:32"
    },
    {
      "start": 875.12,
      "duration": 2.24,
      "text": "right how do we get that 723 and the way",
      "timestamp": "14:35"
    },
    {
      "start": 877.36,
      "duration": 1.52,
      "text": "we do is AI engineering we have a team",
      "timestamp": "14:37"
    },
    {
      "start": 878.88,
      "duration": 2.079,
      "text": "of three AI engineers we typically have",
      "timestamp": "14:38"
    },
    {
      "start": 880.959,
      "duration": 1.761,
      "text": "five to eight AI servers at any given",
      "timestamp": "14:40"
    },
    {
      "start": 882.72,
      "duration": 1.6,
      "text": "time and we're just running a lot of",
      "timestamp": "14:42"
    },
    {
      "start": 884.32,
      "duration": 1.519,
      "text": "benchmarks you know we work a lot with",
      "timestamp": "14:44"
    },
    {
      "start": 885.839,
      "duration": 1.921,
      "text": "Val and his team to think about what are",
      "timestamp": "14:45"
    },
    {
      "start": 887.76,
      "duration": 1.68,
      "text": "some use cases and and how would they",
      "timestamp": "14:47"
    },
    {
      "start": 889.44,
      "duration": 2.32,
      "text": "perform and how if we you know kind of",
      "timestamp": "14:49"
    },
    {
      "start": 891.76,
      "duration": 1.439,
      "text": "do creative things you know what can we",
      "timestamp": "14:51"
    },
    {
      "start": 893.199,
      "duration": 2.08,
      "text": "get out of this so that's how we get in",
      "timestamp": "14:53"
    },
    {
      "start": 895.279,
      "duration": 2.56,
      "text": "723 if you do the math the cost is going",
      "timestamp": "14:55"
    },
    {
      "start": 897.839,
      "duration": 2.8,
      "text": "to be 150 right and what goes on top",
      "timestamp": "14:57"
    },
    {
      "start": 900.639,
      "duration": 2.64,
      "text": "think of cursor cursor will purchase",
      "timestamp": "15:00"
    },
    {
      "start": 903.279,
      "duration": 3.601,
      "text": "inference off of Claude Gemini or OpenAI",
      "timestamp": "15:03"
    },
    {
      "start": 906.88,
      "duration": 1.759,
      "text": "and they'll sell it as an Arboo product",
      "timestamp": "15:06"
    },
    {
      "start": 908.639,
      "duration": 2.721,
      "text": "right so a typical user might consume",
      "timestamp": "15:08"
    },
    {
      "start": 911.36,
      "duration": 2.719,
      "text": "six million tokens a month at 150 per",
      "timestamp": "15:11"
    },
    {
      "start": 914.079,
      "duration": 2.961,
      "text": "million token um their profit margin is",
      "timestamp": "15:14"
    },
    {
      "start": 917.04,
      "duration": 2.799,
      "text": "about $11 and this is how we look at the",
      "timestamp": "15:17"
    },
    {
      "start": 919.839,
      "duration": 3.041,
      "text": "world you can't understand the whole",
      "timestamp": "15:19"
    },
    {
      "start": 922.88,
      "duration": 2.0,
      "text": "factory without understanding each part",
      "timestamp": "15:22"
    },
    {
      "start": 924.88,
      "duration": 2.8,
      "text": "of the stack this is the capital cost",
      "timestamp": "15:24"
    },
    {
      "start": 927.68,
      "duration": 1.76,
      "text": "just to show you the assumptions 13%",
      "timestamp": "15:27"
    },
    {
      "start": 929.44,
      "duration": 2.48,
      "text": "cost of capital for your useful life the",
      "timestamp": "15:29"
    },
    {
      "start": 931.92,
      "duration": 3.359,
      "text": "chip will last um the chip will last six",
      "timestamp": "15:31"
    },
    {
      "start": 935.279,
      "duration": 1.68,
      "text": "years maybe what we like to say you want",
      "timestamp": "15:35"
    },
    {
      "start": 936.959,
      "duration": 2.0,
      "text": "to make your economic useful life for",
      "timestamp": "15:36"
    },
    {
      "start": 938.959,
      "duration": 1.201,
      "text": "this is all available in selling",
      "timestamp": "15:38"
    },
    {
      "start": 940.16,
      "duration": 2.64,
      "text": "analysis.com we got tons of tables so",
      "timestamp": "15:40"
    },
    {
      "start": 942.8,
      "duration": 2.24,
      "text": "okay and then this is just to show the",
      "timestamp": "15:42"
    },
    {
      "start": 945.04,
      "duration": 2.239,
      "text": "upshot is a 76% capital cost so very",
      "timestamp": "15:45"
    },
    {
      "start": 947.279,
      "duration": 3.841,
      "text": "capital intensive um so Neoclouds let's",
      "timestamp": "15:47"
    },
    {
      "start": 951.12,
      "duration": 2.0,
      "text": "talk about Neoclouds this is the the",
      "timestamp": "15:51"
    },
    {
      "start": 953.12,
      "duration": 2.24,
      "text": "trend and one of the big worries about",
      "timestamp": "15:53"
    },
    {
      "start": 955.36,
      "duration": 3.279,
      "text": "Neoclouds is well how quickly um you",
      "timestamp": "15:55"
    },
    {
      "start": 958.639,
      "duration": 1.681,
      "text": "know will the rental price go down right",
      "timestamp": "15:58"
    },
    {
      "start": 960.32,
      "duration": 1.68,
      "text": "and with the furious space in which",
      "timestamp": "16:00"
    },
    {
      "start": 962.0,
      "duration": 2.16,
      "text": "Nvidia is advancing a lot of a lot of",
      "timestamp": "16:02"
    },
    {
      "start": 964.16,
      "duration": 1.919,
      "text": "them ask like why should I ever buy GPUs",
      "timestamp": "16:04"
    },
    {
      "start": 966.079,
      "duration": 1.521,
      "text": "because they'll be superseded pretty",
      "timestamp": "16:06"
    },
    {
      "start": 967.6,
      "duration": 1.919,
      "text": "soon um but you know if you're doing the",
      "timestamp": "16:07"
    },
    {
      "start": 969.519,
      "duration": 1.44,
      "text": "right thing doing term contracts you'll",
      "timestamp": "16:09"
    },
    {
      "start": 970.959,
      "duration": 2.481,
      "text": "be okay why are NeoCloud so important",
      "timestamp": "16:10"
    },
    {
      "start": 973.44,
      "duration": 2.48,
      "text": "you see OpenAI as a perfect example",
      "timestamp": "16:13"
    },
    {
      "start": 975.92,
      "duration": 2.0,
      "text": "they're out of Microsoft capacity they",
      "timestamp": "16:15"
    },
    {
      "start": 977.92,
      "duration": 1.839,
      "text": "went to Oracle they're out of Oracle",
      "timestamp": "16:17"
    },
    {
      "start": 979.759,
      "duration": 1.76,
      "text": "capacity they go to Corewave and",
      "timestamp": "16:19"
    },
    {
      "start": 981.519,
      "duration": 1.921,
      "text": "NeoCloud for more capacity now they're",
      "timestamp": "16:21"
    },
    {
      "start": 983.44,
      "duration": 1.759,
      "text": "even going to Google for extra GPU",
      "timestamp": "16:23"
    },
    {
      "start": 985.199,
      "duration": 2.241,
      "text": "capacity there just quite frankly is not",
      "timestamp": "16:25"
    },
    {
      "start": 987.44,
      "duration": 2.319,
      "text": "enough capacity in the world for the",
      "timestamp": "16:27"
    },
    {
      "start": 989.759,
      "duration": 2.161,
      "text": "demand for all these you know apps like",
      "timestamp": "16:29"
    },
    {
      "start": 991.92,
      "duration": 1.52,
      "text": "cursor and others and that's why",
      "timestamp": "16:31"
    },
    {
      "start": 993.44,
      "duration": 2.319,
      "text": "NeoCloud economics are vital\n and supply",
      "timestamp": "16:33"
    },
    {
      "start": 995.759,
      "duration": 3.121,
      "text": "you can analyze demand is hard right but",
      "timestamp": "16:35"
    },
    {
      "start": 998.88,
      "duration": 1.759,
      "text": "we have price right so if you have two",
      "timestamp": "16:38"
    },
    {
      "start": 1000.639,
      "duration": 1.281,
      "text": "you can figure out demand right and",
      "timestamp": "16:40"
    },
    {
      "start": 1001.92,
      "duration": 1.44,
      "text": "we'll show you some examples but first",
      "timestamp": "16:41"
    },
    {
      "start": 1003.36,
      "duration": 1.279,
      "text": "let me take you through an illustrative",
      "timestamp": "16:43"
    },
    {
      "start": 1004.639,
      "duration": 3.12,
      "text": "example right so it's a bit like the",
      "timestamp": "16:44"
    },
    {
      "start": 1007.759,
      "duration": 2.241,
      "text": "analogy I like to use is it's a bus and",
      "timestamp": "16:47"
    },
    {
      "start": 1010.0,
      "duration": 1.759,
      "text": "a private jet right we've got this",
      "timestamp": "16:50"
    },
    {
      "start": 1011.759,
      "duration": 2.32,
      "text": "inference system if we have two users",
      "timestamp": "16:51"
    },
    {
      "start": 1014.079,
      "duration": 1.921,
      "text": "it's going to be fast right look at this",
      "timestamp": "16:54"
    },
    {
      "start": 1016.0,
      "duration": 2.72,
      "text": "141 tokens per second per user that's uh",
      "timestamp": "16:56"
    },
    {
      "start": 1018.72,
      "duration": 1.76,
      "text": "fast it's cheetah fast as you'll see in",
      "timestamp": "16:58"
    },
    {
      "start": 1020.48,
      "duration": 2.24,
      "text": "the next slide um and it's like riding a",
      "timestamp": "17:00"
    },
    {
      "start": 1022.72,
      "duration": 3.198,
      "text": "private jet now if we put 128 users",
      "timestamp": "17:02"
    },
    {
      "start": 1025.919,
      "duration": 2.801,
      "text": "right um then we get it slows down right",
      "timestamp": "17:05"
    },
    {
      "start": 1028.72,
      "duration": 3.119,
      "text": "a bus has to make many stops right and",
      "timestamp": "17:08"
    },
    {
      "start": 1031.839,
      "duration": 3.199,
      "text": "um as a result 18 tokens per second it's",
      "timestamp": "17:11"
    },
    {
      "start": 1035.039,
      "duration": 1.92,
      "text": "slow right but if you look at this",
      "timestamp": "17:15"
    },
    {
      "start": 1036.959,
      "duration": 2.0,
      "text": "column total tokens per second we see",
      "timestamp": "17:16"
    },
    {
      "start": 1038.959,
      "duration": 1.599,
      "text": "the throughput is high right so we're",
      "timestamp": "17:18"
    },
    {
      "start": 1040.559,
      "duration": 3.041,
      "text": "sweating our capacity really effectively",
      "timestamp": "17:20"
    },
    {
      "start": 1043.6,
      "duration": 2.8,
      "text": "um so what does this mean right this",
      "timestamp": "17:23"
    },
    {
      "start": 1046.4,
      "duration": 2.08,
      "text": "means that in order at this fixed",
      "timestamp": "17:26"
    },
    {
      "start": 1048.48,
      "duration": 1.6,
      "text": "selling price in order to make the",
      "timestamp": "17:28"
    },
    {
      "start": 1050.08,
      "duration": 2.16,
      "text": "economics work you have to be operating",
      "timestamp": "17:30"
    },
    {
      "start": 1052.24,
      "duration": 2.0,
      "text": "as if you're a bus right and if you want",
      "timestamp": "17:32"
    },
    {
      "start": 1054.24,
      "duration": 1.36,
      "text": "to run a private jet well you need to",
      "timestamp": "17:34"
    },
    {
      "start": 1055.6,
      "duration": 2.24,
      "text": "change the price of tokens right so I",
      "timestamp": "17:35"
    },
    {
      "start": 1057.84,
      "duration": 1.6,
      "text": "think the big animal takeaway here is",
      "timestamp": "17:37"
    },
    {
      "start": 1059.44,
      "duration": 1.76,
      "text": "some people prefer the the bus some",
      "timestamp": "17:39"
    },
    {
      "start": 1061.2,
      "duration": 1.599,
      "text": "people prefer the jet if you want",
      "timestamp": "17:41"
    },
    {
      "start": 1062.799,
      "duration": 2.721,
      "text": "cheetah inference well well if you want",
      "timestamp": "17:42"
    },
    {
      "start": 1065.52,
      "duration": 2.24,
      "text": "tortoise inference that's the bus and if",
      "timestamp": "17:45"
    },
    {
      "start": 1067.76,
      "duration": 1.279,
      "text": "you want cheetah inference that's the",
      "timestamp": "17:47"
    },
    {
      "start": 1069.039,
      "duration": 3.041,
      "text": "private jet um and let's look at how we",
      "timestamp": "17:49"
    },
    {
      "start": 1072.08,
      "duration": 3.2,
      "text": "analyze this okay so in this graph up",
      "timestamp": "17:52"
    },
    {
      "start": 1075.28,
      "duration": 2.24,
      "text": "here this graphs on the y-axis",
      "timestamp": "17:55"
    },
    {
      "start": 1077.52,
      "duration": 2.0,
      "text": "throughput per GPU how many tokens we're",
      "timestamp": "17:57"
    },
    {
      "start": 1079.52,
      "duration": 4.0,
      "text": "getting per per GPU right and on the uh",
      "timestamp": "17:59"
    },
    {
      "start": 1083.52,
      "duration": 2.399,
      "text": "x-axis this is latency remember how long",
      "timestamp": "18:03"
    },
    {
      "start": 1085.919,
      "duration": 1.841,
      "text": "it takes from when you first ask a",
      "timestamp": "18:05"
    },
    {
      "start": 1087.76,
      "duration": 1.76,
      "text": "question to when it completes giving you",
      "timestamp": "18:07"
    },
    {
      "start": 1089.52,
      "duration": 2.24,
      "text": "the answer and let's overlay an analogy",
      "timestamp": "18:09"
    },
    {
      "start": 1091.76,
      "duration": 2.0,
      "text": "here right just so you all don't get",
      "timestamp": "18:11"
    },
    {
      "start": 1093.76,
      "duration": 3.68,
      "text": "lost um the bus is up here so the bus is",
      "timestamp": "18:13"
    },
    {
      "start": 1097.44,
      "duration": 2.479,
      "text": "up here long latency it takes a long",
      "timestamp": "18:17"
    },
    {
      "start": 1099.919,
      "duration": 2.801,
      "text": "time to get everything uh there but",
      "timestamp": "18:19"
    },
    {
      "start": 1102.72,
      "duration": 1.92,
      "text": "we're running a lot of users right and",
      "timestamp": "18:22"
    },
    {
      "start": 1104.64,
      "duration": 1.52,
      "text": "that is why your throughput is high",
      "timestamp": "18:24"
    },
    {
      "start": 1106.16,
      "duration": 4.08,
      "text": "private jet few users very fast but we",
      "timestamp": "18:26"
    },
    {
      "start": 1110.24,
      "duration": 1.92,
      "text": "don't get a lot right now what does this",
      "timestamp": "18:30"
    },
    {
      "start": 1112.16,
      "duration": 1.68,
      "text": "mean for cost per token right and the",
      "timestamp": "18:32"
    },
    {
      "start": 1113.84,
      "duration": 2.24,
      "text": "way we get this right is we actually you",
      "timestamp": "18:33"
    },
    {
      "start": 1116.08,
      "duration": 2.08,
      "text": "know have run I guess it's hundreds of",
      "timestamp": "18:36"
    },
    {
      "start": 1118.16,
      "duration": 1.759,
      "text": "benchmarks you know we took about a few",
      "timestamp": "18:38"
    },
    {
      "start": 1119.919,
      "duration": 2.0,
      "text": "months to do this um I mean we ran on",
      "timestamp": "18:39"
    },
    {
      "start": 1121.919,
      "duration": 2.161,
      "text": "all these servers what's interesting is",
      "timestamp": "18:41"
    },
    {
      "start": 1124.08,
      "duration": 1.68,
      "text": "by the way this is both a benchmark of",
      "timestamp": "18:44"
    },
    {
      "start": 1125.76,
      "duration": 1.68,
      "text": "the hardware and the software because",
      "timestamp": "18:45"
    },
    {
      "start": 1127.44,
      "duration": 2.08,
      "text": "what you're seeing at the top there are",
      "timestamp": "18:47"
    },
    {
      "start": 1129.52,
      "duration": 1.44,
      "text": "different you know labels for the",
      "timestamp": "18:49"
    },
    {
      "start": 1130.96,
      "duration": 2.88,
      "text": "different color bars the TRT means it's",
      "timestamp": "18:50"
    },
    {
      "start": 1133.84,
      "duration": 3.36,
      "text": "not the open-source VLM Python based",
      "timestamp": "18:53"
    },
    {
      "start": 1137.2,
      "duration": 2.16,
      "text": "inference server it's actually Nvidia's",
      "timestamp": "18:57"
    },
    {
      "start": 1139.36,
      "duration": 3.84,
      "text": "faster often faster TRTLM C-based",
      "timestamp": "18:59"
    },
    {
      "start": 1143.2,
      "duration": 1.44,
      "text": "inference server so that's why you're",
      "timestamp": "19:03"
    },
    {
      "start": 1144.64,
      "duration": 1.12,
      "text": "seeing some differences in the",
      "timestamp": "19:04"
    },
    {
      "start": 1145.76,
      "duration": 2.32,
      "text": "performance the latency and so forth yep",
      "timestamp": "19:05"
    },
    {
      "start": 1148.08,
      "duration": 1.92,
      "text": "so everything else is VLM and that's",
      "timestamp": "19:08"
    },
    {
      "start": 1150.0,
      "duration": 2.0,
      "text": "tensor RT which which runs really well",
      "timestamp": "19:10"
    },
    {
      "start": 1152.0,
      "duration": 2.72,
      "text": "in terms of throughput um so again if we",
      "timestamp": "19:12"
    },
    {
      "start": 1154.72,
      "duration": 1.92,
      "text": "use our analogy right remember the",
      "timestamp": "19:14"
    },
    {
      "start": 1156.64,
      "duration": 3.039,
      "text": "private jet right um we actually had",
      "timestamp": "19:16"
    },
    {
      "start": 1159.679,
      "duration": 2.801,
      "text": "very fast you know response very low",
      "timestamp": "19:19"
    },
    {
      "start": 1162.48,
      "duration": 2.16,
      "text": "latency but we had very low throughput",
      "timestamp": "19:22"
    },
    {
      "start": 1164.64,
      "duration": 1.12,
      "text": "what does that mean right you're paying",
      "timestamp": "19:24"
    },
    {
      "start": 1165.76,
      "duration": 1.6,
      "text": "a fix remember you're paying a fixed $2",
      "timestamp": "19:25"
    },
    {
      "start": 1167.36,
      "duration": 1.679,
      "text": "an hour right if you generate less",
      "timestamp": "19:27"
    },
    {
      "start": 1169.039,
      "duration": 1.921,
      "text": "tokens the cost per token goes up",
      "timestamp": "19:29"
    },
    {
      "start": 1170.96,
      "duration": 1.76,
      "text": "obviously flying a private jet's more",
      "timestamp": "19:30"
    },
    {
      "start": 1172.72,
      "duration": 2.64,
      "text": "expensive than flying coach um but if",
      "timestamp": "19:32"
    },
    {
      "start": 1175.36,
      "duration": 2.96,
      "text": "you ride coach then you're out here um",
      "timestamp": "19:35"
    },
    {
      "start": 1178.32,
      "duration": 2.719,
      "text": "but here's the point right you know as",
      "timestamp": "19:38"
    },
    {
      "start": 1181.039,
      "duration": 1.841,
      "text": "Val said it's it's a it's a triad there",
      "timestamp": "19:41"
    },
    {
      "start": 1182.88,
      "duration": 1.52,
      "text": "are trade-offs and there's no",
      "timestamp": "19:42"
    },
    {
      "start": 1184.4,
      "duration": 1.6,
      "text": "one-sizefits-all some people like to fly",
      "timestamp": "19:44"
    },
    {
      "start": 1186.0,
      "duration": 1.2,
      "text": "in the private jet some people like to",
      "timestamp": "19:46"
    },
    {
      "start": 1187.2,
      "duration": 2.16,
      "text": "take the bus right concretely you know",
      "timestamp": "19:47"
    },
    {
      "start": 1189.36,
      "duration": 1.92,
      "text": "some people like to do 1,000 tokens some",
      "timestamp": "19:49"
    },
    {
      "start": 1191.28,
      "duration": 1.759,
      "text": "people want to do like 100,000 tokens",
      "timestamp": "19:51"
    },
    {
      "start": 1193.039,
      "duration": 1.76,
      "text": "there's all these different you know use",
      "timestamp": "19:53"
    },
    {
      "start": 1194.799,
      "duration": 1.521,
      "text": "cases and what NVIDIA showed here in",
      "timestamp": "19:54"
    },
    {
      "start": 1196.32,
      "duration": 2.239,
      "text": "their GTC is all the different trade-off",
      "timestamp": "19:56"
    },
    {
      "start": 1198.559,
      "duration": 3.041,
      "text": "points you can have uh and here's our",
      "timestamp": "19:58"
    },
    {
      "start": 1201.6,
      "duration": 2.72,
      "text": "private jet here's our bus private jet",
      "timestamp": "20:01"
    },
    {
      "start": 1204.32,
      "duration": 2.56,
      "text": "this here is tokens per second so this",
      "timestamp": "20:04"
    },
    {
      "start": 1206.88,
      "duration": 1.76,
      "text": "side means more interactive tokens are",
      "timestamp": "20:06"
    },
    {
      "start": 1208.64,
      "duration": 2.24,
      "text": "coming faster and then on this side is",
      "timestamp": "20:08"
    },
    {
      "start": 1210.88,
      "duration": 2.64,
      "text": "slower right but we get more throughput",
      "timestamp": "20:10"
    },
    {
      "start": 1213.52,
      "duration": 2.399,
      "text": "um so let's walk through a few examples",
      "timestamp": "20:13"
    },
    {
      "start": 1215.919,
      "duration": 1.521,
      "text": "right let's say we want to have the",
      "timestamp": "20:15"
    },
    {
      "start": 1217.44,
      "duration": 1.92,
      "text": "private jet right what does that mean",
      "timestamp": "20:17"
    },
    {
      "start": 1219.36,
      "duration": 1.84,
      "text": "that means obviously fewer tokens what",
      "timestamp": "20:19"
    },
    {
      "start": 1221.2,
      "duration": 2.88,
      "text": "does that do to economics well inference",
      "timestamp": "20:21"
    },
    {
      "start": 1224.08,
      "duration": 3.44,
      "text": "compute cost same $2 but now we've got",
      "timestamp": "20:24"
    },
    {
      "start": 1227.52,
      "duration": 1.84,
      "text": "7.48 for a million tokens because we're",
      "timestamp": "20:27"
    },
    {
      "start": 1229.36,
      "duration": 1.92,
      "text": "generating fewer inference provider",
      "timestamp": "20:29"
    },
    {
      "start": 1231.28,
      "duration": 4.399,
      "text": "gross margin minus 66% uh oh",
      "timestamp": "20:31"
    },
    {
      "start": 1235.679,
      "duration": 3.041,
      "text": "yeah uh and then if the uh if the",
      "timestamp": "20:35"
    },
    {
      "start": 1238.72,
      "duration": 2.56,
      "text": "upstream the the provider um the",
      "timestamp": "20:38"
    },
    {
      "start": 1241.28,
      "duration": 1.759,
      "text": "application providers adjust prices it's",
      "timestamp": "20:41"
    },
    {
      "start": 1243.039,
      "duration": 2.961,
      "text": "still a bit tight right um here's your",
      "timestamp": "20:43"
    },
    {
      "start": 1246.0,
      "duration": 3.52,
      "text": "bus private jet um let's talk about",
      "timestamp": "20:46"
    },
    {
      "start": 1249.52,
      "duration": 1.76,
      "text": "another angle we talked a lot about the",
      "timestamp": "20:49"
    },
    {
      "start": 1251.28,
      "duration": 2.16,
      "text": "trade-off between interactivity right",
      "timestamp": "20:51"
    },
    {
      "start": 1253.44,
      "duration": 2.32,
      "text": "having faster tokens what about more",
      "timestamp": "20:53"
    },
    {
      "start": 1255.76,
      "duration": 1.44,
      "text": "tokens what if you want instead of",
      "timestamp": "20:55"
    },
    {
      "start": 1257.2,
      "duration": 2.24,
      "text": "putting you know a summary of a website",
      "timestamp": "20:57"
    },
    {
      "start": 1259.44,
      "duration": 1.28,
      "text": "you want to put in a summary of like",
      "timestamp": "20:59"
    },
    {
      "start": 1260.72,
      "duration": 2.0,
      "text": "five perspectuses you want to put in",
      "timestamp": "21:00"
    },
    {
      "start": 1262.72,
      "duration": 1.6,
      "text": "your entire codebase what does that mean",
      "timestamp": "21:02"
    },
    {
      "start": 1264.32,
      "duration": 2.32,
      "text": "right as your context length increases",
      "timestamp": "21:04"
    },
    {
      "start": 1266.64,
      "duration": 2.399,
      "text": "you put in more that means your memory",
      "timestamp": "21:06"
    },
    {
      "start": 1269.039,
      "duration": 4.481,
      "text": "requirements sore and what happens well",
      "timestamp": "21:09"
    },
    {
      "start": 1273.52,
      "duration": 3.12,
      "text": "cheetah inference with only a little bit",
      "timestamp": "21:13"
    },
    {
      "start": 1276.64,
      "duration": 1.6,
      "text": "you're putting in you put a perspectus",
      "timestamp": "21:16"
    },
    {
      "start": 1278.24,
      "duration": 2.16,
      "text": "in you put your codebase and it's a",
      "timestamp": "21:18"
    },
    {
      "start": 1280.4,
      "duration": 2.32,
      "text": "tortoise inference unfortunately right",
      "timestamp": "21:20"
    },
    {
      "start": 1282.72,
      "duration": 2.16,
      "text": "but you know we want to be putting code",
      "timestamp": "21:22"
    },
    {
      "start": 1284.88,
      "duration": 1.84,
      "text": "bases we want to ingest multiple",
      "timestamp": "21:24"
    },
    {
      "start": 1286.72,
      "duration": 1.68,
      "text": "practices multiple earning calls you",
      "timestamp": "21:26"
    },
    {
      "start": 1288.4,
      "duration": 2.48,
      "text": "know all this health data like",
      "timestamp": "21:28"
    },
    {
      "start": 1290.88,
      "duration": 2.159,
      "text": "electronic health records right\n yeah",
      "timestamp": "21:30"
    },
    {
      "start": 1293.039,
      "duration": 1.441,
      "text": "just you know in the next stage over if",
      "timestamp": "21:33"
    },
    {
      "start": 1294.48,
      "duration": 1.84,
      "text": "you saw the manis keynote towo and",
      "timestamp": "21:34"
    },
    {
      "start": 1296.32,
      "duration": 3.359,
      "text": "cursor that is this workload it's all",
      "timestamp": "21:36"
    },
    {
      "start": 1299.679,
      "duration": 2.401,
      "text": "long context it's all multi-turn",
      "timestamp": "21:39"
    },
    {
      "start": 1302.08,
      "duration": 1.52,
      "text": "multiple questions hundreds of thousands",
      "timestamp": "21:42"
    },
    {
      "start": 1303.6,
      "duration": 2.24,
      "text": "of tokens right we used to think about",
      "timestamp": "21:43"
    },
    {
      "start": 1305.84,
      "duration": 1.839,
      "text": "you know maybe summarize you know it",
      "timestamp": "21:45"
    },
    {
      "start": 1307.679,
      "duration": 1.841,
      "text": "used to be um you know for information",
      "timestamp": "21:47"
    },
    {
      "start": 1309.52,
      "duration": 1.92,
      "text": "the the input output ratio is typically",
      "timestamp": "21:49"
    },
    {
      "start": 1311.44,
      "duration": 2.479,
      "text": "three input to one so people are sending",
      "timestamp": "21:51"
    },
    {
      "start": 1313.919,
      "duration": 1.681,
      "text": "a lot of information for summarization",
      "timestamp": "21:53"
    },
    {
      "start": 1315.6,
      "duration": 2.24,
      "text": "analysis right and we're going you know",
      "timestamp": "21:55"
    },
    {
      "start": 1317.84,
      "duration": 2.56,
      "text": "even higher thousand 10,000 to one and",
      "timestamp": "21:57"
    },
    {
      "start": 1320.4,
      "duration": 2.159,
      "text": "hundreds of thousands of tokens right so",
      "timestamp": "22:00"
    },
    {
      "start": 1322.559,
      "duration": 1.921,
      "text": "As it stands today we have our bus our",
      "timestamp": "22:02"
    },
    {
      "start": 1324.48,
      "duration": 2.16,
      "text": "private jet but if we want long context",
      "timestamp": "22:04"
    },
    {
      "start": 1326.64,
      "duration": 2.56,
      "text": "it's um going to be expensive so they're",
      "timestamp": "22:06"
    },
    {
      "start": 1329.2,
      "duration": 2.08,
      "text": "going to be a launch vehicle practically",
      "timestamp": "22:09"
    },
    {
      "start": 1331.28,
      "duration": 2.48,
      "text": "um and you can see the economics right",
      "timestamp": "22:11"
    },
    {
      "start": 1333.76,
      "duration": 2.88,
      "text": "so you know what do we do right well",
      "timestamp": "22:13"
    },
    {
      "start": 1336.64,
      "duration": 2.48,
      "text": "Deepseek figured this out right and",
      "timestamp": "22:16"
    },
    {
      "start": 1339.12,
      "duration": 1.679,
      "text": "they're able to create a lot of",
      "timestamp": "22:19"
    },
    {
      "start": 1340.799,
      "duration": 2.641,
      "text": "optimizations to get us uh on this curve",
      "timestamp": "22:20"
    },
    {
      "start": 1343.44,
      "duration": 1.76,
      "text": "and this is the curve I was mentioning",
      "timestamp": "22:23"
    },
    {
      "start": 1345.2,
      "duration": 3.599,
      "text": "to Valund times lower in two years and",
      "timestamp": "22:25"
    },
    {
      "start": 1348.799,
      "duration": 2.161,
      "text": "um you know this will continue right",
      "timestamp": "22:28"
    },
    {
      "start": 1350.96,
      "duration": 2.079,
      "text": "we'll keep marching down the various",
      "timestamp": "22:30"
    },
    {
      "start": 1353.039,
      "duration": 3.361,
      "text": "optimizations this will hold true and as",
      "timestamp": "22:33"
    },
    {
      "start": 1356.4,
      "duration": 2.0,
      "text": "I mentioned to Val earlier and you know",
      "timestamp": "22:36"
    },
    {
      "start": 1358.4,
      "duration": 2.56,
      "text": "um Jevon's paradoxes at work when price",
      "timestamp": "22:38"
    },
    {
      "start": 1360.96,
      "duration": 2.719,
      "text": "goes down empirically demand goes up so",
      "timestamp": "22:40"
    },
    {
      "start": 1363.679,
      "duration": 1.921,
      "text": "what this chart shows here is this is",
      "timestamp": "22:43"
    },
    {
      "start": 1365.6,
      "duration": 2.16,
      "text": "when deepseek v3 came out and what",
      "timestamp": "22:45"
    },
    {
      "start": 1367.76,
      "duration": 1.919,
      "text": "happened in the next month prices shot",
      "timestamp": "22:47"
    },
    {
      "start": 1369.679,
      "duration": 1.601,
      "text": "up as everyone rushed to implement",
      "timestamp": "22:49"
    },
    {
      "start": 1371.28,
      "duration": 1.44,
      "text": "deepseeek it's one of the most popular",
      "timestamp": "22:51"
    },
    {
      "start": 1372.72,
      "duration": 1.76,
      "text": "inference providers i already mentioned",
      "timestamp": "22:52"
    },
    {
      "start": 1374.48,
      "duration": 1.92,
      "text": "double quarter and quarter and another",
      "timestamp": "22:54"
    },
    {
      "start": 1376.4,
      "duration": 3.6,
      "text": "50% um in the in the the second quarter",
      "timestamp": "22:56"
    },
    {
      "start": 1380.0,
      "duration": 1.6,
      "text": "and this is what happened insatiable",
      "timestamp": "23:00"
    },
    {
      "start": 1381.6,
      "duration": 1.439,
      "text": "demand for inference and price have",
      "timestamp": "23:01"
    },
    {
      "start": 1383.039,
      "duration": 1.681,
      "text": "stabilized on the NeoCloud but they're",
      "timestamp": "23:03"
    },
    {
      "start": 1384.72,
      "duration": 1.6,
      "text": "still going to you know gradually",
      "timestamp": "23:04"
    },
    {
      "start": 1386.32,
      "duration": 2.88,
      "text": "decline as the new systems come out um",
      "timestamp": "23:06"
    },
    {
      "start": 1389.2,
      "duration": 2.4,
      "text": "and um you know I'll turn over to Val to",
      "timestamp": "23:09"
    },
    {
      "start": 1391.6,
      "duration": 1.36,
      "text": "talk a little bit about how we keep",
      "timestamp": "23:11"
    },
    {
      "start": 1392.96,
      "duration": 2.16,
      "text": "marching lower in cost per token there's",
      "timestamp": "23:12"
    },
    {
      "start": 1395.12,
      "duration": 2.16,
      "text": "always a set of trade-offs to make but",
      "timestamp": "23:15"
    },
    {
      "start": 1397.28,
      "duration": 1.92,
      "text": "you can certainly make better trade-offs",
      "timestamp": "23:17"
    },
    {
      "start": 1399.2,
      "duration": 2.8,
      "text": "right you you can improve a situation",
      "timestamp": "23:19"
    },
    {
      "start": 1402.0,
      "duration": 1.679,
      "text": "the technology behind what I showed",
      "timestamp": "23:22"
    },
    {
      "start": 1403.679,
      "duration": 2.641,
      "text": "earlier on that 5 to 15 minutes of cache",
      "timestamp": "23:23"
    },
    {
      "start": 1406.32,
      "duration": 1.839,
      "text": "is actually well illustrated by Nvidia",
      "timestamp": "23:26"
    },
    {
      "start": 1408.159,
      "duration": 1.841,
      "text": "in a recent blog post when they launched",
      "timestamp": "23:28"
    },
    {
      "start": 1410.0,
      "duration": 2.4,
      "text": "their project Dynamo but it shows that",
      "timestamp": "23:30"
    },
    {
      "start": 1412.4,
      "duration": 2.96,
      "text": "we have GPUs in the old days over on the",
      "timestamp": "23:32"
    },
    {
      "start": 1415.36,
      "duration": 2.319,
      "text": "left trying to do too much trying to",
      "timestamp": "23:35"
    },
    {
      "start": 1417.679,
      "duration": 2.24,
      "text": "have different kinds of performance some",
      "timestamp": "23:37"
    },
    {
      "start": 1419.919,
      "duration": 1.841,
      "text": "that stresses the processing power of",
      "timestamp": "23:39"
    },
    {
      "start": 1421.76,
      "duration": 2.24,
      "text": "the GPU and then quickly after you do",
      "timestamp": "23:41"
    },
    {
      "start": 1424.0,
      "duration": 1.36,
      "text": "this thing called prefill you have to",
      "timestamp": "23:44"
    },
    {
      "start": 1425.36,
      "duration": 2.08,
      "text": "decode where you actually generate the",
      "timestamp": "23:45"
    },
    {
      "start": 1427.44,
      "duration": 1.44,
      "text": "reasoning tokens and especially the",
      "timestamp": "23:47"
    },
    {
      "start": 1428.88,
      "duration": 1.919,
      "text": "output tokens that's very memory",
      "timestamp": "23:48"
    },
    {
      "start": 1430.799,
      "duration": 1.521,
      "text": "bandwidth oriented it's not as",
      "timestamp": "23:50"
    },
    {
      "start": 1432.32,
      "duration": 1.92,
      "text": "processing heavy and having the same",
      "timestamp": "23:52"
    },
    {
      "start": 1434.24,
      "duration": 2.72,
      "text": "GPUs do that work simultaneously it's",
      "timestamp": "23:54"
    },
    {
      "start": 1436.96,
      "duration": 2.16,
      "text": "very inefficient deepse seek did away",
      "timestamp": "23:56"
    },
    {
      "start": 1439.12,
      "duration": 2.64,
      "text": "with that and one of the innovations the",
      "timestamp": "23:59"
    },
    {
      "start": 1441.76,
      "duration": 1.6,
      "text": "open innovations that everyone is",
      "timestamp": "24:01"
    },
    {
      "start": 1443.36,
      "duration": 2.16,
      "text": "adopting right now is something called",
      "timestamp": "24:03"
    },
    {
      "start": 1445.52,
      "duration": 2.08,
      "text": "disagregated a lot of lot of syllables",
      "timestamp": "24:05"
    },
    {
      "start": 1447.6,
      "duration": 3.439,
      "text": "there so we shorten it to disag prefill",
      "timestamp": "24:07"
    },
    {
      "start": 1451.039,
      "duration": 2.24,
      "text": "and that's separating the workload so",
      "timestamp": "24:11"
    },
    {
      "start": 1453.279,
      "duration": 2.4,
      "text": "you can have a cluster of GPUs that",
      "timestamp": "24:13"
    },
    {
      "start": 1455.679,
      "duration": 2.401,
      "text": "focus just on the comput intensive part",
      "timestamp": "24:15"
    },
    {
      "start": 1458.08,
      "duration": 1.92,
      "text": "so Nvidia for example would like that to",
      "timestamp": "24:18"
    },
    {
      "start": 1460.0,
      "duration": 1.76,
      "text": "be blackwell and then you have a",
      "timestamp": "24:20"
    },
    {
      "start": 1461.76,
      "duration": 2.0,
      "text": "different class of GPUs on the same",
      "timestamp": "24:21"
    },
    {
      "start": 1463.76,
      "duration": 2.0,
      "text": "network that are focused on the decode",
      "timestamp": "24:23"
    },
    {
      "start": 1465.76,
      "duration": 1.36,
      "text": "part that's more memory bandwidth",
      "timestamp": "24:25"
    },
    {
      "start": 1467.12,
      "duration": 1.919,
      "text": "intensive and that can be older prior",
      "timestamp": "24:27"
    },
    {
      "start": 1469.039,
      "duration": 2.24,
      "text": "generations of GPUs and as you do your",
      "timestamp": "24:29"
    },
    {
      "start": 1471.279,
      "duration": 1.921,
      "text": "depreciation and you figure out your",
      "timestamp": "24:31"
    },
    {
      "start": 1473.2,
      "duration": 2.0,
      "text": "overall cost of capital uh and you",
      "timestamp": "24:33"
    },
    {
      "start": 1475.2,
      "duration": 1.52,
      "text": "figure out how you want to optimize at",
      "timestamp": "24:35"
    },
    {
      "start": 1476.72,
      "duration": 2.319,
      "text": "scale you now can combine these two",
      "timestamp": "24:36"
    },
    {
      "start": 1479.039,
      "duration": 2.081,
      "text": "within the same inference workload and",
      "timestamp": "24:39"
    },
    {
      "start": 1481.12,
      "duration": 1.52,
      "text": "really get each one to optimize what",
      "timestamp": "24:41"
    },
    {
      "start": 1482.64,
      "duration": 2.24,
      "text": "they're good for but you still have that",
      "timestamp": "24:42"
    },
    {
      "start": 1484.88,
      "duration": 1.919,
      "text": "really troublesome five to 15 minute",
      "timestamp": "24:44"
    },
    {
      "start": 1486.799,
      "duration": 1.921,
      "text": "window bottleneck and what that means",
      "timestamp": "24:46"
    },
    {
      "start": 1488.72,
      "duration": 2.24,
      "text": "when you run out is you have to go back",
      "timestamp": "24:48"
    },
    {
      "start": 1490.96,
      "duration": 3.92,
      "text": "and burn 10,000 watts per user every 15",
      "timestamp": "24:50"
    },
    {
      "start": 1494.88,
      "duration": 2.24,
      "text": "minutes or more just to go back to",
      "timestamp": "24:54"
    },
    {
      "start": 1497.12,
      "duration": 2.159,
      "text": "prefill so this is incredibly",
      "timestamp": "24:57"
    },
    {
      "start": 1499.279,
      "duration": 1.601,
      "text": "inefficient and the best way to think of",
      "timestamp": "24:59"
    },
    {
      "start": 1500.88,
      "duration": 3.279,
      "text": "it is that inference today hasn't had",
      "timestamp": "25:00"
    },
    {
      "start": 1504.159,
      "duration": 3.281,
      "text": "its model T moment yet inference today",
      "timestamp": "25:04"
    },
    {
      "start": 1507.44,
      "duration": 2.4,
      "text": "still doesn't use assembly lines in",
      "timestamp": "25:07"
    },
    {
      "start": 1509.84,
      "duration": 2.16,
      "text": "these multi10 billion dollar AI",
      "timestamp": "25:09"
    },
    {
      "start": 1512.0,
      "duration": 3.039,
      "text": "factories which is crazy so a year ago",
      "timestamp": "25:12"
    },
    {
      "start": 1515.039,
      "duration": 2.161,
      "text": "DeepSeek released pricing based on this",
      "timestamp": "25:15"
    },
    {
      "start": 1517.2,
      "duration": 3.2,
      "text": "technology of theirs that's open now and",
      "timestamp": "25:17"
    },
    {
      "start": 1520.4,
      "duration": 1.759,
      "text": "about three months ago they finally",
      "timestamp": "25:20"
    },
    {
      "start": 1522.159,
      "duration": 1.841,
      "text": "finally published the white paper behind",
      "timestamp": "25:22"
    },
    {
      "start": 1524.0,
      "duration": 2.32,
      "text": "it but you see at the bottom between",
      "timestamp": "25:24"
    },
    {
      "start": 1526.32,
      "duration": 2.239,
      "text": "that compute intensive prefill on the",
      "timestamp": "25:26"
    },
    {
      "start": 1528.559,
      "duration": 2.641,
      "text": "left and that memory intensive decode on",
      "timestamp": "25:28"
    },
    {
      "start": 1531.2,
      "duration": 2.079,
      "text": "the right you see this assembly line",
      "timestamp": "25:31"
    },
    {
      "start": 1533.279,
      "duration": 2.561,
      "text": "this key value cache that for",
      "timestamp": "25:33"
    },
    {
      "start": 1535.84,
      "duration": 2.88,
      "text": "performance reasons DeepS was able to",
      "timestamp": "25:35"
    },
    {
      "start": 1538.72,
      "duration": 2.64,
      "text": "migrate down to storage so now we",
      "timestamp": "25:38"
    },
    {
      "start": 1541.36,
      "duration": 1.6,
      "text": "revisit the triad how can we improve",
      "timestamp": "25:41"
    },
    {
      "start": 1542.96,
      "duration": 2.48,
      "text": "this with faster processors with more",
      "timestamp": "25:42"
    },
    {
      "start": 1545.44,
      "duration": 2.4,
      "text": "memory bandwidth with more processing",
      "timestamp": "25:45"
    },
    {
      "start": 1547.84,
      "duration": 2.8,
      "text": "power with more demands GPUs are melting",
      "timestamp": "25:47"
    },
    {
      "start": 1550.64,
      "duration": 2.32,
      "text": "agentic applications how do we improve",
      "timestamp": "25:50"
    },
    {
      "start": 1552.96,
      "duration": 2.56,
      "text": "upon this well we use a formula so",
      "timestamp": "25:52"
    },
    {
      "start": 1555.52,
      "duration": 1.68,
      "text": "DeepSc I think was the first to really",
      "timestamp": "25:55"
    },
    {
      "start": 1557.2,
      "duration": 2.8,
      "text": "document this formula XPYD and now we",
      "timestamp": "25:57"
    },
    {
      "start": 1560.0,
      "duration": 2.24,
      "text": "see the open-source community adopting",
      "timestamp": "26:00"
    },
    {
      "start": 1562.24,
      "duration": 1.84,
      "text": "it really really widely right now so the",
      "timestamp": "26:02"
    },
    {
      "start": 1564.08,
      "duration": 2.32,
      "text": "VLM community and Nvidia software",
      "timestamp": "26:04"
    },
    {
      "start": 1566.4,
      "duration": 4.08,
      "text": "developers are all adopting it and XBYD",
      "timestamp": "26:06"
    },
    {
      "start": 1570.48,
      "duration": 2.079,
      "text": "is as simple as count the number of",
      "timestamp": "26:10"
    },
    {
      "start": 1572.559,
      "duration": 3.041,
      "text": "prefills that's your X count the number",
      "timestamp": "26:12"
    },
    {
      "start": 1575.6,
      "duration": 2.72,
      "text": "of decodes that's your Y and in a",
      "timestamp": "26:15"
    },
    {
      "start": 1578.32,
      "duration": 2.4,
      "text": "perfect world you want to get as close",
      "timestamp": "26:18"
    },
    {
      "start": 1580.72,
      "duration": 2.319,
      "text": "to just one prefill as possible because",
      "timestamp": "26:20"
    },
    {
      "start": 1583.039,
      "duration": 2.401,
      "text": "once you've got the key values for your",
      "timestamp": "26:23"
    },
    {
      "start": 1585.44,
      "duration": 2.4,
      "text": "tokens in theory you shouldn't have to",
      "timestamp": "26:25"
    },
    {
      "start": 1587.84,
      "duration": 2.0,
      "text": "recomputee them",
      "timestamp": "26:27"
    },
    {
      "start": 1589.84,
      "duration": 1.839,
      "text": "and if you break through this memory",
      "timestamp": "26:29"
    },
    {
      "start": 1591.679,
      "duration": 1.6,
      "text": "barrier",
      "timestamp": "26:31"
    },
    {
      "start": 1593.279,
      "duration": 1.681,
      "text": "then you're able to actually get to that",
      "timestamp": "26:33"
    },
    {
      "start": 1594.96,
      "duration": 2.319,
      "text": "ideal world of one prefill for an",
      "timestamp": "26:34"
    },
    {
      "start": 1597.279,
      "duration": 2.321,
      "text": "infinite amount of decodes and we're",
      "timestamp": "26:37"
    },
    {
      "start": 1599.6,
      "duration": 2.8,
      "text": "we're down to a hierarchy here that uh",
      "timestamp": "26:39"
    },
    {
      "start": 1602.4,
      "duration": 1.68,
      "text": "is an interesting hierarchy because I",
      "timestamp": "26:42"
    },
    {
      "start": 1604.08,
      "duration": 2.079,
      "text": "like to say it's a little bit deceiving",
      "timestamp": "26:44"
    },
    {
      "start": 1606.159,
      "duration": 2.0,
      "text": "in order to get the performance that you",
      "timestamp": "26:46"
    },
    {
      "start": 1608.159,
      "duration": 1.841,
      "text": "want particularly if you're not using",
      "timestamp": "26:48"
    },
    {
      "start": 1610.0,
      "duration": 2.48,
      "text": "older processors or or D-tuned",
      "timestamp": "26:50"
    },
    {
      "start": 1612.48,
      "duration": 1.52,
      "text": "processors like Deep Seekers is forced",
      "timestamp": "26:52"
    },
    {
      "start": 1614.0,
      "duration": 2.4,
      "text": "to use but modern ones you need to be at",
      "timestamp": "26:54"
    },
    {
      "start": 1616.4,
      "duration": 1.759,
      "text": "that memory class of performance you",
      "timestamp": "26:56"
    },
    {
      "start": 1618.159,
      "duration": 1.441,
      "text": "need to be in those top two tiers of",
      "timestamp": "26:58"
    },
    {
      "start": 1619.6,
      "duration": 2.88,
      "text": "this pyramid\n so to run your your private",
      "timestamp": "26:59"
    },
    {
      "start": 1622.48,
      "duration": 1.76,
      "text": "jet or your bus right you need to have",
      "timestamp": "27:02"
    },
    {
      "start": 1624.24,
      "duration": 1.919,
      "text": "it the infrastructure right your airport",
      "timestamp": "27:04"
    },
    {
      "start": 1626.159,
      "duration": 1.52,
      "text": "your bus station you know everything you",
      "timestamp": "27:06"
    },
    {
      "start": 1627.679,
      "duration": 1.521,
      "text": "need up there right\n yeah and we should",
      "timestamp": "27:07"
    },
    {
      "start": 1629.2,
      "duration": 1.839,
      "text": "have drawn it but you actually want you",
      "timestamp": "27:09"
    },
    {
      "start": 1631.039,
      "duration": 1.681,
      "text": "know certainly the private jet at the",
      "timestamp": "27:11"
    },
    {
      "start": 1632.72,
      "duration": 1.92,
      "text": "top but the bus doesn't go to the bottom",
      "timestamp": "27:12"
    },
    {
      "start": 1634.64,
      "duration": 2.88,
      "text": "here the bus with modern processors goes",
      "timestamp": "27:14"
    },
    {
      "start": 1637.52,
      "duration": 2.159,
      "text": "actually at the host memory layer and if",
      "timestamp": "27:17"
    },
    {
      "start": 1639.679,
      "duration": 1.6,
      "text": "you go to your Amazon cart and you want",
      "timestamp": "27:19"
    },
    {
      "start": 1641.279,
      "duration": 2.161,
      "text": "to buy memory dim and you want to buy a",
      "timestamp": "27:21"
    },
    {
      "start": 1643.44,
      "duration": 2.719,
      "text": "enterprise class NVME drive you'll see",
      "timestamp": "27:23"
    },
    {
      "start": 1646.159,
      "duration": 1.52,
      "text": "that there's about a 20 times price",
      "timestamp": "27:26"
    },
    {
      "start": 1647.679,
      "duration": 1.441,
      "text": "difference the markets already value",
      "timestamp": "27:27"
    },
    {
      "start": 1649.12,
      "duration": 2.48,
      "text": "these things differently so with getting",
      "timestamp": "27:29"
    },
    {
      "start": 1651.6,
      "duration": 1.84,
      "text": "this right getting assembly lines",
      "timestamp": "27:31"
    },
    {
      "start": 1653.44,
      "duration": 2.08,
      "text": "implemented in inference is all about is",
      "timestamp": "27:33"
    },
    {
      "start": 1655.52,
      "duration": 2.32,
      "text": "improving and making better tradeoffs",
      "timestamp": "27:35"
    },
    {
      "start": 1657.84,
      "duration": 1.6,
      "text": "you get new performance frontiers and",
      "timestamp": "27:37"
    },
    {
      "start": 1659.44,
      "duration": 2.96,
      "text": "the cool thing is you go from you know a",
      "timestamp": "27:39"
    },
    {
      "start": 1662.4,
      "duration": 2.159,
      "text": "traditional sweet spot on the Hopper",
      "timestamp": "27:42"
    },
    {
      "start": 1664.559,
      "duration": 2.641,
      "text": "processors a traditional or a new sweet",
      "timestamp": "27:44"
    },
    {
      "start": 1667.2,
      "duration": 2.56,
      "text": "spot on the Blackwell processors you're",
      "timestamp": "27:47"
    },
    {
      "start": 1669.76,
      "duration": 2.72,
      "text": "able now to get your old processors to",
      "timestamp": "27:49"
    },
    {
      "start": 1672.48,
      "duration": 2.319,
      "text": "perform like the modern ones just",
      "timestamp": "27:52"
    },
    {
      "start": 1674.799,
      "duration": 2.561,
      "text": "through software inference software\n i",
      "timestamp": "27:54"
    },
    {
      "start": 1677.36,
      "duration": 1.52,
      "text": "think it's important with this chart Val",
      "timestamp": "27:57"
    },
    {
      "start": 1678.88,
      "duration": 2.0,
      "text": "right because remember I I I said it was",
      "timestamp": "27:58"
    },
    {
      "start": 1680.88,
      "duration": 1.76,
      "text": "a trade-off right but this is a shift in",
      "timestamp": "28:00"
    },
    {
      "start": 1682.64,
      "duration": 1.68,
      "text": "the curve right so it's still the bus",
      "timestamp": "28:02"
    },
    {
      "start": 1684.32,
      "duration": 1.599,
      "text": "but we sort of just put rocket engines",
      "timestamp": "28:04"
    },
    {
      "start": 1685.919,
      "duration": 1.36,
      "text": "on the bus right and shift the entire",
      "timestamp": "28:05"
    },
    {
      "start": 1687.279,
      "duration": 2.161,
      "text": "curve out right you can do more with",
      "timestamp": "28:07"
    },
    {
      "start": 1689.44,
      "duration": 1.52,
      "text": "less and that's what you know a lot of",
      "timestamp": "28:09"
    },
    {
      "start": 1690.96,
      "duration": 1.12,
      "text": "the work that a lot of these folks are",
      "timestamp": "28:10"
    },
    {
      "start": 1692.08,
      "duration": 1.839,
      "text": "doing with Dynamo and uh AB cache",
      "timestamp": "28:12"
    },
    {
      "start": 1693.919,
      "duration": 2.321,
      "text": "offload\n exactly semi analysis is",
      "timestamp": "28:13"
    },
    {
      "start": 1696.24,
      "duration": 1.2,
      "text": "actually going to be benchmarking this",
      "timestamp": "28:16"
    },
    {
      "start": 1697.44,
      "duration": 1.52,
      "text": "you know at an upcoming report in the",
      "timestamp": "28:17"
    },
    {
      "start": 1698.96,
      "duration": 1.92,
      "text": "future so it's not just a vendor",
      "timestamp": "28:18"
    },
    {
      "start": 1700.88,
      "duration": 1.84,
      "text": "claiming it it's objective benchmarking",
      "timestamp": "28:20"
    },
    {
      "start": 1702.72,
      "duration": 1.679,
      "text": "that's going to prove\n new new graphs for",
      "timestamp": "28:22"
    },
    {
      "start": 1704.399,
      "duration": 2.88,
      "text": "you soon hopefully\n exactly and so what",
      "timestamp": "28:24"
    },
    {
      "start": 1707.279,
      "duration": 1.921,
      "text": "this really means for the industry it's",
      "timestamp": "28:27"
    },
    {
      "start": 1709.2,
      "duration": 3.92,
      "text": "profound we go from three classes of",
      "timestamp": "28:29"
    },
    {
      "start": 1713.12,
      "duration": 1.6,
      "text": "pricing",
      "timestamp": "28:33"
    },
    {
      "start": 1714.72,
      "duration": 2.0,
      "text": "collapse an entire major class of",
      "timestamp": "28:34"
    },
    {
      "start": 1716.72,
      "duration": 1.76,
      "text": "pricing in the industry and down to just",
      "timestamp": "28:36"
    },
    {
      "start": 1718.48,
      "duration": 2.799,
      "text": "two no more need for separate input",
      "timestamp": "28:38"
    },
    {
      "start": 1721.279,
      "duration": 3.041,
      "text": "token costs all input processing is now",
      "timestamp": "28:41"
    },
    {
      "start": 1724.32,
      "duration": 2.239,
      "text": "cash processing depending on whether you",
      "timestamp": "28:44"
    },
    {
      "start": 1726.559,
      "duration": 1.441,
      "text": "want it for weeks or months we don't",
      "timestamp": "28:46"
    },
    {
      "start": 1728.0,
      "duration": 1.919,
      "text": "think people or apps or developers will",
      "timestamp": "28:48"
    },
    {
      "start": 1729.919,
      "duration": 2.0,
      "text": "want more than that but it's just input",
      "timestamp": "28:49"
    },
    {
      "start": 1731.919,
      "duration": 2.64,
      "text": "and output now no need for separate cash",
      "timestamp": "28:51"
    },
    {
      "start": 1734.559,
      "duration": 2.24,
      "text": "input pricing so if you want to find out",
      "timestamp": "28:54"
    },
    {
      "start": 1736.799,
      "duration": 1.921,
      "text": "more feel free to come and approach us",
      "timestamp": "28:56"
    },
    {
      "start": 1738.72,
      "duration": 1.76,
      "text": "and ask questions and I'll get out of",
      "timestamp": "28:58"
    },
    {
      "start": 1740.48,
      "duration": 1.199,
      "text": "your way so you can take a picture of",
      "timestamp": "29:00"
    },
    {
      "start": 1741.679,
      "duration": 1.921,
      "text": "this QR code and we can get you more",
      "timestamp": "29:01"
    },
    {
      "start": 1743.6,
      "duration": 1.52,
      "text": "detailed information but thank you very",
      "timestamp": "29:03"
    },
    {
      "start": 1745.12,
      "duration": 3.48,
      "text": "much for your time",
      "timestamp": "29:05"
    }
  ],
  "extraction_timestamp": "2025-06-29T21:04:35.359405",
  "playlist_title": "SuperAI Singapore 2025: WEKA Stage"
}