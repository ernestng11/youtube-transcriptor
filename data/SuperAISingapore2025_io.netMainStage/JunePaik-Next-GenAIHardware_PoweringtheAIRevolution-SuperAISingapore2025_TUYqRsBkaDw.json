{
  "video_id": "TUYqRsBkaDw",
  "video_title": "June Paik - Next-Gen AI Hardware: Powering the AI Revolution - SuperAI Singapore 2025",
  "video_url": "https://www.youtube.com/watch?v=TUYqRsBkaDw",
  "channel_title": "SuperAI",
  "published_at": "2025-06-27T07:51:05+00:00",
  "duration_seconds": null,
  "view_count": 14,
  "like_count": 0,
  "description": "Learn more about SuperAI: superai.com\nFollow us on X: x.com/superai_conf\n\nKeynote: Next-Gen AI Hardware: Powering the AI Revolution\n\nSpeaker:\nJune Paik, Founder and CEO @ FuriosaAI\n\nStage: io.net Main Stage\n#superai #furiosaai #aihardware #ai #llms \n\nRecorded on 18 June 2025",
  "transcript": {
    "language": "en",
    "is_auto_generated": false,
    "total_segments": 213,
    "aggregated_text": "my name is Jun uh I am CEO and co-founder of Furious AI uh and we are Furious AI is uh building a silicon chip uh for AI uh especially with a strong focus on this uh uh inferencing in the data center uh infrastructure so uh AI is a very uh consequential technology it's about to become far more pervasive than it's today embedded everywhere in everything we are right now talking about agents are eating the world we are witnessing a new era of a very powerful AI applications powered by frontier models like GPT uh like llama and deepc who can do even reasoning these days so um these rapidly evolving applications uh are driving an unprecedented demand for compute especially for uh inference just recently uh Sam Elman kind of tweeted that their GPUs are melting from the load of generating the kibini style images just want those simple application put so much pressure on their infrastructure but now imagine the scale of uh compute we will need going forward so here the question is is our infrastructure ready um the the the energy consumption in data centers are really spiking right now uh these days right now we are talking about gigawatt scale data centers gawatt actually means uh you need like one nuclear power plant and uh like right right now we are talking about building like a 20 30 even more gigawatt scale uh data centers uh globally uh so this uh also this energy cost is actually fundamental cost of running AI a lot of AI companies are actually losing uh basically money due to this uh infrastructure cost so our mission uh is very clear uh so we uh our mission is to make AI computing sustainable uh and by sustainable we mean not only like uh we don't mean just the environmental sustainability we also mean the business sustainability as I mentioned u um the uh the cost of AI uh is related to highly related to energy cost and if you basically don't make a money out of uh doing this AI service the business model is not basically sustainable so just the world now must shift from the gasoline to electric vehicle uh we need the same kind of a transformation in in AI computing toward a more sustainable and energy efficient future um so quite similar to the shift from gasoline car to electric car with a new leap uh in technology and very similar uh from uh to the evolution from the CPU to the CPU uh we need uh a breakthrough product and fundamentally new approach uh in uh in AI computing very first principle approach to tackle uh uh this vision so uh here comes our renegade our second gen product uh I brought the the real chip here actually uh we launched this product uh actually last year uh if you look at the uh AI chip there are three important specs probably you you remember one is the what's the compute performance uh like u we are delivering 512 teraflops of performance and what is the the second thing is your memory performance uh and um right now we can we are capable of loading data at like 1.5 terabyte per second also The third important spec is then what's your energy uh to uh deliver this total like horsepower actually so uh right now this chip is consuming 180 watt under 200 compared to our competing GPU product which is uh usually consuming over uh 300 watt actually uh if you save just one watt uh it translate to $20 saving per year so uh if you save 100 watt uh you can save $2,000 per year if you operate uh this chip for let's say five years it translate just single chip can save you $10,000 per year and also imagine that you are uh deploying tons of these chips through your data center actually uh this is a huge kind of costsaving as well uh to enable this frontier uh kind of product we are definitely working with worldclass partners uh right now this chip is fabricated on TSMC using 5 nanometer technology and this chip is almost a packing 40 billion transistors inside we are also working with uh uh SKH highix to use the latest uh one of the uh the most advanced GM technology uh and we are using also the most advanced packaging technology as well uh to uh deliver uh this product um but the most important thing the core technology is like not only is the spec but like what how we make this uh achieve this energy efficiency this is all comes from our fundamental innovation in our chip architecture and uh uh and our software stack on top of it so we call our chip architecture as a tensor contraction processor uh it's quite a jargon but um I can say the tensor in simply speaking here tensor contraction and the tensor means like a multi-dimensional large scale matrix multiplication uh kind of it means the tensor conction means those large scale matrix multiplication so our processor uh is uh quite designed to run this large scale tensor uh you know you know as a fast as possible and at the same time as a energy efficient way as a possible and uh the challenge of AI chip design also is that AI workloads are quickly changing uh it's not like a mining algorithm mining algorithm is just a fixed right you can just optimize for fixed algorithm but AI workloads are advancing so quickly so the your design should have some flexibility good enough flexibility to accommodate all the changes happening on like an an AI innovation so uh our innovation focus is that we achieve a good sweet spot of being generalized for many tensorbased workload yet specializ ize enough to exploit many parallelism in AI accelerations so you can read our technical paper if you're more interesting so what's the our benchmark so uh there is a very uh kind of golden kind of uh metric to measure the efficiency and performance of AI model these days which is actually tokens per second per watt um so uh when you measure our token per second per watt against the kind of competing GPU products we are already our benchmarking already demonstrate we achieve almost 190% greater efficiency compared to GPU it's why now we are sampling with many uh quite good uh customers already globally uh they are uh there are many enterprise looking for alternative to the current GPU products and it's why they are right now testing our chip and uh thinking of adopting our chip for their future inference platforms so I'd like to briefly also uh introduce our company uh uh we started as a startup company eight years ago uh with the three co-founders including me uh my career was all uh about the semiconductor and the electrical engineering uh and when you started in 201 uh we actually had a high uh kind of belief about the future of AI uh it is a very uh transformative uh technology and uh from the very beginning we set out to build high performance in energy efficiency designed for AI at scale um and uh after our first flagship product uh actually we decided to take a very bold bet uh designing a large scale SOC for a massive language models even though at that time the language models were still uh quite experimental at that time so it was quite risky move for a startup but we believed in the future this model would would bring uh to uh to the u to the to the world actually uh because uh we still have the same belief that the more capable AI will bring the more value to the people and uh um to realize the future uh and uh uh we definitely need u uh quite sustainable AI infrastructure as well so uh uh I hope um uh like uh if you anyone who like to need a strong efficient AI compute solution whether in cloud and especially also if you like to own your own like on premise like your own AI computing infrastructure uh we hope we can uh work with your team and we are very committed to support uh that uh building this new AI infrastructure together uh with you guys as well so thank you very much",
    "text_length": 7677,
    "word_count": 1430
  },
  "segments": [
    {
      "start": 7.44,
      "duration": 2.8,
      "text": "my name is Jun uh I am CEO and",
      "timestamp": "00:07"
    },
    {
      "start": 10.24,
      "duration": 4.08,
      "text": "co-founder of Furious AI uh and we are",
      "timestamp": "00:10"
    },
    {
      "start": 14.32,
      "duration": 4.24,
      "text": "Furious AI is uh building a silicon chip",
      "timestamp": "00:14"
    },
    {
      "start": 18.56,
      "duration": 3.92,
      "text": "uh for AI uh especially with a strong",
      "timestamp": "00:18"
    },
    {
      "start": 22.48,
      "duration": 4.08,
      "text": "focus on this uh uh inferencing in the",
      "timestamp": "00:22"
    },
    {
      "start": 26.56,
      "duration": 3.44,
      "text": "data center uh infrastructure",
      "timestamp": "00:26"
    },
    {
      "start": 30.0,
      "duration": 4.32,
      "text": "so uh AI is a very uh consequential",
      "timestamp": "00:30"
    },
    {
      "start": 34.32,
      "duration": 4.079,
      "text": "technology it's about to become far more",
      "timestamp": "00:34"
    },
    {
      "start": 38.399,
      "duration": 2.641,
      "text": "pervasive than it's today embedded",
      "timestamp": "00:38"
    },
    {
      "start": 41.04,
      "duration": 2.88,
      "text": "everywhere in everything we are right",
      "timestamp": "00:41"
    },
    {
      "start": 43.92,
      "duration": 3.119,
      "text": "now talking about agents are eating the",
      "timestamp": "00:43"
    },
    {
      "start": 47.039,
      "duration": 3.52,
      "text": "world we are witnessing a new era of a",
      "timestamp": "00:47"
    },
    {
      "start": 50.559,
      "duration": 4.401,
      "text": "very powerful AI applications powered by",
      "timestamp": "00:50"
    },
    {
      "start": 54.96,
      "duration": 3.68,
      "text": "frontier models like GPT uh like llama",
      "timestamp": "00:54"
    },
    {
      "start": 58.64,
      "duration": 2.32,
      "text": "and deepc who can do even reasoning",
      "timestamp": "00:58"
    },
    {
      "start": 60.96,
      "duration": 4.32,
      "text": "these days so um these rapidly evolving",
      "timestamp": "01:00"
    },
    {
      "start": 65.28,
      "duration": 2.96,
      "text": "applications uh are driving an",
      "timestamp": "01:05"
    },
    {
      "start": 68.24,
      "duration": 3.52,
      "text": "unprecedented demand for compute",
      "timestamp": "01:08"
    },
    {
      "start": 71.76,
      "duration": 3.28,
      "text": "especially for uh inference",
      "timestamp": "01:11"
    },
    {
      "start": 75.04,
      "duration": 2.719,
      "text": "just recently uh Sam Elman kind of",
      "timestamp": "01:15"
    },
    {
      "start": 77.759,
      "duration": 3.441,
      "text": "tweeted that their GPUs are melting from",
      "timestamp": "01:17"
    },
    {
      "start": 81.2,
      "duration": 3.68,
      "text": "the load of generating the kibini style",
      "timestamp": "01:21"
    },
    {
      "start": 84.88,
      "duration": 2.48,
      "text": "images just want those simple",
      "timestamp": "01:24"
    },
    {
      "start": 87.36,
      "duration": 2.24,
      "text": "application put so much pressure on",
      "timestamp": "01:27"
    },
    {
      "start": 89.6,
      "duration": 3.12,
      "text": "their infrastructure but now imagine the",
      "timestamp": "01:29"
    },
    {
      "start": 92.72,
      "duration": 3.68,
      "text": "scale of uh compute we will need going",
      "timestamp": "01:32"
    },
    {
      "start": 96.4,
      "duration": 3.679,
      "text": "forward so here the question is is our",
      "timestamp": "01:36"
    },
    {
      "start": 100.079,
      "duration": 4.0,
      "text": "infrastructure ready",
      "timestamp": "01:40"
    },
    {
      "start": 104.079,
      "duration": 4.961,
      "text": "um the the the energy consumption in",
      "timestamp": "01:44"
    },
    {
      "start": 109.04,
      "duration": 2.96,
      "text": "data centers are really spiking right",
      "timestamp": "01:49"
    },
    {
      "start": 112.0,
      "duration": 3.04,
      "text": "now uh these days right now we are",
      "timestamp": "01:52"
    },
    {
      "start": 115.04,
      "duration": 3.439,
      "text": "talking about gigawatt scale data",
      "timestamp": "01:55"
    },
    {
      "start": 118.479,
      "duration": 3.6,
      "text": "centers gawatt actually means uh you",
      "timestamp": "01:58"
    },
    {
      "start": 122.079,
      "duration": 4.161,
      "text": "need like one nuclear power plant and uh",
      "timestamp": "02:02"
    },
    {
      "start": 126.24,
      "duration": 1.519,
      "text": "like right right now we are talking",
      "timestamp": "02:06"
    },
    {
      "start": 127.759,
      "duration": 4.241,
      "text": "about building like a 20 30 even more",
      "timestamp": "02:07"
    },
    {
      "start": 132.0,
      "duration": 2.72,
      "text": "gigawatt scale uh data centers uh",
      "timestamp": "02:12"
    },
    {
      "start": 134.72,
      "duration": 3.2,
      "text": "globally uh so",
      "timestamp": "02:14"
    },
    {
      "start": 137.92,
      "duration": 3.28,
      "text": "this uh also this energy cost is",
      "timestamp": "02:17"
    },
    {
      "start": 141.2,
      "duration": 3.2,
      "text": "actually fundamental cost of running AI",
      "timestamp": "02:21"
    },
    {
      "start": 144.4,
      "duration": 2.72,
      "text": "a lot of AI companies are actually",
      "timestamp": "02:24"
    },
    {
      "start": 147.12,
      "duration": 3.52,
      "text": "losing uh basically money due to this uh",
      "timestamp": "02:27"
    },
    {
      "start": 150.64,
      "duration": 3.92,
      "text": "infrastructure cost",
      "timestamp": "02:30"
    },
    {
      "start": 154.56,
      "duration": 4.56,
      "text": "so our mission uh is very clear uh so we",
      "timestamp": "02:34"
    },
    {
      "start": 159.12,
      "duration": 3.52,
      "text": "uh our mission is to make AI computing",
      "timestamp": "02:39"
    },
    {
      "start": 162.64,
      "duration": 3.679,
      "text": "sustainable uh and by sustainable we",
      "timestamp": "02:42"
    },
    {
      "start": 166.319,
      "duration": 2.64,
      "text": "mean not only like uh we don't mean just",
      "timestamp": "02:46"
    },
    {
      "start": 168.959,
      "duration": 2.481,
      "text": "the environmental sustainability we also",
      "timestamp": "02:48"
    },
    {
      "start": 171.44,
      "duration": 2.64,
      "text": "mean the business sustainability as I",
      "timestamp": "02:51"
    },
    {
      "start": 174.08,
      "duration": 5.12,
      "text": "mentioned u um the uh the cost of AI uh",
      "timestamp": "02:54"
    },
    {
      "start": 179.2,
      "duration": 2.72,
      "text": "is related to highly related to energy",
      "timestamp": "02:59"
    },
    {
      "start": 181.92,
      "duration": 2.8,
      "text": "cost and if you basically don't make a",
      "timestamp": "03:01"
    },
    {
      "start": 184.72,
      "duration": 2.72,
      "text": "money out of uh doing this AI service",
      "timestamp": "03:04"
    },
    {
      "start": 187.44,
      "duration": 2.96,
      "text": "the business model is not basically",
      "timestamp": "03:07"
    },
    {
      "start": 190.4,
      "duration": 1.6,
      "text": "sustainable",
      "timestamp": "03:10"
    },
    {
      "start": 192.0,
      "duration": 3.92,
      "text": "so just the world now must shift from",
      "timestamp": "03:12"
    },
    {
      "start": 195.92,
      "duration": 3.28,
      "text": "the gasoline to electric vehicle uh we",
      "timestamp": "03:15"
    },
    {
      "start": 199.2,
      "duration": 2.72,
      "text": "need the same kind of a transformation",
      "timestamp": "03:19"
    },
    {
      "start": 201.92,
      "duration": 2.399,
      "text": "in in AI computing toward a more",
      "timestamp": "03:21"
    },
    {
      "start": 204.319,
      "duration": 4.961,
      "text": "sustainable and energy efficient future",
      "timestamp": "03:24"
    },
    {
      "start": 209.28,
      "duration": 2.0,
      "text": "um",
      "timestamp": "03:29"
    },
    {
      "start": 211.28,
      "duration": 3.92,
      "text": "so quite similar to the shift from",
      "timestamp": "03:31"
    },
    {
      "start": 215.2,
      "duration": 3.52,
      "text": "gasoline car to electric car with a new",
      "timestamp": "03:35"
    },
    {
      "start": 218.72,
      "duration": 3.92,
      "text": "leap uh in technology and very similar",
      "timestamp": "03:38"
    },
    {
      "start": 222.64,
      "duration": 3.84,
      "text": "uh from uh to the evolution from the CPU",
      "timestamp": "03:42"
    },
    {
      "start": 226.48,
      "duration": 4.96,
      "text": "to the CPU uh we need uh a breakthrough",
      "timestamp": "03:46"
    },
    {
      "start": 231.44,
      "duration": 3.6,
      "text": "product and fundamentally new approach",
      "timestamp": "03:51"
    },
    {
      "start": 235.04,
      "duration": 2.88,
      "text": "uh in uh in AI computing very first",
      "timestamp": "03:55"
    },
    {
      "start": 237.92,
      "duration": 3.28,
      "text": "principle approach to tackle uh uh this",
      "timestamp": "03:57"
    },
    {
      "start": 241.2,
      "duration": 2.8,
      "text": "vision",
      "timestamp": "04:01"
    },
    {
      "start": 244.0,
      "duration": 2.959,
      "text": "so uh here comes our renegade our second",
      "timestamp": "04:04"
    },
    {
      "start": 246.959,
      "duration": 2.64,
      "text": "gen product uh I brought the the real",
      "timestamp": "04:06"
    },
    {
      "start": 249.599,
      "duration": 3.121,
      "text": "chip here actually uh we launched this",
      "timestamp": "04:09"
    },
    {
      "start": 252.72,
      "duration": 3.519,
      "text": "product uh actually last year uh if you",
      "timestamp": "04:12"
    },
    {
      "start": 256.239,
      "duration": 2.641,
      "text": "look at the uh AI chip there are three",
      "timestamp": "04:16"
    },
    {
      "start": 258.88,
      "duration": 2.96,
      "text": "important specs probably you you",
      "timestamp": "04:18"
    },
    {
      "start": 261.84,
      "duration": 3.52,
      "text": "remember one is the what's the compute",
      "timestamp": "04:21"
    },
    {
      "start": 265.36,
      "duration": 3.76,
      "text": "performance uh like u we are delivering",
      "timestamp": "04:25"
    },
    {
      "start": 269.12,
      "duration": 4.16,
      "text": "512 teraflops of performance and what is",
      "timestamp": "04:29"
    },
    {
      "start": 273.28,
      "duration": 1.76,
      "text": "the the second thing is your memory",
      "timestamp": "04:33"
    },
    {
      "start": 275.04,
      "duration": 4.32,
      "text": "performance uh and um right now we can",
      "timestamp": "04:35"
    },
    {
      "start": 279.36,
      "duration": 2.72,
      "text": "we are capable of loading data at like",
      "timestamp": "04:39"
    },
    {
      "start": 282.08,
      "duration": 3.44,
      "text": "1.5 terabyte per second also The third",
      "timestamp": "04:42"
    },
    {
      "start": 285.52,
      "duration": 2.32,
      "text": "important spec is then what's your",
      "timestamp": "04:45"
    },
    {
      "start": 287.84,
      "duration": 3.44,
      "text": "energy uh to uh deliver this total like",
      "timestamp": "04:47"
    },
    {
      "start": 291.28,
      "duration": 3.52,
      "text": "horsepower actually so uh right now this",
      "timestamp": "04:51"
    },
    {
      "start": 294.8,
      "duration": 2.56,
      "text": "chip is consuming",
      "timestamp": "04:54"
    },
    {
      "start": 297.36,
      "duration": 3.36,
      "text": "180 watt under 200 compared to our",
      "timestamp": "04:57"
    },
    {
      "start": 300.72,
      "duration": 2.4,
      "text": "competing GPU product which is uh",
      "timestamp": "05:00"
    },
    {
      "start": 303.12,
      "duration": 4.32,
      "text": "usually consuming over uh 300 watt",
      "timestamp": "05:03"
    },
    {
      "start": 307.44,
      "duration": 4.24,
      "text": "actually uh if you save just one watt uh",
      "timestamp": "05:07"
    },
    {
      "start": 311.68,
      "duration": 5.04,
      "text": "it translate to $20 saving per year so",
      "timestamp": "05:11"
    },
    {
      "start": 316.72,
      "duration": 3.52,
      "text": "uh if you save 100 watt uh you can save",
      "timestamp": "05:16"
    },
    {
      "start": 320.24,
      "duration": 3.679,
      "text": "$2,000 per year if you operate uh this",
      "timestamp": "05:20"
    },
    {
      "start": 323.919,
      "duration": 2.641,
      "text": "chip for let's say five years it",
      "timestamp": "05:23"
    },
    {
      "start": 326.56,
      "duration": 3.12,
      "text": "translate just single chip can save you",
      "timestamp": "05:26"
    },
    {
      "start": 329.68,
      "duration": 3.359,
      "text": "$10,000 per year and also imagine that",
      "timestamp": "05:29"
    },
    {
      "start": 333.039,
      "duration": 2.801,
      "text": "you are uh deploying tons of these chips",
      "timestamp": "05:33"
    },
    {
      "start": 335.84,
      "duration": 2.32,
      "text": "through your data center actually uh",
      "timestamp": "05:35"
    },
    {
      "start": 338.16,
      "duration": 2.72,
      "text": "this is a huge kind of costsaving as",
      "timestamp": "05:38"
    },
    {
      "start": 340.88,
      "duration": 2.159,
      "text": "well",
      "timestamp": "05:40"
    },
    {
      "start": 343.039,
      "duration": 3.121,
      "text": "uh to enable this frontier uh kind of",
      "timestamp": "05:43"
    },
    {
      "start": 346.16,
      "duration": 2.56,
      "text": "product we are definitely working with",
      "timestamp": "05:46"
    },
    {
      "start": 348.72,
      "duration": 3.44,
      "text": "worldclass partners uh right now this",
      "timestamp": "05:48"
    },
    {
      "start": 352.16,
      "duration": 3.68,
      "text": "chip is fabricated on TSMC using 5",
      "timestamp": "05:52"
    },
    {
      "start": 355.84,
      "duration": 3.04,
      "text": "nanometer technology and this chip is",
      "timestamp": "05:55"
    },
    {
      "start": 358.88,
      "duration": 3.28,
      "text": "almost a packing 40 billion transistors",
      "timestamp": "05:58"
    },
    {
      "start": 362.16,
      "duration": 3.28,
      "text": "inside we are also working with uh uh",
      "timestamp": "06:02"
    },
    {
      "start": 365.44,
      "duration": 3.36,
      "text": "SKH highix to use the latest uh one of",
      "timestamp": "06:05"
    },
    {
      "start": 368.8,
      "duration": 4.32,
      "text": "the uh the most advanced GM technology",
      "timestamp": "06:08"
    },
    {
      "start": 373.12,
      "duration": 1.76,
      "text": "uh and we are using also the most",
      "timestamp": "06:13"
    },
    {
      "start": 374.88,
      "duration": 2.48,
      "text": "advanced packaging technology as well uh",
      "timestamp": "06:14"
    },
    {
      "start": 377.36,
      "duration": 4.399,
      "text": "to uh deliver uh this product",
      "timestamp": "06:17"
    },
    {
      "start": 381.759,
      "duration": 3.44,
      "text": "um but the most important thing the core",
      "timestamp": "06:21"
    },
    {
      "start": 385.199,
      "duration": 3.681,
      "text": "technology is like not only is the spec",
      "timestamp": "06:25"
    },
    {
      "start": 388.88,
      "duration": 3.2,
      "text": "but like what how we make this uh",
      "timestamp": "06:28"
    },
    {
      "start": 392.08,
      "duration": 2.559,
      "text": "achieve this energy efficiency this is",
      "timestamp": "06:32"
    },
    {
      "start": 394.639,
      "duration": 1.921,
      "text": "all comes from our fundamental",
      "timestamp": "06:34"
    },
    {
      "start": 396.56,
      "duration": 3.28,
      "text": "innovation in our chip architecture and",
      "timestamp": "06:36"
    },
    {
      "start": 399.84,
      "duration": 2.24,
      "text": "uh uh and our software stack on top of",
      "timestamp": "06:39"
    },
    {
      "start": 402.08,
      "duration": 3.839,
      "text": "it so we call our chip architecture as a",
      "timestamp": "06:42"
    },
    {
      "start": 405.919,
      "duration": 3.041,
      "text": "tensor contraction processor uh it's",
      "timestamp": "06:45"
    },
    {
      "start": 408.96,
      "duration": 3.76,
      "text": "quite a jargon but um I can say the",
      "timestamp": "06:48"
    },
    {
      "start": 412.72,
      "duration": 3.68,
      "text": "tensor in simply speaking here tensor",
      "timestamp": "06:52"
    },
    {
      "start": 416.4,
      "duration": 2.799,
      "text": "contraction and the tensor means like a",
      "timestamp": "06:56"
    },
    {
      "start": 419.199,
      "duration": 3.041,
      "text": "multi-dimensional large scale matrix",
      "timestamp": "06:59"
    },
    {
      "start": 422.24,
      "duration": 2.399,
      "text": "multiplication uh kind of it means the",
      "timestamp": "07:02"
    },
    {
      "start": 424.639,
      "duration": 2.721,
      "text": "tensor conction means those large scale",
      "timestamp": "07:04"
    },
    {
      "start": 427.36,
      "duration": 3.6,
      "text": "matrix multiplication so our processor",
      "timestamp": "07:07"
    },
    {
      "start": 430.96,
      "duration": 3.359,
      "text": "uh is uh quite designed to run this",
      "timestamp": "07:10"
    },
    {
      "start": 434.319,
      "duration": 2.641,
      "text": "large scale tensor uh you know you know",
      "timestamp": "07:14"
    },
    {
      "start": 436.96,
      "duration": 2.16,
      "text": "as a fast as possible and at the same",
      "timestamp": "07:16"
    },
    {
      "start": 439.12,
      "duration": 2.72,
      "text": "time as a energy efficient way as a",
      "timestamp": "07:19"
    },
    {
      "start": 441.84,
      "duration": 3.919,
      "text": "possible and uh the challenge of AI chip",
      "timestamp": "07:21"
    },
    {
      "start": 445.759,
      "duration": 3.84,
      "text": "design also is that AI workloads are",
      "timestamp": "07:25"
    },
    {
      "start": 449.599,
      "duration": 2.481,
      "text": "quickly changing uh it's not like a",
      "timestamp": "07:29"
    },
    {
      "start": 452.08,
      "duration": 2.0,
      "text": "mining algorithm mining algorithm is",
      "timestamp": "07:32"
    },
    {
      "start": 454.08,
      "duration": 1.839,
      "text": "just a fixed right you can just optimize",
      "timestamp": "07:34"
    },
    {
      "start": 455.919,
      "duration": 3.601,
      "text": "for fixed algorithm but AI workloads are",
      "timestamp": "07:35"
    },
    {
      "start": 459.52,
      "duration": 3.84,
      "text": "advancing so quickly so the your design",
      "timestamp": "07:39"
    },
    {
      "start": 463.36,
      "duration": 3.279,
      "text": "should have some flexibility good enough",
      "timestamp": "07:43"
    },
    {
      "start": 466.639,
      "duration": 2.641,
      "text": "flexibility to accommodate all the",
      "timestamp": "07:46"
    },
    {
      "start": 469.28,
      "duration": 2.96,
      "text": "changes happening on like an an AI",
      "timestamp": "07:49"
    },
    {
      "start": 472.24,
      "duration": 3.519,
      "text": "innovation so uh our innovation focus is",
      "timestamp": "07:52"
    },
    {
      "start": 475.759,
      "duration": 2.88,
      "text": "that we achieve a good sweet spot of",
      "timestamp": "07:55"
    },
    {
      "start": 478.639,
      "duration": 2.56,
      "text": "being generalized for many tensorbased",
      "timestamp": "07:58"
    },
    {
      "start": 481.199,
      "duration": 2.961,
      "text": "workload yet specializ ize enough to",
      "timestamp": "08:01"
    },
    {
      "start": 484.16,
      "duration": 2.879,
      "text": "exploit many parallelism in AI",
      "timestamp": "08:04"
    },
    {
      "start": 487.039,
      "duration": 2.401,
      "text": "accelerations so you can read our",
      "timestamp": "08:07"
    },
    {
      "start": 489.44,
      "duration": 1.84,
      "text": "technical paper if you're more",
      "timestamp": "08:09"
    },
    {
      "start": 491.28,
      "duration": 1.759,
      "text": "interesting",
      "timestamp": "08:11"
    },
    {
      "start": 493.039,
      "duration": 3.121,
      "text": "so what's the our benchmark so uh there",
      "timestamp": "08:13"
    },
    {
      "start": 496.16,
      "duration": 3.759,
      "text": "is a very uh kind of golden kind of uh",
      "timestamp": "08:16"
    },
    {
      "start": 499.919,
      "duration": 3.521,
      "text": "metric to measure the efficiency and",
      "timestamp": "08:19"
    },
    {
      "start": 503.44,
      "duration": 3.039,
      "text": "performance of AI model these days which",
      "timestamp": "08:23"
    },
    {
      "start": 506.479,
      "duration": 3.601,
      "text": "is actually tokens per second per watt",
      "timestamp": "08:26"
    },
    {
      "start": 510.08,
      "duration": 2.48,
      "text": "um so",
      "timestamp": "08:30"
    },
    {
      "start": 512.56,
      "duration": 2.399,
      "text": "uh when you measure our token per second",
      "timestamp": "08:32"
    },
    {
      "start": 514.959,
      "duration": 2.801,
      "text": "per watt against the kind of competing",
      "timestamp": "08:34"
    },
    {
      "start": 517.76,
      "duration": 2.639,
      "text": "GPU products we are already our",
      "timestamp": "08:37"
    },
    {
      "start": 520.399,
      "duration": 2.88,
      "text": "benchmarking already demonstrate we",
      "timestamp": "08:40"
    },
    {
      "start": 523.279,
      "duration": 4.641,
      "text": "achieve almost 190% greater efficiency",
      "timestamp": "08:43"
    },
    {
      "start": 527.92,
      "duration": 3.359,
      "text": "compared to GPU it's why now we are",
      "timestamp": "08:47"
    },
    {
      "start": 531.279,
      "duration": 2.961,
      "text": "sampling with many uh quite good uh",
      "timestamp": "08:51"
    },
    {
      "start": 534.24,
      "duration": 3.44,
      "text": "customers already globally uh they are",
      "timestamp": "08:54"
    },
    {
      "start": 537.68,
      "duration": 2.48,
      "text": "uh there are many enterprise looking for",
      "timestamp": "08:57"
    },
    {
      "start": 540.16,
      "duration": 3.52,
      "text": "alternative to the current GPU products",
      "timestamp": "09:00"
    },
    {
      "start": 543.68,
      "duration": 2.88,
      "text": "and it's why they are right now testing",
      "timestamp": "09:03"
    },
    {
      "start": 546.56,
      "duration": 3.12,
      "text": "our chip and uh thinking of adopting our",
      "timestamp": "09:06"
    },
    {
      "start": 549.68,
      "duration": 1.92,
      "text": "chip for their future inference",
      "timestamp": "09:09"
    },
    {
      "start": 551.6,
      "duration": 2.239,
      "text": "platforms",
      "timestamp": "09:11"
    },
    {
      "start": 553.839,
      "duration": 3.68,
      "text": "so I'd like to briefly also uh introduce",
      "timestamp": "09:13"
    },
    {
      "start": 557.519,
      "duration": 3.841,
      "text": "our company uh uh we started as a",
      "timestamp": "09:17"
    },
    {
      "start": 561.36,
      "duration": 2.56,
      "text": "startup company eight years ago uh with",
      "timestamp": "09:21"
    },
    {
      "start": 563.92,
      "duration": 2.8,
      "text": "the three co-founders including me uh my",
      "timestamp": "09:23"
    },
    {
      "start": 566.72,
      "duration": 2.239,
      "text": "career was all uh about the",
      "timestamp": "09:26"
    },
    {
      "start": 568.959,
      "duration": 1.761,
      "text": "semiconductor and the electrical",
      "timestamp": "09:28"
    },
    {
      "start": 570.72,
      "duration": 3.119,
      "text": "engineering uh and when you started in",
      "timestamp": "09:30"
    },
    {
      "start": 573.839,
      "duration": 2.321,
      "text": "201",
      "timestamp": "09:33"
    },
    {
      "start": 576.16,
      "duration": 4.64,
      "text": "uh we actually had a high uh kind of",
      "timestamp": "09:36"
    },
    {
      "start": 580.8,
      "duration": 3.92,
      "text": "belief about the future of AI uh it is a",
      "timestamp": "09:40"
    },
    {
      "start": 584.72,
      "duration": 3.44,
      "text": "very uh transformative uh technology and",
      "timestamp": "09:44"
    },
    {
      "start": 588.16,
      "duration": 2.239,
      "text": "uh from the very beginning we set out to",
      "timestamp": "09:48"
    },
    {
      "start": 590.399,
      "duration": 2.721,
      "text": "build high performance in energy",
      "timestamp": "09:50"
    },
    {
      "start": 593.12,
      "duration": 3.0,
      "text": "efficiency",
      "timestamp": "09:53"
    },
    {
      "start": 596.56,
      "duration": 5.2,
      "text": "designed for AI at scale um and uh after",
      "timestamp": "09:56"
    },
    {
      "start": 601.76,
      "duration": 3.12,
      "text": "our first flagship product uh actually",
      "timestamp": "10:01"
    },
    {
      "start": 604.88,
      "duration": 3.28,
      "text": "we decided to take a very bold bet uh",
      "timestamp": "10:04"
    },
    {
      "start": 608.16,
      "duration": 3.2,
      "text": "designing a large scale SOC for a",
      "timestamp": "10:08"
    },
    {
      "start": 611.36,
      "duration": 2.719,
      "text": "massive language models even though at",
      "timestamp": "10:11"
    },
    {
      "start": 614.079,
      "duration": 2.721,
      "text": "that time the language models were still",
      "timestamp": "10:14"
    },
    {
      "start": 616.8,
      "duration": 3.44,
      "text": "uh quite experimental at that time so it",
      "timestamp": "10:16"
    },
    {
      "start": 620.24,
      "duration": 2.56,
      "text": "was quite risky move for a startup but",
      "timestamp": "10:20"
    },
    {
      "start": 622.8,
      "duration": 2.8,
      "text": "we believed in the future this model",
      "timestamp": "10:22"
    },
    {
      "start": 625.6,
      "duration": 4.4,
      "text": "would would bring uh to uh to the u to",
      "timestamp": "10:25"
    },
    {
      "start": 630.0,
      "duration": 2.88,
      "text": "the to the world actually uh because uh",
      "timestamp": "10:30"
    },
    {
      "start": 632.88,
      "duration": 2.56,
      "text": "we still have the same belief that the",
      "timestamp": "10:32"
    },
    {
      "start": 635.44,
      "duration": 2.399,
      "text": "more capable AI will bring the more",
      "timestamp": "10:35"
    },
    {
      "start": 637.839,
      "duration": 5.201,
      "text": "value to the people and uh um to realize",
      "timestamp": "10:37"
    },
    {
      "start": 643.04,
      "duration": 3.2,
      "text": "the future uh and uh uh we definitely",
      "timestamp": "10:43"
    },
    {
      "start": 646.24,
      "duration": 3.279,
      "text": "need u uh quite sustainable AI",
      "timestamp": "10:46"
    },
    {
      "start": 649.519,
      "duration": 2.401,
      "text": "infrastructure as well",
      "timestamp": "10:49"
    },
    {
      "start": 651.92,
      "duration": 4.64,
      "text": "so uh uh I hope um uh like uh if you",
      "timestamp": "10:51"
    },
    {
      "start": 656.56,
      "duration": 3.04,
      "text": "anyone who like to need a strong",
      "timestamp": "10:56"
    },
    {
      "start": 659.6,
      "duration": 2.96,
      "text": "efficient AI compute solution whether in",
      "timestamp": "10:59"
    },
    {
      "start": 662.56,
      "duration": 2.88,
      "text": "cloud and especially also if you like to",
      "timestamp": "11:02"
    },
    {
      "start": 665.44,
      "duration": 3.44,
      "text": "own your own like on premise like your",
      "timestamp": "11:05"
    },
    {
      "start": 668.88,
      "duration": 2.72,
      "text": "own AI computing infrastructure uh we",
      "timestamp": "11:08"
    },
    {
      "start": 671.6,
      "duration": 2.72,
      "text": "hope we can uh work with your team and",
      "timestamp": "11:11"
    },
    {
      "start": 674.32,
      "duration": 3.28,
      "text": "we are very committed to support uh that",
      "timestamp": "11:14"
    },
    {
      "start": 677.6,
      "duration": 3.12,
      "text": "uh building this new AI infrastructure",
      "timestamp": "11:17"
    },
    {
      "start": 680.72,
      "duration": 2.72,
      "text": "together uh with you guys as well so",
      "timestamp": "11:20"
    },
    {
      "start": 683.44,
      "duration": 3.48,
      "text": "thank you very much",
      "timestamp": "11:23"
    }
  ],
  "extraction_timestamp": "2025-06-29T20:47:10.713002",
  "playlist_title": "SuperAI Singapore 2025: io.net Main Stage"
}